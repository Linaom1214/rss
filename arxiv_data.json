[
  {
    "title": "Dual Prototype Evolving for Test-Time Generalization of Vision-Language Models",
    "author": "Ce Zhang, Simon Stepputtis, Katia Sycara, Yaqi Xie",
    "summary": "Test-time adaptation, which enables models to generalize to diverse data with unlabeled test samples, holds significant value in real-world scenarios. Recently, researchers have applied this setting to advanced pre-trained vision-language models (VLMs), developing approaches such as test-time prompt tuning to further extend their practical applicability. However, these methods typically focus solely on adapting VLMs from a single modality and fail to accumulate task-specific knowledge as more samples are processed. To address this, we introduce Dual Prototype Evolving (DPE), a novel test-time adaptation approach for VLMs that effectively accumulates task-specific knowledge from multi-modalities. Specifically, we create and evolve two sets of prototypes--textual and visual--to progressively capture more accurate multi-modal representations for target classes during test time. Moreover, to promote consistent multi-modal representations, we introduce and optimize learnable residuals for each test sample to align the prototypes from both modalities. Extensive experimental results on 15 benchmark datasets demonstrate that our proposed DPE consistently outperforms previous state-of-the-art methods while also exhibiting competitive computational efficiency. Code is available at https://github.com/zhangce01/DPE-CLIP.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12790v1",
    "code_url": null,
    "category": "ISTD-infrared small target"
  },
  {
    "title": "Long-LRM: Long-sequence Large Reconstruction Model for Wide-coverage Gaussian Splats",
    "author": "Chen Ziwen, Hao Tan, Kai Zhang, Sai Bi, Fujun Luan, Yicong Hong, Li Fuxin, Zexiang Xu",
    "summary": "We propose Long-LRM, a generalizable 3D Gaussian reconstruction model that is capable of reconstructing a large scene from a long sequence of input images. Specifically, our model can process 32 source images at 960x540 resolution within only 1.3 seconds on a single A100 80G GPU. Our architecture features a mixture of the recent Mamba2 blocks and the classical transformer blocks which allowed many more tokens to be processed than prior work, enhanced by efficient token merging and Gaussian pruning steps that balance between quality and efficiency. Unlike previous feed-forward models that are limited to processing 1~4 input images and can only reconstruct a small portion of a large scene, Long-LRM reconstructs the entire scene in a single feed-forward step. On large-scale scene datasets such as DL3DV-140 and Tanks and Temples, our method achieves performance comparable to optimization-based approaches while being two orders of magnitude more efficient. Project page: https://arthurhero.github.io/projects/llrm",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12781v1",
    "code_url": null,
    "category": "ISTD-infrared small target"
  },
  {
    "title": "Collimated $ \u03b3$-flash emission along the target surface irradiated by a laser at non-grazing incidence",
    "author": "M. Matys, P. Hadjisolomou, R. Shaisultanov, P. Valenta, M. Lama\u010d, T. M. Jeong, J. P. Thistlewood, C. P. Ridgers, A. S. Pirozhkov, S. V. Bulanov",
    "summary": "The interaction of a high-power laser with a solid target provides ways to produce beams of $\\gamma$-photons. For normal incidence of the laser on the target the beams usually appear in a form of two lobes, which are symmetric with respect to the laser propagation axis. In this work we demonstrate via three-dimensional particle-in-cell simulations a regime where for oblique incidence the emission of a collimated $\\gamma$-photon beam is in the direction parallel to the target surface. The process is ascribed to the interference pattern in the electromagnetic field formed by the incident and reflected laser pulse. The electromagnetic field accelerates electrons to the GeV energy level, while temporarily directing their momentum along the target surface. Consequently, they emit a collimated $\\gamma$-photon beam in the same direction. The dependencies of $\\gamma$-photon emission on the incident angle, laser pulse polarization, power and duration and target thickness are also addressed in the paper. The beam directionality is important for designing future experiments. In addition, this setup causes the generation of high-order harmonics propagating along the target surface.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12780v1",
    "code_url": null,
    "category": "ISTD-infrared small target"
  },
  {
    "title": "Measuring time-resolved heat transfer fluctuations on a heated-thin foil in a turbulent channel airflow",
    "author": "Antonio Cu\u00e9llar, Enrico Amico, Jacopo Serpieri, Gioacchino Cafiero, Woutijn J Baars, Stefano Discetti, Andrea Ianiro",
    "summary": "We present an experimental setup to perform time-resolved convective heat transfer measurements in a turbulent channel flow with air as the working fluid. We employ a heated thin foil coupled with high-speed infrared thermography. The measurement technique is challenged by the thermal inertia of the foil, the high frequency of turbulent fluctuations, and the measurement noise of the infrared camera. We discuss in detail the advantages and drawbacks of all the design choices that were made, thereby providing a successful implementation strategy to obtain high-quality data. This experimental approach could be useful for experimental studies employing wall-based measurements of turbulence, such as flow control applications in wall-bounded turbulence.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12778v1",
    "code_url": null,
    "category": "ISTD-infrared small target"
  },
  {
    "title": "Should exponential integrators be used for advection-dominated problems?",
    "author": "Lukas Einkemmer, Trung-Hau Hoang, Alexander Ostermann",
    "summary": "In this paper, we consider the application of exponential integrators to problems that are advection dominated, either on the entire or on a subset of the domain. In this context, we compare Leja and Krylov based methods to compute the action of exponential and related matrix functions. We set up a performance model by counting the different operations needed to implement the considered algorithms. This model assumes that the evaluation of the right-hand side is memory bound and allows us to evaluate performance in a hardware independent way. We find that exponential integrators perform comparably to explicit Runge-Kutta schemes for problems that are advection dominated in the entire domain. Moreover, they are able to outperform explicit methods in situations where small parts of the domain are diffusion dominated. We generally observe that Leja based methods outperform Krylov iterations in the problems considered. This is in particular true if computing inner products is expensive.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12765v1",
    "code_url": null,
    "category": "ISTD-infrared small target"
  },
  {
    "title": "Polarization options in inclusive DIS off tensor polarized deuteron",
    "author": "Wim Cosyn, Brandon Roldan Tomei, Alan Sosa, Allison Zec",
    "summary": "In the near future, the Jefferson Lab $b_1$ experiment will provide the second measurement of tensor polarized asymmetries in inclusive DIS on the deuteron. In this asymmetry, 4 independent tensor polarized structure functions contribute. This necessitates systematic approximations in the extraction of the leading twist structure function $b_1$ from a single tensor asymmetry measurement. Contamination from higher twist structure functions and kinematic effects is discussed here. Using a deuteron convolution model, we quantify the systematic errors from these approximations for two different choices for the target polarization direction (momentum transfer, electron beam direction). For Jefferson Lab 12 GeV kinematics, the systematic error turns out to be comparable between the two polarization options, while at higher $Q^2$ values the momentum transfer direction is preferred.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12764v1",
    "code_url": null,
    "category": "ISTD-infrared small target"
  },
  {
    "title": "SAFREE: Training-Free and Adaptive Guard for Safe Text-to-Image And Video Generation",
    "author": "Jaehong Yoon, Shoubin Yu, Vaidehi Patil, Huaxiu Yao, Mohit Bansal",
    "summary": "Recent advances in diffusion models have significantly enhanced their ability to generate high-quality images and videos, but they have also increased the risk of producing unsafe content. Existing unlearning/editing-based methods for safe generation remove harmful concepts from models but face several challenges: (1) They cannot instantly remove harmful concepts without training. (2) Their safe generation capabilities depend on collected training data. (3) They alter model weights, risking degradation in quality for content unrelated to toxic concepts. To address these, we propose SAFREE, a novel, training-free approach for safe T2I and T2V, that does not alter the model's weights. Specifically, we detect a subspace corresponding to a set of toxic concepts in the text embedding space and steer prompt embeddings away from this subspace, thereby filtering out harmful content while preserving intended semantics. To balance the trade-off between filtering toxicity and preserving safe concepts, SAFREE incorporates a novel self-validating filtering mechanism that dynamically adjusts the denoising steps when applying the filtered embeddings. Additionally, we incorporate adaptive re-attention mechanisms within the diffusion latent space to selectively diminish the influence of features related to toxic concepts at the pixel level. In the end, SAFREE ensures coherent safety checking, preserving the fidelity, quality, and safety of the output. SAFREE achieves SOTA performance in suppressing unsafe content in T2I generation compared to training-free baselines and effectively filters targeted concepts while maintaining high-quality images. It also shows competitive results against training-based methods. We extend SAFREE to various T2I backbones and T2V tasks, showcasing its flexibility and generalization. SAFREE provides a robust and adaptable safeguard for ensuring safe visual generation.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12761v1",
    "code_url": null,
    "category": "ISTD-infrared small target"
  },
  {
    "title": "Unitary Multi-Margin BERT for Robust Natural Language Processing",
    "author": "Hao-Yuan Chang, Kang L. Wang",
    "summary": "Recent developments in adversarial attacks on deep learning leave many mission-critical natural language processing (NLP) systems at risk of exploitation. To address the lack of computationally efficient adversarial defense methods, this paper reports a novel, universal technique that drastically improves the robustness of Bidirectional Encoder Representations from Transformers (BERT) by combining the unitary weights with the multi-margin loss. We discover that the marriage of these two simple ideas amplifies the protection against malicious interference. Our model, the unitary multi-margin BERT (UniBERT), boosts post-attack classification accuracies significantly by 5.3% to 73.8% while maintaining competitive pre-attack accuracies. Furthermore, the pre-attack and post-attack accuracy tradeoff can be adjusted via a single scalar parameter to best fit the design requirements for the target applications.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12759v1",
    "code_url": null,
    "category": "ISTD-infrared small target"
  },
  {
    "title": "Toleo: Scaling Freshness to Tera-scale Memory using CXL and PIM",
    "author": "Juechu Dong, Jonah Rosenblum, Satish Narayanasamy",
    "summary": "Trusted hardware's freshness guarantee ensures that an adversary cannot replay an old value in response to a memory read request. They rely on maintaining a version number for each cache block and ensuring their integrity using a Merkle tree. However, these existing solutions protect only a small amount of main memory (few MBs), as the extraneous memory accesses to the Merkle tree increase prohibitively with the protected memory size. We present Toleo, which uses trusted smart memory connected through a secure CXL IDE network to safely store version numbers. Toleo eliminates the need for an unscalable Merkle tree to protect the integrity of version numbers by instead using smart memory as the root of trust. Additionally, Toleo ensures version confidentiality which enables stealth versions that reduce the version storage overhead in half.   Furthermore, in the absence of Merkle tree imposed constraints, we effectively exploit version locality at page granularity to compress version number by a factor of 240. These space optimizations make it feasible for one 168 GB Toleo smart memory device to provide freshness to a 28 TB CXL-expanded main memory pool in a rack server for a negligible performance overhead. We analyze the benefits of Toleo using several privacy-sensitive genomics, graph, generative AI, and database workloads.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12749v1",
    "code_url": null,
    "category": "ISTD-infrared small target"
  },
  {
    "title": "DRIP: A Versatile Family of Space-Time ISAC Waveforms",
    "author": "Dexin Wang, Ahmad Bazzi, Marwa Chafii",
    "summary": "The following paper introduces Dual beam-similarity awaRe Integrated sensing and communications (ISAC) with controlled Peak-to-average power ratio (DRIP) waveforms. DRIP is a novel family of space-time ISAC waveforms designed for dynamic peak-to-average power ratio (PAPR) adjustment. The proposed DRIP waveforms are designed to conform to specified PAPR levels while exhibiting beampattern properties, effectively targeting multiple desired directions and suppressing interference for multi-target sensing applications, while closely resembling radar chirps. For communication purposes, the proposed DRIP waveforms aim to minimize multi-user interference across various constellations. Addressing the non-convexity of the optimization framework required for generating DRIP waveforms, we introduce a block cyclic coordinate descent algorithm. This iterative approach ensures convergence to an optimal ISAC waveform solution. Simulation results validate the DRIP waveforms' superior performance, versatility, and favorable ISAC trade-offs, highlighting their potential in advanced multi-target sensing and communication systems.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12746v1",
    "code_url": null,
    "category": "ISTD-infrared small target"
  },
  {
    "title": "WorldMedQA-V: a multilingual, multimodal medical examination dataset for multimodal language models evaluation",
    "author": "Jo\u00e3o Matos, Shan Chen, Siena Placino, Yingya Li, Juan Carlos Climent Pardo, Daphna Idan, Takeshi Tohyama, David Restrepo, Luis F. Nakayama, Jose M. M. Pascual-Leone, Guergana Savova, Hugo Aerts, Leo A. Celi, A. Ian Wong, Danielle S. Bitterman, Jack Gallifant",
    "summary": "Multimodal/vision language models (VLMs) are increasingly being deployed in healthcare settings worldwide, necessitating robust benchmarks to ensure their safety, efficacy, and fairness. Multiple-choice question and answer (QA) datasets derived from national medical examinations have long served as valuable evaluation tools, but existing datasets are largely text-only and available in a limited subset of languages and countries. To address these challenges, we present WorldMedQA-V, an updated multilingual, multimodal benchmarking dataset designed to evaluate VLMs in healthcare. WorldMedQA-V includes 568 labeled multiple-choice QAs paired with 568 medical images from four countries (Brazil, Israel, Japan, and Spain), covering original languages and validated English translations by native clinicians, respectively. Baseline performance for common open- and closed-source models are provided in the local language and English translations, and with and without images provided to the model. The WorldMedQA-V benchmark aims to better match AI systems to the diverse healthcare environments in which they are deployed, fostering more equitable, effective, and representative applications.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12722v1",
    "code_url": null,
    "category": "segmentation-medical"
  },
  {
    "title": "VividMed: Vision Language Model with Versatile Visual Grounding for Medicine",
    "author": "Lingxiao Luo, Bingda Tang, Xuanzhong Chen, Rong Han, Ting Chen",
    "summary": "Recent advancements in Vision Language Models (VLMs) have demonstrated remarkable promise in generating visually grounded responses. However, their application in the medical domain is hindered by unique challenges. For instance, most VLMs rely on a single method of visual grounding, whereas complex medical tasks demand more versatile approaches. Additionally, while most VLMs process only 2D images, a large portion of medical images are 3D. The lack of medical data further compounds these obstacles. To address these challenges, we present VividMed, a vision language model with versatile visual grounding for medicine. Our model supports generating both semantic segmentation masks and instance-level bounding boxes, and accommodates various imaging modalities, including both 2D and 3D data. We design a three-stage training procedure and an automatic data synthesis pipeline based on open datasets and models. Besides visual grounding tasks, VividMed also excels in other common downstream tasks, including Visual Question Answering (VQA) and report generation. Ablation studies empirically show that the integration of visual grounding ability leads to improved performance on these tasks. Our code is publicly available at https://github.com/function2-llx/MMMM.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12694v1",
    "code_url": null,
    "category": "segmentation-medical"
  },
  {
    "title": "Machine Learning Approach to Brain Tumor Detection and Classification",
    "author": "Alice Oh, Inyoung Noh, Jian Choo, Jihoo Lee, Justin Park, Kate Hwang, Sanghyeon Kim, Soo Min Oh",
    "summary": "Brain tumor detection and classification are critical tasks in medical image analysis, particularly in early-stage diagnosis, where accurate and timely detection can significantly improve treatment outcomes. In this study, we apply various statistical and machine learning models to detect and classify brain tumors using brain MRI images. We explore a variety of statistical models including linear, logistic, and Bayesian regressions, and the machine learning models including decision tree, random forest, single-layer perceptron, multi-layer perceptron, convolutional neural network (CNN), recurrent neural network, and long short-term memory. Our findings show that CNN outperforms other models, achieving the best performance. Additionally, we confirm that the CNN model can also work for multi-class classification, distinguishing between four categories of brain MRI images such as normal, glioma, meningioma, and pituitary tumor images. This study demonstrates that machine learning approaches are suitable for brain tumor detection and classification, facilitating real-world medical applications in assisting radiologists with early and accurate diagnosis.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12692v1",
    "code_url": null,
    "category": "segmentation-medical"
  },
  {
    "title": "Automatic Mapping of Anatomical Landmarks from Free-Text Using Large Language Models: Insights from Llama-2",
    "author": "Mohamad Abdi, Gerardo Hemosillo Valadez, Halid Ziya Yerebakan",
    "summary": "Anatomical landmarks are vital in medical imaging for navigation and anomaly detection. Modern large language models (LLMs), like Llama-2, offer promise for automating the mapping of these landmarks in free-text radiology reports to corresponding positions in image data. Recent studies propose LLMs may develop coherent representations of generative processes. Motivated by these insights, we investigated whether LLMs accurately represent the spatial positions of anatomical landmarks. Through experiments with Llama-2 models, we found that they can linearly represent anatomical landmarks in space with considerable robustness to different prompts. These results underscore the potential of LLMs to enhance the efficiency and accuracy of medical imaging workflows.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12686v1",
    "code_url": null,
    "category": "segmentation-medical"
  },
  {
    "title": "Cascade learning in multi-task encoder-decoder networks for concurrent bone segmentation and glenohumeral joint assessment in shoulder CT scans",
    "author": "Luca Marsilio, Davide Marzorati, Matteo Rossi, Andrea Moglia, Luca Mainardi, Alfonso Manzotti, Pietro Cerveri",
    "summary": "Osteoarthritis is a degenerative condition affecting bones and cartilage, often leading to osteophyte formation, bone density loss, and joint space narrowing. Treatment options to restore normal joint function vary depending on the severity of the condition. This work introduces an innovative deep-learning framework processing shoulder CT scans. It features the semantic segmentation of the proximal humerus and scapula, the 3D reconstruction of bone surfaces, the identification of the glenohumeral (GH) joint region, and the staging of three common osteoarthritic-related pathologies: osteophyte formation (OS), GH space reduction (JS), and humeroscapular alignment (HSA). The pipeline comprises two cascaded CNN architectures: 3D CEL-UNet for segmentation and 3D Arthro-Net for threefold classification. A retrospective dataset of 571 CT scans featuring patients with various degrees of GH osteoarthritic-related pathologies was used to train, validate, and test the pipeline. Root mean squared error and Hausdorff distance median values for 3D reconstruction were 0.22mm and 1.48mm for the humerus and 0.24mm and 1.48mm for the scapula, outperforming state-of-the-art architectures and making it potentially suitable for a PSI-based shoulder arthroplasty preoperative plan context. The classification accuracy for OS, JS, and HSA consistently reached around 90% across all three categories. The computational time for the inference pipeline was less than 15s, showcasing the framework's efficiency and compatibility with orthopedic radiology practice. The outcomes represent a promising advancement toward the medical translation of artificial intelligence tools. This progress aims to streamline the preoperative planning pipeline delivering high-quality bone surfaces and supporting surgeons in selecting the most suitable surgical approach according to the unique patient joint conditions.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12641v1",
    "code_url": null,
    "category": "segmentation-medical"
  },
  {
    "title": "From Lab to Pocket: A Novel Continual Learning-based Mobile Application for Screening COVID-19",
    "author": "Danny Falero, Muhammad Ashad Kabir, Nusrat Homaira",
    "summary": "Artificial intelligence (AI) has emerged as a promising tool for predicting COVID-19 from medical images. In this paper, we propose a novel continual learning-based approach and present the design and implementation of a mobile application for screening COVID-19. Our approach demonstrates the ability to adapt to evolving datasets, including data collected from different locations or hospitals, varying virus strains, and diverse clinical presentations, without retraining from scratch. We have evaluated state-of-the-art continual learning methods for detecting COVID-19 from chest X-rays and selected the best-performing model for our mobile app. We evaluated various deep learning architectures to select the best-performing one as a foundation model for continual learning. Both regularization and memory-based methods for continual learning were tested, using different memory sizes to develop the optimal continual learning model for our app. DenseNet161 emerged as the best foundation model with 96.87\\% accuracy, and Learning without Forgetting (LwF) was the top continual learning method with an overall performance of 71.99\\%. The mobile app design considers both patient and doctor perspectives. It incorporates the continual learning DenseNet161 LwF model on a cloud server, enabling the model to learn from new instances of chest X-rays and their classifications as they are submitted. The app is designed, implemented, and evaluated to ensure it provides an efficient tool for COVID-19 screening. The app is available to download from https://github.com/DannyFGitHub/COVID-19PneumoCheckApp.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12589v1",
    "code_url": null,
    "category": "segmentation-medical"
  },
  {
    "title": "REST API Testing in DevOps: A Study on an Evolving Healthcare IoT Application",
    "author": "Hassan Sartaj, Shaukat Ali, Julie Marie Gj\u00f8by",
    "summary": "Healthcare Internet of Things (IoT) applications often integrate various third-party healthcare applications and medical devices through REST APIs, resulting in complex and interdependent networks of REST APIs. Oslo City's healthcare department collaborates with various industry partners to develop such healthcare IoT applications enriched with a diverse set of REST APIs. Following the DevOps process, these REST APIs continuously evolve to accommodate evolving needs such as new features, services, and devices. Oslo City's primary goal is to utilize automated solutions for continuous testing of these REST APIs at each evolution stage, thereby ensuring their dependability. Although the literature offers various automated REST API testing tools, their effectiveness in regression testing of the evolving REST APIs of healthcare IoT applications within a DevOps context remains undetermined. This paper evaluates state-of-the-art and well-established REST API testing tools-specifically, RESTest, EvoMaster, Schemathesis, RESTler, and RestTestGen-for the regression testing of a real-world healthcare IoT application, considering failures, faults, coverage, regressions, and cost. We conducted experiments using all accessible REST APIs (17 APIs with 120 endpoints), and 14 releases evolved during DevOps. Overall, all tools generated tests leading to several failures, 18 potential faults, up to 84% coverage, 23 regressions, and over 80% cost overhead.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12547v1",
    "code_url": null,
    "category": "segmentation-medical"
  },
  {
    "title": "Evaluating Utility of Memory Efficient Medical Image Generation: A Study on Lung Nodule Segmentation",
    "author": "Kathrin Khadra, Utku T\u00fcrkbey",
    "summary": "The scarcity of publicly available medical imaging data limits the development of effective AI models. This work proposes a memory-efficient patch-wise denoising diffusion probabilistic model (DDPM) for generating synthetic medical images, focusing on CT scans with lung nodules. Our approach generates high-utility synthetic images with nodule segmentation while efficiently managing memory constraints, enabling the creation of training datasets. We evaluate the method in two scenarios: training a segmentation model exclusively on synthetic data, and augmenting real-world training data with synthetic images. In the first case, models trained solely on synthetic data achieve Dice scores comparable to those trained on real-world data benchmarks. In the second case, augmenting real-world data with synthetic images significantly improves segmentation performance. The generated images demonstrate their potential to enhance medical image datasets in scenarios with limited real-world data.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12542v1",
    "code_url": null,
    "category": "segmentation-medical"
  },
  {
    "title": "Radiation damage and recovery of plastic scintillators under ultra-high dose rate 200 MeV electrons at CERN CLEAR facility",
    "author": "Clo\u00e9 Gigu\u00e8re, Alexander Hart, Joseph Bateman, Pierre Korysko, Wilfrid Farabolini, Yoan LeChasseur, Magdalena Bazalova-Carter, Luc Beaulieu",
    "summary": "The FLASH effect holds significant potential in improving radiotherapy treatment outcomes. Very high energy electrons (VHEEs) can effectively target tumors deep in the body and can be accelerated to achieve ultra-high dose rates (UHDR), making them a promising modality for delivering FLASH radiotherapy in the clinic. However, apart from suitable VHEE sources, clinical translation requires accurate dosimetry, which is challenging due to the limitation of standard dosimeters under UHDR. Water-equivalent and real-time plastic scintillation dosimeters (PSDs) may offer a solution. In this study, a 4-channel PSD, consisting of polystyrene-based BCF12 and Medscint proprietary scintillators, polyvinyltoluene (PVT)-based EJ-212 and a clear plastic fiber channel for Cherenkov subtraction was exposed to the 200 MeV VHEE UHDR beam at the CLEAR CERN facility. The Hyperscint RP200 platform was used to assess linearity to dose pulses of up to 90 Gy and dose rates up to 4.6x10$^9$ Gy/s, and to investigate radiation damage and recovery after dose accumulation of 37.2 kGy. While clear fiber response was linear across the entire dose range studied, light output saturated above ~50 Gy/pulse for scintillators. Despite radiation damage, linearity was preserved, though it resulted in a decrease of scintillator and clear fiber light output of <1.85 %/kGy and a shift in spectra towards longer wavelengths. Short-term recovery (<100h) of these changes was observed and depended on rest duration and accumulated dose. After long-term rest (<172 days), light output recovery was partial, with 6-22% of residual permanent damage remaining, while spectral recovery was complete. We showed that PSDs are sensitive to radiation damage, but maintain dose linearity even after accumulated dose of 37.2 kGy, and exhibit significant response recovery. This work highlights the potential of PSDs for dosimetry in UHDR conditions.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12535v1",
    "code_url": null,
    "category": "segmentation-medical"
  },
  {
    "title": "Imaging neutrons with a position-sensitive monolithic CLYC detector",
    "author": "J. Lerendegui-Marco, G. Cisterna, J. Hallam, V. Babiano-Su\u00e1rez, J. Balibrea-Correa, D. Calvo, I. Ladarescu, G. de la Fuente, B. Gameiro, A. Sanchis-Molt\u00f3, P. Torres-S\u00e1nchez, C. Domingo-Pardo",
    "summary": "In this work, we have developed and characterized a position-sensitive CLYC detector that acts as the neutron imaging layer and $\\gamma$-ray Compton scatterer of the novel dual \\g-ray and neutron imaging system GN-Vision, which aims at simultaneously obtaining information about the spatial origin of \\g-ray and neutron sources. We first investigated the performance of large 50$\\times$50~mm$^{2}$ monolithic CLYC crystals coupled to a pixelated SiPM in terms of energy resolution and neutron-gamma discrimination. The response of two different 95\\% $^{6}$Li-enriched CLYC detectors coupled to an array of 8$\\times$8 SiPMs was studied in comparison to the results of a conventional photo-multiplier tube. Energy resolution ranging from 6-8\\% for the $^{137}$Cs peak and a figure of merit of 3-4 for the neutron-gamma discrimination have been obtained. The spatial response of the CLYC-SiPM detector to $\\gamma$-rays and neutrons has also been characterized using charge modulation-based multiplexing techniques based on a diode-coupled charge division circuit. Average resolutions close to 5~mm FWHM with good linearity are obtained in the transverse crystal plane. Last, this work presents the first proof-of-concept experiments of the neutron imaging capability using a neutron pinhole collimator attached to the developed position sensitive CLYC detector.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12533v1",
    "code_url": null,
    "category": "segmentation-medical"
  },
  {
    "title": "Adaptive Prompt Learning with SAM for Few-shot Scanning Probe Microscope Image Segmentation",
    "author": "Yao Shen, Ziwei Wei, Chunmeng Liu, Shuming Wei, Qi Zhao, Kaiyang Zeng, Guangyao Li",
    "summary": "The Segment Anything Model (SAM) has demonstrated strong performance in image segmentation of natural scene images. However, its effectiveness diminishes markedly when applied to specific scientific domains, such as Scanning Probe Microscope (SPM) images. This decline in accuracy can be attributed to the distinct data distribution and limited availability of the data inherent in the scientific images. On the other hand, the acquisition of adequate SPM datasets is both time-intensive and laborious as well as skill-dependent. To address these challenges, we propose an Adaptive Prompt Learning with SAM (APL-SAM) framework tailored for few-shot SPM image segmentation. Our approach incorporates two key innovations to enhance SAM: 1) An Adaptive Prompt Learning module leverages few-shot embeddings derived from limited support set to learn adaptively central representatives, serving as visual prompts. This innovation eliminates the need for time-consuming online user interactions for providing prompts, such as exhaustively marking points and bounding boxes slice by slice; 2) A multi-source, multi-level mask decoder specifically designed for few-shot SPM image segmentation is introduced, which can effectively capture the correspondence between the support and query images. To facilitate comprehensive training and evaluation, we introduce a new dataset, SPM-Seg, curated for SPM image segmentation. Extensive experiments on this dataset reveal that the proposed APL-SAM framework significantly outperforms the original SAM, achieving over a 30% improvement in terms of Dice Similarity Coefficient with only one-shot guidance. Moreover, APL-SAM surpasses state-of-the-art few-shot segmentation methods and even fully supervised approaches in performance. Code and dataset used in this study will be made available upon acceptance.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12562v1",
    "code_url": null,
    "category": "segmentation-seg"
  },
  {
    "title": "A Holistic Weakly Supervised Approach for Liver Tumor Segmentation with Clinical Knowledge-Informed Label Smoothing",
    "author": "Hairong Wang, Lingchao Mao, Zihan Zhang, Jing Li",
    "summary": "Liver cancer is a leading cause of mortality worldwide, and accurate CT-based tumor segmentation is essential for diagnosis and treatment. Manual delineation is time-intensive, prone to variability, and highlights the need for reliable automation. While deep learning has shown promise for automated liver segmentation, precise liver tumor segmentation remains challenging due to the heterogeneous nature of tumors, imprecise tumor margins, and limited labeled data. We present a novel holistic weakly supervised framework that integrates clinical knowledge to address these challenges with (1) A knowledge-informed label smoothing technique that leverages clinical data to generate smooth labels, which regularizes model training reducing the risk of overfitting and enhancing model performance; (2) A global and local-view segmentation framework, breaking down the task into two simpler sub-tasks, allowing optimized preprocessing and training for each; and (3) Pre- and post-processing pipelines customized to the challenges of each subtask, which enhances tumor visibility and refines tumor boundaries. We evaluated the proposed method on the HCC-TACE-Seg dataset and showed that these three key components complementarily contribute to the improved performance. Lastly, we prototyped a tool for automated liver tumor segmentation and diagnosis summary generation called MedAssistLiver. The app and code are published at https://github.com/lingchm/medassist-liver-cancer.",
    "published": "2024-10-13",
    "link": "http://arxiv.org/abs/2410.10005v1",
    "code_url": "https://github.com/lingchm/medassist-liver-cancer",
    "category": "segmentation-seg"
  },
  {
    "title": "Iterative Optimization Annotation Pipeline and ALSS-YOLO-Seg for Efficient Banana Plantation Segmentation in UAV Imagery",
    "author": "Ang He, Ximei Wu, Xing Xu, Jing Chen, Xiaobin Guo, Sheng Xu",
    "summary": "Precise segmentation of Unmanned Aerial Vehicle (UAV)-captured images plays a vital role in tasks such as crop yield estimation and plant health assessment in banana plantations. By identifying and classifying planted areas, crop area can be calculated, which is indispensable for accurate yield predictions. However, segmenting banana plantation scenes requires a substantial amount of annotated data, and manual labeling of these images is both time-consuming and labor-intensive, limiting the development of large-scale datasets. Furthermore, challenges such as changing target sizes, complex ground backgrounds, limited computational resources, and correct identification of crop categories make segmentation even more difficult. To address these issues, we proposed a comprehensive solution. Firstly, we designed an iterative optimization annotation pipeline leveraging SAM2's zero-shot capabilities to generate high-quality segmentation annotations, thereby reducing the cost and time associated with data annotation significantly. Secondly, we developed ALSS-YOLO-Seg, an efficient lightweight segmentation model optimized for UAV imagery. The model's backbone includes an Adaptive Lightweight Channel Splitting and Shuffling (ALSS) module to improve information exchange between channels and optimize feature extraction, aiding accurate crop identification. Additionally, a Multi-Scale Channel Attention (MSCA) module combines multi-scale feature extraction with channel attention to tackle challenges of varying target sizes and complex ground backgrounds.",
    "published": "2024-10-09",
    "link": "http://arxiv.org/abs/2410.07955v1",
    "code_url": "https://github.com/helloworlder8/computer_vision",
    "category": "segmentation-seg"
  },
  {
    "title": "Leveraging CAM Algorithms for Explaining Medical Semantic Segmentation",
    "author": "Tillmann Rheude, Andreas Wirtz, Arjan Kuijper, Stefan Wesarg",
    "summary": "Convolutional neural networks (CNNs) achieve prevailing results in segmentation tasks nowadays and represent the state-of-the-art for image-based analysis. However, the understanding of the accurate decision-making process of a CNN is rather unknown. The research area of explainable artificial intelligence (xAI) primarily revolves around understanding and interpreting this black-box behavior. One way of interpreting a CNN is the use of class activation maps (CAMs) that represent heatmaps to indicate the importance of image areas for the prediction of the CNN. For classification tasks, a variety of CAM algorithms exist. But for segmentation tasks, only one CAM algorithm for the interpretation of the output of a CNN exist. We propose a transfer between existing classification- and segmentation-based methods for more detailed, explainable, and consistent results which show salient pixels in semantic segmentation tasks. The resulting Seg-HiRes-Grad CAM is an extension of the segmentation-based Seg-Grad CAM with the transfer to the classification-based HiRes CAM. Our method improves the previously-mentioned existing segmentation-based method by adjusting it to recently published classification-based methods. Especially for medical image segmentation, this transfer solves existing explainability disadvantages.",
    "published": "2024-09-30",
    "link": "http://arxiv.org/abs/2409.20287v1",
    "code_url": "https://github.com/TillmannRheude/SegHiResGrad_CAM",
    "category": "segmentation-seg"
  },
  {
    "title": "One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos",
    "author": "Zechen Bai, Tong He, Haiyang Mei, Pichao Wang, Ziteng Gao, Joya Chen, Lei Liu, Zheng Zhang, Mike Zheng Shou",
    "summary": "We introduce VideoLISA, a video-based multimodal large language model designed to tackle the problem of language-instructed reasoning segmentation in videos. Leveraging the reasoning capabilities and world knowledge of large language models, and augmented by the Segment Anything Model, VideoLISA generates temporally consistent segmentation masks in videos based on language instructions. Existing image-based methods, such as LISA, struggle with video tasks due to the additional temporal dimension, which requires temporal dynamic understanding and consistent segmentation across frames. VideoLISA addresses these challenges by integrating a Sparse Dense Sampling strategy into the video-LLM, which balances temporal context and spatial detail within computational constraints. Additionally, we propose a One-Token-Seg-All approach using a specially designed <TRK> token, enabling the model to segment and track objects across multiple frames. Extensive evaluations on diverse benchmarks, including our newly introduced ReasonVOS benchmark, demonstrate VideoLISA's superior performance in video object segmentation tasks involving complex reasoning, temporal understanding, and object tracking. While optimized for videos, VideoLISA also shows promising generalization to image segmentation, revealing its potential as a unified foundation model for language-instructed object segmentation. Code and model will be available at: https://github.com/showlab/VideoLISA.",
    "published": "2024-09-29",
    "link": "http://arxiv.org/abs/2409.19603v1",
    "code_url": null,
    "category": "segmentation-seg"
  },
  {
    "title": "Diff-VPS: Video Polyp Segmentation via a Multi-task Diffusion Network with Adversarial Temporal Reasoning",
    "author": "Yingling Lu, Yijun Yang, Zhaohu Xing, Qiong Wang, Lei Zhu",
    "summary": "Diffusion Probabilistic Models have recently attracted significant attention in the community of computer vision due to their outstanding performance. However, while a substantial amount of diffusion-based research has focused on generative tasks, no work introduces diffusion models to advance the results of polyp segmentation in videos, which is frequently challenged by polyps' high camouflage and redundant temporal cues.In this paper, we present a novel diffusion-based network for video polyp segmentation task, dubbed as Diff-VPS. We incorporate multi-task supervision into diffusion models to promote the discrimination of diffusion models on pixel-by-pixel segmentation. This integrates the contextual high-level information achieved by the joint classification and detection tasks. To explore the temporal dependency, Temporal Reasoning Module (TRM) is devised via reasoning and reconstructing the target frame from the previous frames. We further equip TRM with a generative adversarial self-supervised strategy to produce more realistic frames and thus capture better dynamic cues. Extensive experiments are conducted on SUN-SEG, and the results indicate that our proposed Diff-VPS significantly achieves state-of-the-art performance. Code is available at https://github.com/lydia-yllu/Diff-VPS.",
    "published": "2024-09-11",
    "link": "http://arxiv.org/abs/2409.07238v1",
    "code_url": "https://github.com/lydia-yllu/diff-vps",
    "category": "segmentation-seg"
  },
  {
    "title": "Seg-HGNN: Unsupervised and Light-Weight Image Segmentation with Hyperbolic Graph Neural Networks",
    "author": "Debjyoti Mondal, Rahul Mishra, Chandan Pandey",
    "summary": "Image analysis in the euclidean space through linear hyperspaces is well studied. However, in the quest for more effective image representations, we turn to hyperbolic manifolds. They provide a compelling alternative to capture complex hierarchical relationships in images with remarkably small dimensionality. To demonstrate hyperbolic embeddings' competence, we introduce a light-weight hyperbolic graph neural network for image segmentation, encompassing patch-level features in a very small embedding size. Our solution, Seg-HGNN, surpasses the current best unsupervised method by 2.5\\%, 4\\% on VOC-07, VOC-12 for localization, and by 0.8\\%, 1.3\\% on CUB-200, ECSSD for segmentation, respectively. With less than 7.5k trainable parameters, Seg-HGNN delivers effective and fast ($\\approx 2$ images/second) results on very standard GPUs like the GTX1650. This empirical evaluation presents compelling evidence of the efficacy and potential of hyperbolic representations for vision tasks.",
    "published": "2024-09-10",
    "link": "http://arxiv.org/abs/2409.06589v1",
    "code_url": null,
    "category": "segmentation-seg"
  },
  {
    "title": "Segmenting Object Affordances: Reproducibility and Sensitivity to Scale",
    "author": "Tommaso Apicella, Alessio Xompero, Paolo Gastaldo, Andrea Cavallaro",
    "summary": "Visual affordance segmentation identifies image regions of an object an agent can interact with. Existing methods re-use and adapt learning-based architectures for semantic segmentation to the affordance segmentation task and evaluate on small-size datasets. However, experimental setups are often not reproducible, thus leading to unfair and inconsistent comparisons. In this work, we benchmark these methods under a reproducible setup on two single objects scenarios, tabletop without occlusions and hand-held containers, to facilitate future comparisons. We include a version of a recent architecture, Mask2Former, re-trained for affordance segmentation and show that this model is the best-performing on most testing sets of both scenarios. Our analysis shows that models are not robust to scale variations when object resolutions differ from those in the training set.",
    "published": "2024-09-03",
    "link": "http://arxiv.org/abs/2409.01814v1",
    "code_url": "https://github.com/apicis/aff-seg",
    "category": "segmentation-seg"
  },
  {
    "title": "LSMS: Language-guided Scale-aware MedSegmentor for Medical Image Referring Segmentation",
    "author": "Shuyi Ouyang, Jinyang Zhang, Xiangye Lin, Xilai Wang, Qingqing Chen, Yen-Wei Chen, Lanfen Lin",
    "summary": "Conventional medical image segmentation methods have been found inadequate in facilitating physicians with the identification of specific lesions for diagnosis and treatment. Given the utility of text as an instructional format, we introduce a novel task termed Medical Image Referring Segmentation (MIRS), which requires segmenting specified lesions in images based on the given language expressions. Due to the varying object scales in medical images, MIRS demands robust vision-language modeling and comprehensive multi-scale interaction for precise localization and segmentation under linguistic guidance. However, existing medical image segmentation methods fall short in meeting these demands, resulting in insufficient segmentation accuracy. In response, we propose an approach named Language-guided Scale-aware MedSegmentor (LSMS), incorporating two appealing designs: (1)~a Scale-aware Vision-Language Attention module that leverages diverse convolutional kernels to acquire rich visual knowledge and interact closely with linguistic features, thereby enhancing lesion localization capability; (2)~a Full-Scale Decoder that globally models multi-modal features across various scales, capturing complementary information between scales to accurately outline lesion boundaries. Addressing the lack of suitable datasets for MIRS, we constructed a vision-language medical dataset called Reference Hepatic Lesion Segmentation (RefHL-Seg). This dataset comprises 2,283 abdominal CT slices from 231 cases, with corresponding textual annotations and segmentation masks for various liver lesions in images. We validated the performance of LSMS for MIRS and conventional medical image segmentation tasks across various datasets. Our LSMS consistently outperforms on all datasets with lower computational costs. The code and datasets will be released.",
    "published": "2024-08-30",
    "link": "http://arxiv.org/abs/2408.17347v2",
    "code_url": null,
    "category": "segmentation-seg"
  },
  {
    "title": "LV-UNet: A Lightweight and Vanilla Model for Medical Image Segmentation",
    "author": "Juntao Jiang, Mengmeng Wang, Huizhong Tian, Lingbo Cheng, Yong Liu",
    "summary": "Although the progress made by large models in computer vision, optimization challenges, the complexity of transformer models, computational limitations, and the requirements of practical applications call for simpler designs in model architecture for medical image segmentation, especially in mobile medical devices that require lightweight and deployable models with real-time performance. However, some of the current lightweight models exhibit poor robustness across different datasets, which hinders their broader adoption. This paper proposes a lightweight and vanilla model called LV-UNet, which effectively utilizes pre-trained MobileNetv3-Large models and introduces fusible modules. It can be trained using an improved deep training strategy and switched to deployment mode during inference, reducing both parameter count and computational load. Experiments are conducted on ISIC 2016, BUSI, CVC- ClinicDB, CVC-ColonDB, and Kvair-SEG datasets, achieving better performance compared to the state-of-the-art and classic models.",
    "published": "2024-08-29",
    "link": "http://arxiv.org/abs/2408.16886v1",
    "code_url": null,
    "category": "segmentation-seg"
  },
  {
    "title": "MambaBEV: An efficient 3D detection model with Mamba2",
    "author": "Zihan You, Hao Wang, Qichao Zhao, Jinxiang Wang",
    "summary": "A stable 3D object detection model based on BEV paradigm with temporal information is very important for autonomous driving systems. However, current temporal fusion model use convolutional layer or deformable self-attention is not conducive to the exchange of global information of BEV space and has more computational cost. Recently, a newly proposed based model specialized in processing sequence called mamba has shown great potential in multiple downstream task. In this work, we proposed a mamba2-based BEV 3D object detection model named MambaBEV. We also adapt an end to end self driving paradigm to test the performance of the model. Our work performs pretty good results on nucences datasets:Our base version achieves 51.7% NDS. Our code will be available soon.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12673v1",
    "code_url": null,
    "category": "mamba-mamba"
  },
  {
    "title": "UmambaTSF: A U-shaped Multi-Scale Long-Term Time Series Forecasting Method Using Mamba",
    "author": "Li Wu, Wenbin Pei, Jiulong Jiao, Qiang Zhang",
    "summary": "Multivariate Time series forecasting is crucial in domains such as transportation, meteorology, and finance, especially for predicting extreme weather events. State-of-the-art methods predominantly rely on Transformer architectures, which utilize attention mechanisms to capture temporal dependencies. However, these methods are hindered by quadratic time complexity, limiting the model's scalability with respect to input sequence length. This significantly restricts their practicality in the real world. Mamba, based on state space models (SSM), provides a solution with linear time complexity, increasing the potential for efficient forecasting of sequential data. In this study, we propose UmambaTSF, a novel long-term time series forecasting framework that integrates multi-scale feature extraction capabilities of U-shaped encoder-decoder multilayer perceptrons (MLP) with Mamba's long sequence representation. To improve performance and efficiency, the Mamba blocks introduced in the framework adopt a refined residual structure and adaptable design, enabling the capture of unique temporal signals and flexible channel processing. In the experiments, UmambaTSF achieves state-of-the-art performance and excellent generality on widely used benchmark datasets while maintaining linear time complexity and low memory consumption.",
    "published": "2024-10-15",
    "link": "http://arxiv.org/abs/2410.11278v1",
    "code_url": null,
    "category": "mamba-mamba"
  },
  {
    "title": "Mimetic Initialization Helps State Space Models Learn to Recall",
    "author": "Asher Trockman, Hrayr Harutyunyan, J. Zico Kolter, Sanjiv Kumar, Srinadh Bhojanapalli",
    "summary": "Recent work has shown that state space models such as Mamba are significantly worse than Transformers on recall-based tasks due to the fact that their state size is constant with respect to their input sequence length. But in practice, state space models have fairly large state sizes, and we conjecture that they should be able to perform much better at these tasks than previously reported. We investigate whether their poor copying and recall performance could be due in part to training difficulties rather than fundamental capacity constraints. Based on observations of their \"attention\" maps, we propose a structured initialization technique that allows state space layers to more readily mimic attention. Across a variety of architecture settings, our initialization makes it substantially easier for Mamba to learn to copy and do associative recall from scratch.",
    "published": "2024-10-14",
    "link": "http://arxiv.org/abs/2410.11135v1",
    "code_url": null,
    "category": "mamba-mamba"
  },
  {
    "title": "CleanUMamba: A Compact Mamba Network for Speech Denoising using Channel Pruning",
    "author": "Sjoerd Groot, Qinyu Chen, Jan C. van Gemert, Chang Gao",
    "summary": "This paper presents CleanUMamba, a time-domain neural network architecture designed for real-time causal audio denoising directly applied to raw waveforms. CleanUMamba leverages a U-Net encoder-decoder structure, incorporating the Mamba state-space model in the bottleneck layer. By replacing conventional self-attention and LSTM mechanisms with Mamba, our architecture offers superior denoising performance while maintaining a constant memory footprint, enabling streaming operation. To enhance efficiency, we applied structured channel pruning, achieving an 8X reduction in model size without compromising audio quality. Our model demonstrates strong results in the Interspeech 2020 Deep Noise Suppression challenge. Specifically, CleanUMamba achieves a PESQ score of 2.42 and STOI of 95.1% with only 442K parameters and 468M MACs, matching or outperforming larger models in real-time performance. Code will be available at: https://github.com/lab-emi/CleanUMamba",
    "published": "2024-10-14",
    "link": "http://arxiv.org/abs/2410.11062v1",
    "code_url": null,
    "category": "mamba-mamba"
  },
  {
    "title": "V2M: Visual 2-Dimensional Mamba for Image Representation Learning",
    "author": "Chengkun Wang, Wenzhao Zheng, Yuanhui Huang, Jie Zhou, Jiwen Lu",
    "summary": "Mamba has garnered widespread attention due to its flexible design and efficient hardware performance to process 1D sequences based on the state space model (SSM). Recent studies have attempted to apply Mamba to the visual domain by flattening 2D images into patches and then regarding them as a 1D sequence. To compensate for the 2D structure information loss (e.g., local similarity) of the original image, most existing methods focus on designing different orders to sequentially process the tokens, which could only alleviate this issue to some extent. In this paper, we propose a Visual 2-Dimensional Mamba (V2M) model as a complete solution, which directly processes image tokens in the 2D space. We first generalize SSM to the 2-dimensional space which generates the next state considering two adjacent states on both dimensions (e.g., columns and rows). We then construct our V2M based on the 2-dimensional SSM formulation and incorporate Mamba to achieve hardware-efficient parallel processing. The proposed V2M effectively incorporates the 2D locality prior yet inherits the efficiency and input-dependent scalability of Mamba. Extensive experimental results on ImageNet classification and downstream visual tasks including object detection and instance segmentation on COCO and semantic segmentation on ADE20K demonstrate the effectiveness of our V2M compared with other visual backbones.",
    "published": "2024-10-14",
    "link": "http://arxiv.org/abs/2410.10382v1",
    "code_url": "https://github.com/wangck20/v2m",
    "category": "mamba-mamba"
  },
  {
    "title": "GlobalMamba: Global Image Serialization for Vision Mamba",
    "author": "Chengkun Wang, Wenzhao Zheng, Jie Zhou, Jiwen Lu",
    "summary": "Vision mambas have demonstrated strong performance with linear complexity to the number of vision tokens. Their efficiency results from processing image tokens sequentially. However, most existing methods employ patch-based image tokenization and then flatten them into 1D sequences for causal processing, which ignore the intrinsic 2D structural correlations of images. It is also difficult to extract global information by sequential processing of local patches. In this paper, we propose a global image serialization method to transform the image into a sequence of causal tokens, which contain global information of the 2D image. We first convert the image from the spatial domain to the frequency domain using Discrete Cosine Transform (DCT) and then arrange the pixels with corresponding frequency ranges. We further transform each set within the same frequency band back to the spatial domain to obtain a series of images before tokenization. We construct a vision mamba model, GlobalMamba, with a causal input format based on the proposed global image serialization, which can better exploit the causal relations among image sequences. Extensive experiments demonstrate the effectiveness of our GlobalMamba, including image classification on ImageNet-1K, object detection on COCO, and semantic segmentation on ADE20K.",
    "published": "2024-10-14",
    "link": "http://arxiv.org/abs/2410.10316v1",
    "code_url": "https://github.com/wangck20/globalmamba",
    "category": "mamba-mamba"
  },
  {
    "title": "Hi-Mamba: Hierarchical Mamba for Efficient Image Super-Resolution",
    "author": "Junbo Qiao, Jincheng Liao, Wei Li, Yulun Zhang, Yong Guo, Yi Wen, Zhangxizi Qiu, Jiao Xie, Jie Hu, Shaohui Lin",
    "summary": "State Space Models (SSM), such as Mamba, have shown strong representation ability in modeling long-range dependency with linear complexity, achieving successful applications from high-level to low-level vision tasks. However, SSM's sequential nature necessitates multiple scans in different directions to compensate for the loss of spatial dependency when unfolding the image into a 1D sequence. This multi-direction scanning strategy significantly increases the computation overhead and is unbearable for high-resolution image processing. To address this problem, we propose a novel Hierarchical Mamba network, namely, Hi-Mamba, for image super-resolution (SR). Hi-Mamba consists of two key designs: (1) The Hierarchical Mamba Block (HMB) assembled by a Local SSM (L-SSM) and a Region SSM (R-SSM) both with the single-direction scanning, aggregates multi-scale representations to enhance the context modeling ability. (2) The Direction Alternation Hierarchical Mamba Group (DA-HMG) allocates the isomeric single-direction scanning into cascading HMBs to enrich the spatial relationship modeling. Extensive experiments demonstrate the superiority of Hi-Mamba across five benchmark datasets for efficient SR. For example, Hi-Mamba achieves a significant PSNR improvement of 0.29 dB on Manga109 for $\\times3$ SR, compared to the strong lightweight MambaIR.",
    "published": "2024-10-14",
    "link": "http://arxiv.org/abs/2410.10140v1",
    "code_url": null,
    "category": "mamba-mamba"
  },
  {
    "title": "SlimSeiz: Efficient Channel-Adaptive Seizure Prediction Using a Mamba-Enhanced Network",
    "author": "Guorui Lu, Jing Peng, Bingyuan Huang, Chang Gao, Todor Stefanov, Yong Hao, Qinyu Chen",
    "summary": "Epileptic seizures cause abnormal brain activity, and their unpredictability can lead to accidents, underscoring the need for long-term seizure prediction. Although seizures can be predicted by analyzing electroencephalogram (EEG) signals, existing methods often require too many electrode channels or larger models, limiting mobile usability. This paper introduces a SlimSeiz framework that utilizes adaptive channel selection with a lightweight neural network model. SlimSeiz operates in two states: the first stage selects the optimal channel set for seizure prediction using machine learning algorithms, and the second stage employs a lightweight neural network based on convolution and Mamba for prediction. On the Children's Hospital Boston-MIT (CHB-MIT) EEG dataset, SlimSeiz can reduce channels from 22 to 8 while achieving a satisfactory result of 94.8% accuracy, 95.5% sensitivity, and 94.0% specificity with only 21.2K model parameters, matching or outperforming larger models' performance. We also validate SlimSeiz on a new EEG dataset, SRH-LEI, collected from Shanghai Renji Hospital, demonstrating its effectiveness across different patients. The code and SRH-LEI dataset are available at https://github.com/guoruilu/SlimSeiz.",
    "published": "2024-10-13",
    "link": "http://arxiv.org/abs/2410.09998v1",
    "code_url": null,
    "category": "mamba-mamba"
  },
  {
    "title": "Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models",
    "author": "Sathya Kamesh Bhethanabhotla, Omar Swelam, Julien Siems, David Salinas, Frank Hutter",
    "summary": "This paper introduces Mamba4Cast, a zero-shot foundation model for time series forecasting. Based on the Mamba architecture and inspired by Prior-data Fitted Networks (PFNs), Mamba4Cast generalizes robustly across diverse time series tasks without the need for dataset specific fine-tuning. Mamba4Cast's key innovation lies in its ability to achieve strong zero-shot performance on real-world datasets while having much lower inference times than time series foundation models based on the transformer architecture. Trained solely on synthetic data, the model generates forecasts for entire horizons in a single pass, outpacing traditional auto-regressive approaches. Our experiments show that Mamba4Cast performs competitively against other state-of-the-art foundation models in various data sets while scaling significantly better with the prediction length. The source code can be accessed at https://github.com/automl/Mamba4Cast.",
    "published": "2024-10-12",
    "link": "http://arxiv.org/abs/2410.09385v1",
    "code_url": null,
    "category": "mamba-mamba"
  },
  {
    "title": "Parameter-Efficient Fine-Tuning of State Space Models",
    "author": "Kevin Galim, Wonjun Kang, Yuchen Zeng, Hyung Il Koo, Kangwook Lee",
    "summary": "Deep State Space Models (SSMs), such as Mamba (Gu & Dao, 2024), have emerged as powerful tools for language modeling, offering high performance with efficient inference and linear scaling in sequence length. However, the application of parameter-efficient fine-tuning (PEFT) methods to SSM-based models remains largely unexplored. This paper aims to systematically study two key questions: (i) How do existing PEFT methods perform on SSM-based models? (ii) Which modules are most effective for fine-tuning? We conduct an empirical benchmark of four basic PEFT methods on SSM-based models. Our findings reveal that prompt-based methods (e.g., prefix-tuning) are no longer effective, an empirical result further supported by theoretical analysis. In contrast, LoRA remains effective for SSM-based models. We further investigate the optimal application of LoRA within these models, demonstrating both theoretically and experimentally that applying LoRA to linear projection matrices without modifying SSM modules yields the best results, as LoRA is not effective at tuning SSM modules. To further improve performance, we introduce LoRA with Selective Dimension tuning (SDLoRA), which selectively updates certain channels and states on SSM modules while applying LoRA to linear projection matrices. Extensive experimental results show that this approach outperforms standard LoRA.",
    "published": "2024-10-11",
    "link": "http://arxiv.org/abs/2410.09016v1",
    "code_url": "https://github.com/furiosa-ai/ssm-peft",
    "category": "mamba-mamba"
  },
  {
    "title": "Adaptive Prompt Learning with SAM for Few-shot Scanning Probe Microscope Image Segmentation",
    "author": "Yao Shen, Ziwei Wei, Chunmeng Liu, Shuming Wei, Qi Zhao, Kaiyang Zeng, Guangyao Li",
    "summary": "The Segment Anything Model (SAM) has demonstrated strong performance in image segmentation of natural scene images. However, its effectiveness diminishes markedly when applied to specific scientific domains, such as Scanning Probe Microscope (SPM) images. This decline in accuracy can be attributed to the distinct data distribution and limited availability of the data inherent in the scientific images. On the other hand, the acquisition of adequate SPM datasets is both time-intensive and laborious as well as skill-dependent. To address these challenges, we propose an Adaptive Prompt Learning with SAM (APL-SAM) framework tailored for few-shot SPM image segmentation. Our approach incorporates two key innovations to enhance SAM: 1) An Adaptive Prompt Learning module leverages few-shot embeddings derived from limited support set to learn adaptively central representatives, serving as visual prompts. This innovation eliminates the need for time-consuming online user interactions for providing prompts, such as exhaustively marking points and bounding boxes slice by slice; 2) A multi-source, multi-level mask decoder specifically designed for few-shot SPM image segmentation is introduced, which can effectively capture the correspondence between the support and query images. To facilitate comprehensive training and evaluation, we introduce a new dataset, SPM-Seg, curated for SPM image segmentation. Extensive experiments on this dataset reveal that the proposed APL-SAM framework significantly outperforms the original SAM, achieving over a 30% improvement in terms of Dice Similarity Coefficient with only one-shot guidance. Moreover, APL-SAM surpasses state-of-the-art few-shot segmentation methods and even fully supervised approaches in performance. Code and dataset used in this study will be made available upon acceptance.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12562v1",
    "code_url": null,
    "category": "mamba-seg"
  },
  {
    "title": "A Holistic Weakly Supervised Approach for Liver Tumor Segmentation with Clinical Knowledge-Informed Label Smoothing",
    "author": "Hairong Wang, Lingchao Mao, Zihan Zhang, Jing Li",
    "summary": "Liver cancer is a leading cause of mortality worldwide, and accurate CT-based tumor segmentation is essential for diagnosis and treatment. Manual delineation is time-intensive, prone to variability, and highlights the need for reliable automation. While deep learning has shown promise for automated liver segmentation, precise liver tumor segmentation remains challenging due to the heterogeneous nature of tumors, imprecise tumor margins, and limited labeled data. We present a novel holistic weakly supervised framework that integrates clinical knowledge to address these challenges with (1) A knowledge-informed label smoothing technique that leverages clinical data to generate smooth labels, which regularizes model training reducing the risk of overfitting and enhancing model performance; (2) A global and local-view segmentation framework, breaking down the task into two simpler sub-tasks, allowing optimized preprocessing and training for each; and (3) Pre- and post-processing pipelines customized to the challenges of each subtask, which enhances tumor visibility and refines tumor boundaries. We evaluated the proposed method on the HCC-TACE-Seg dataset and showed that these three key components complementarily contribute to the improved performance. Lastly, we prototyped a tool for automated liver tumor segmentation and diagnosis summary generation called MedAssistLiver. The app and code are published at https://github.com/lingchm/medassist-liver-cancer.",
    "published": "2024-10-13",
    "link": "http://arxiv.org/abs/2410.10005v1",
    "code_url": "https://github.com/lingchm/medassist-liver-cancer",
    "category": "mamba-seg"
  },
  {
    "title": "Iterative Optimization Annotation Pipeline and ALSS-YOLO-Seg for Efficient Banana Plantation Segmentation in UAV Imagery",
    "author": "Ang He, Ximei Wu, Xing Xu, Jing Chen, Xiaobin Guo, Sheng Xu",
    "summary": "Precise segmentation of Unmanned Aerial Vehicle (UAV)-captured images plays a vital role in tasks such as crop yield estimation and plant health assessment in banana plantations. By identifying and classifying planted areas, crop area can be calculated, which is indispensable for accurate yield predictions. However, segmenting banana plantation scenes requires a substantial amount of annotated data, and manual labeling of these images is both time-consuming and labor-intensive, limiting the development of large-scale datasets. Furthermore, challenges such as changing target sizes, complex ground backgrounds, limited computational resources, and correct identification of crop categories make segmentation even more difficult. To address these issues, we proposed a comprehensive solution. Firstly, we designed an iterative optimization annotation pipeline leveraging SAM2's zero-shot capabilities to generate high-quality segmentation annotations, thereby reducing the cost and time associated with data annotation significantly. Secondly, we developed ALSS-YOLO-Seg, an efficient lightweight segmentation model optimized for UAV imagery. The model's backbone includes an Adaptive Lightweight Channel Splitting and Shuffling (ALSS) module to improve information exchange between channels and optimize feature extraction, aiding accurate crop identification. Additionally, a Multi-Scale Channel Attention (MSCA) module combines multi-scale feature extraction with channel attention to tackle challenges of varying target sizes and complex ground backgrounds.",
    "published": "2024-10-09",
    "link": "http://arxiv.org/abs/2410.07955v1",
    "code_url": "https://github.com/helloworlder8/computer_vision",
    "category": "mamba-seg"
  },
  {
    "title": "Leveraging CAM Algorithms for Explaining Medical Semantic Segmentation",
    "author": "Tillmann Rheude, Andreas Wirtz, Arjan Kuijper, Stefan Wesarg",
    "summary": "Convolutional neural networks (CNNs) achieve prevailing results in segmentation tasks nowadays and represent the state-of-the-art for image-based analysis. However, the understanding of the accurate decision-making process of a CNN is rather unknown. The research area of explainable artificial intelligence (xAI) primarily revolves around understanding and interpreting this black-box behavior. One way of interpreting a CNN is the use of class activation maps (CAMs) that represent heatmaps to indicate the importance of image areas for the prediction of the CNN. For classification tasks, a variety of CAM algorithms exist. But for segmentation tasks, only one CAM algorithm for the interpretation of the output of a CNN exist. We propose a transfer between existing classification- and segmentation-based methods for more detailed, explainable, and consistent results which show salient pixels in semantic segmentation tasks. The resulting Seg-HiRes-Grad CAM is an extension of the segmentation-based Seg-Grad CAM with the transfer to the classification-based HiRes CAM. Our method improves the previously-mentioned existing segmentation-based method by adjusting it to recently published classification-based methods. Especially for medical image segmentation, this transfer solves existing explainability disadvantages.",
    "published": "2024-09-30",
    "link": "http://arxiv.org/abs/2409.20287v1",
    "code_url": "https://github.com/TillmannRheude/SegHiResGrad_CAM",
    "category": "mamba-seg"
  },
  {
    "title": "One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos",
    "author": "Zechen Bai, Tong He, Haiyang Mei, Pichao Wang, Ziteng Gao, Joya Chen, Lei Liu, Zheng Zhang, Mike Zheng Shou",
    "summary": "We introduce VideoLISA, a video-based multimodal large language model designed to tackle the problem of language-instructed reasoning segmentation in videos. Leveraging the reasoning capabilities and world knowledge of large language models, and augmented by the Segment Anything Model, VideoLISA generates temporally consistent segmentation masks in videos based on language instructions. Existing image-based methods, such as LISA, struggle with video tasks due to the additional temporal dimension, which requires temporal dynamic understanding and consistent segmentation across frames. VideoLISA addresses these challenges by integrating a Sparse Dense Sampling strategy into the video-LLM, which balances temporal context and spatial detail within computational constraints. Additionally, we propose a One-Token-Seg-All approach using a specially designed <TRK> token, enabling the model to segment and track objects across multiple frames. Extensive evaluations on diverse benchmarks, including our newly introduced ReasonVOS benchmark, demonstrate VideoLISA's superior performance in video object segmentation tasks involving complex reasoning, temporal understanding, and object tracking. While optimized for videos, VideoLISA also shows promising generalization to image segmentation, revealing its potential as a unified foundation model for language-instructed object segmentation. Code and model will be available at: https://github.com/showlab/VideoLISA.",
    "published": "2024-09-29",
    "link": "http://arxiv.org/abs/2409.19603v1",
    "code_url": null,
    "category": "mamba-seg"
  },
  {
    "title": "Diff-VPS: Video Polyp Segmentation via a Multi-task Diffusion Network with Adversarial Temporal Reasoning",
    "author": "Yingling Lu, Yijun Yang, Zhaohu Xing, Qiong Wang, Lei Zhu",
    "summary": "Diffusion Probabilistic Models have recently attracted significant attention in the community of computer vision due to their outstanding performance. However, while a substantial amount of diffusion-based research has focused on generative tasks, no work introduces diffusion models to advance the results of polyp segmentation in videos, which is frequently challenged by polyps' high camouflage and redundant temporal cues.In this paper, we present a novel diffusion-based network for video polyp segmentation task, dubbed as Diff-VPS. We incorporate multi-task supervision into diffusion models to promote the discrimination of diffusion models on pixel-by-pixel segmentation. This integrates the contextual high-level information achieved by the joint classification and detection tasks. To explore the temporal dependency, Temporal Reasoning Module (TRM) is devised via reasoning and reconstructing the target frame from the previous frames. We further equip TRM with a generative adversarial self-supervised strategy to produce more realistic frames and thus capture better dynamic cues. Extensive experiments are conducted on SUN-SEG, and the results indicate that our proposed Diff-VPS significantly achieves state-of-the-art performance. Code is available at https://github.com/lydia-yllu/Diff-VPS.",
    "published": "2024-09-11",
    "link": "http://arxiv.org/abs/2409.07238v1",
    "code_url": "https://github.com/lydia-yllu/diff-vps",
    "category": "mamba-seg"
  },
  {
    "title": "Seg-HGNN: Unsupervised and Light-Weight Image Segmentation with Hyperbolic Graph Neural Networks",
    "author": "Debjyoti Mondal, Rahul Mishra, Chandan Pandey",
    "summary": "Image analysis in the euclidean space through linear hyperspaces is well studied. However, in the quest for more effective image representations, we turn to hyperbolic manifolds. They provide a compelling alternative to capture complex hierarchical relationships in images with remarkably small dimensionality. To demonstrate hyperbolic embeddings' competence, we introduce a light-weight hyperbolic graph neural network for image segmentation, encompassing patch-level features in a very small embedding size. Our solution, Seg-HGNN, surpasses the current best unsupervised method by 2.5\\%, 4\\% on VOC-07, VOC-12 for localization, and by 0.8\\%, 1.3\\% on CUB-200, ECSSD for segmentation, respectively. With less than 7.5k trainable parameters, Seg-HGNN delivers effective and fast ($\\approx 2$ images/second) results on very standard GPUs like the GTX1650. This empirical evaluation presents compelling evidence of the efficacy and potential of hyperbolic representations for vision tasks.",
    "published": "2024-09-10",
    "link": "http://arxiv.org/abs/2409.06589v1",
    "code_url": null,
    "category": "mamba-seg"
  },
  {
    "title": "Segmenting Object Affordances: Reproducibility and Sensitivity to Scale",
    "author": "Tommaso Apicella, Alessio Xompero, Paolo Gastaldo, Andrea Cavallaro",
    "summary": "Visual affordance segmentation identifies image regions of an object an agent can interact with. Existing methods re-use and adapt learning-based architectures for semantic segmentation to the affordance segmentation task and evaluate on small-size datasets. However, experimental setups are often not reproducible, thus leading to unfair and inconsistent comparisons. In this work, we benchmark these methods under a reproducible setup on two single objects scenarios, tabletop without occlusions and hand-held containers, to facilitate future comparisons. We include a version of a recent architecture, Mask2Former, re-trained for affordance segmentation and show that this model is the best-performing on most testing sets of both scenarios. Our analysis shows that models are not robust to scale variations when object resolutions differ from those in the training set.",
    "published": "2024-09-03",
    "link": "http://arxiv.org/abs/2409.01814v1",
    "code_url": "https://github.com/apicis/aff-seg",
    "category": "mamba-seg"
  },
  {
    "title": "LSMS: Language-guided Scale-aware MedSegmentor for Medical Image Referring Segmentation",
    "author": "Shuyi Ouyang, Jinyang Zhang, Xiangye Lin, Xilai Wang, Qingqing Chen, Yen-Wei Chen, Lanfen Lin",
    "summary": "Conventional medical image segmentation methods have been found inadequate in facilitating physicians with the identification of specific lesions for diagnosis and treatment. Given the utility of text as an instructional format, we introduce a novel task termed Medical Image Referring Segmentation (MIRS), which requires segmenting specified lesions in images based on the given language expressions. Due to the varying object scales in medical images, MIRS demands robust vision-language modeling and comprehensive multi-scale interaction for precise localization and segmentation under linguistic guidance. However, existing medical image segmentation methods fall short in meeting these demands, resulting in insufficient segmentation accuracy. In response, we propose an approach named Language-guided Scale-aware MedSegmentor (LSMS), incorporating two appealing designs: (1)~a Scale-aware Vision-Language Attention module that leverages diverse convolutional kernels to acquire rich visual knowledge and interact closely with linguistic features, thereby enhancing lesion localization capability; (2)~a Full-Scale Decoder that globally models multi-modal features across various scales, capturing complementary information between scales to accurately outline lesion boundaries. Addressing the lack of suitable datasets for MIRS, we constructed a vision-language medical dataset called Reference Hepatic Lesion Segmentation (RefHL-Seg). This dataset comprises 2,283 abdominal CT slices from 231 cases, with corresponding textual annotations and segmentation masks for various liver lesions in images. We validated the performance of LSMS for MIRS and conventional medical image segmentation tasks across various datasets. Our LSMS consistently outperforms on all datasets with lower computational costs. The code and datasets will be released.",
    "published": "2024-08-30",
    "link": "http://arxiv.org/abs/2408.17347v2",
    "code_url": null,
    "category": "mamba-seg"
  },
  {
    "title": "LV-UNet: A Lightweight and Vanilla Model for Medical Image Segmentation",
    "author": "Juntao Jiang, Mengmeng Wang, Huizhong Tian, Lingbo Cheng, Yong Liu",
    "summary": "Although the progress made by large models in computer vision, optimization challenges, the complexity of transformer models, computational limitations, and the requirements of practical applications call for simpler designs in model architecture for medical image segmentation, especially in mobile medical devices that require lightweight and deployable models with real-time performance. However, some of the current lightweight models exhibit poor robustness across different datasets, which hinders their broader adoption. This paper proposes a lightweight and vanilla model called LV-UNet, which effectively utilizes pre-trained MobileNetv3-Large models and introduces fusible modules. It can be trained using an improved deep training strategy and switched to deployment mode during inference, reducing both parameter count and computational load. Experiments are conducted on ISIC 2016, BUSI, CVC- ClinicDB, CVC-ColonDB, and Kvair-SEG datasets, achieving better performance compared to the state-of-the-art and classic models.",
    "published": "2024-08-29",
    "link": "http://arxiv.org/abs/2408.16886v1",
    "code_url": null,
    "category": "mamba-seg"
  }
]