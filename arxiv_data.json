[
  {
    "title": "Dual Prototype Evolving for Test-Time Generalization of Vision-Language Models",
    "author": "Ce Zhang, Simon Stepputtis, Katia Sycara, Yaqi Xie",
    "summary": "Test-time adaptation, which enables models to generalize to diverse data with unlabeled test samples, holds significant value in real-world scenarios. Recently, researchers have applied this setting to advanced pre-trained vision-language models (VLMs), developing approaches such as test-time prompt tuning to further extend their practical applicability. However, these methods typically focus solely on adapting VLMs from a single modality and fail to accumulate task-specific knowledge as more samples are processed. To address this, we introduce Dual Prototype Evolving (DPE), a novel test-time adaptation approach for VLMs that effectively accumulates task-specific knowledge from multi-modalities. Specifically, we create and evolve two sets of prototypes--textual and visual--to progressively capture more accurate multi-modal representations for target classes during test time. Moreover, to promote consistent multi-modal representations, we introduce and optimize learnable residuals for each test sample to align the prototypes from both modalities. Extensive experimental results on 15 benchmark datasets demonstrate that our proposed DPE consistently outperforms previous state-of-the-art methods while also exhibiting competitive computational efficiency. Code is available at https://github.com/zhangce01/DPE-CLIP.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12790v1",
    "code_url": null
  },
  {
    "title": "Long-LRM: Long-sequence Large Reconstruction Model for Wide-coverage Gaussian Splats",
    "author": "Chen Ziwen, Hao Tan, Kai Zhang, Sai Bi, Fujun Luan, Yicong Hong, Li Fuxin, Zexiang Xu",
    "summary": "We propose Long-LRM, a generalizable 3D Gaussian reconstruction model that is capable of reconstructing a large scene from a long sequence of input images. Specifically, our model can process 32 source images at 960x540 resolution within only 1.3 seconds on a single A100 80G GPU. Our architecture features a mixture of the recent Mamba2 blocks and the classical transformer blocks which allowed many more tokens to be processed than prior work, enhanced by efficient token merging and Gaussian pruning steps that balance between quality and efficiency. Unlike previous feed-forward models that are limited to processing 1~4 input images and can only reconstruct a small portion of a large scene, Long-LRM reconstructs the entire scene in a single feed-forward step. On large-scale scene datasets such as DL3DV-140 and Tanks and Temples, our method achieves performance comparable to optimization-based approaches while being two orders of magnitude more efficient. Project page: https://arthurhero.github.io/projects/llrm",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12781v1",
    "code_url": null
  },
  {
    "title": "Collimated $ \u03b3$-flash emission along the target surface irradiated by a laser at non-grazing incidence",
    "author": "M. Matys, P. Hadjisolomou, R. Shaisultanov, P. Valenta, M. Lama\u010d, T. M. Jeong, J. P. Thistlewood, C. P. Ridgers, A. S. Pirozhkov, S. V. Bulanov",
    "summary": "The interaction of a high-power laser with a solid target provides ways to produce beams of $\\gamma$-photons. For normal incidence of the laser on the target the beams usually appear in a form of two lobes, which are symmetric with respect to the laser propagation axis. In this work we demonstrate via three-dimensional particle-in-cell simulations a regime where for oblique incidence the emission of a collimated $\\gamma$-photon beam is in the direction parallel to the target surface. The process is ascribed to the interference pattern in the electromagnetic field formed by the incident and reflected laser pulse. The electromagnetic field accelerates electrons to the GeV energy level, while temporarily directing their momentum along the target surface. Consequently, they emit a collimated $\\gamma$-photon beam in the same direction. The dependencies of $\\gamma$-photon emission on the incident angle, laser pulse polarization, power and duration and target thickness are also addressed in the paper. The beam directionality is important for designing future experiments. In addition, this setup causes the generation of high-order harmonics propagating along the target surface.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12780v1",
    "code_url": null
  },
  {
    "title": "Measuring time-resolved heat transfer fluctuations on a heated-thin foil in a turbulent channel airflow",
    "author": "Antonio Cu\u00e9llar, Enrico Amico, Jacopo Serpieri, Gioacchino Cafiero, Woutijn J Baars, Stefano Discetti, Andrea Ianiro",
    "summary": "We present an experimental setup to perform time-resolved convective heat transfer measurements in a turbulent channel flow with air as the working fluid. We employ a heated thin foil coupled with high-speed infrared thermography. The measurement technique is challenged by the thermal inertia of the foil, the high frequency of turbulent fluctuations, and the measurement noise of the infrared camera. We discuss in detail the advantages and drawbacks of all the design choices that were made, thereby providing a successful implementation strategy to obtain high-quality data. This experimental approach could be useful for experimental studies employing wall-based measurements of turbulence, such as flow control applications in wall-bounded turbulence.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12778v1",
    "code_url": null
  },
  {
    "title": "Should exponential integrators be used for advection-dominated problems?",
    "author": "Lukas Einkemmer, Trung-Hau Hoang, Alexander Ostermann",
    "summary": "In this paper, we consider the application of exponential integrators to problems that are advection dominated, either on the entire or on a subset of the domain. In this context, we compare Leja and Krylov based methods to compute the action of exponential and related matrix functions. We set up a performance model by counting the different operations needed to implement the considered algorithms. This model assumes that the evaluation of the right-hand side is memory bound and allows us to evaluate performance in a hardware independent way. We find that exponential integrators perform comparably to explicit Runge-Kutta schemes for problems that are advection dominated in the entire domain. Moreover, they are able to outperform explicit methods in situations where small parts of the domain are diffusion dominated. We generally observe that Leja based methods outperform Krylov iterations in the problems considered. This is in particular true if computing inner products is expensive.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12765v1",
    "code_url": null
  },
  {
    "title": "Polarization options in inclusive DIS off tensor polarized deuteron",
    "author": "Wim Cosyn, Brandon Roldan Tomei, Alan Sosa, Allison Zec",
    "summary": "In the near future, the Jefferson Lab $b_1$ experiment will provide the second measurement of tensor polarized asymmetries in inclusive DIS on the deuteron. In this asymmetry, 4 independent tensor polarized structure functions contribute. This necessitates systematic approximations in the extraction of the leading twist structure function $b_1$ from a single tensor asymmetry measurement. Contamination from higher twist structure functions and kinematic effects is discussed here. Using a deuteron convolution model, we quantify the systematic errors from these approximations for two different choices for the target polarization direction (momentum transfer, electron beam direction). For Jefferson Lab 12 GeV kinematics, the systematic error turns out to be comparable between the two polarization options, while at higher $Q^2$ values the momentum transfer direction is preferred.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12764v1",
    "code_url": null
  },
  {
    "title": "SAFREE: Training-Free and Adaptive Guard for Safe Text-to-Image And Video Generation",
    "author": "Jaehong Yoon, Shoubin Yu, Vaidehi Patil, Huaxiu Yao, Mohit Bansal",
    "summary": "Recent advances in diffusion models have significantly enhanced their ability to generate high-quality images and videos, but they have also increased the risk of producing unsafe content. Existing unlearning/editing-based methods for safe generation remove harmful concepts from models but face several challenges: (1) They cannot instantly remove harmful concepts without training. (2) Their safe generation capabilities depend on collected training data. (3) They alter model weights, risking degradation in quality for content unrelated to toxic concepts. To address these, we propose SAFREE, a novel, training-free approach for safe T2I and T2V, that does not alter the model's weights. Specifically, we detect a subspace corresponding to a set of toxic concepts in the text embedding space and steer prompt embeddings away from this subspace, thereby filtering out harmful content while preserving intended semantics. To balance the trade-off between filtering toxicity and preserving safe concepts, SAFREE incorporates a novel self-validating filtering mechanism that dynamically adjusts the denoising steps when applying the filtered embeddings. Additionally, we incorporate adaptive re-attention mechanisms within the diffusion latent space to selectively diminish the influence of features related to toxic concepts at the pixel level. In the end, SAFREE ensures coherent safety checking, preserving the fidelity, quality, and safety of the output. SAFREE achieves SOTA performance in suppressing unsafe content in T2I generation compared to training-free baselines and effectively filters targeted concepts while maintaining high-quality images. It also shows competitive results against training-based methods. We extend SAFREE to various T2I backbones and T2V tasks, showcasing its flexibility and generalization. SAFREE provides a robust and adaptable safeguard for ensuring safe visual generation.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12761v1",
    "code_url": null
  },
  {
    "title": "Unitary Multi-Margin BERT for Robust Natural Language Processing",
    "author": "Hao-Yuan Chang, Kang L. Wang",
    "summary": "Recent developments in adversarial attacks on deep learning leave many mission-critical natural language processing (NLP) systems at risk of exploitation. To address the lack of computationally efficient adversarial defense methods, this paper reports a novel, universal technique that drastically improves the robustness of Bidirectional Encoder Representations from Transformers (BERT) by combining the unitary weights with the multi-margin loss. We discover that the marriage of these two simple ideas amplifies the protection against malicious interference. Our model, the unitary multi-margin BERT (UniBERT), boosts post-attack classification accuracies significantly by 5.3% to 73.8% while maintaining competitive pre-attack accuracies. Furthermore, the pre-attack and post-attack accuracy tradeoff can be adjusted via a single scalar parameter to best fit the design requirements for the target applications.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12759v1",
    "code_url": null
  },
  {
    "title": "Toleo: Scaling Freshness to Tera-scale Memory using CXL and PIM",
    "author": "Juechu Dong, Jonah Rosenblum, Satish Narayanasamy",
    "summary": "Trusted hardware's freshness guarantee ensures that an adversary cannot replay an old value in response to a memory read request. They rely on maintaining a version number for each cache block and ensuring their integrity using a Merkle tree. However, these existing solutions protect only a small amount of main memory (few MBs), as the extraneous memory accesses to the Merkle tree increase prohibitively with the protected memory size. We present Toleo, which uses trusted smart memory connected through a secure CXL IDE network to safely store version numbers. Toleo eliminates the need for an unscalable Merkle tree to protect the integrity of version numbers by instead using smart memory as the root of trust. Additionally, Toleo ensures version confidentiality which enables stealth versions that reduce the version storage overhead in half.   Furthermore, in the absence of Merkle tree imposed constraints, we effectively exploit version locality at page granularity to compress version number by a factor of 240. These space optimizations make it feasible for one 168 GB Toleo smart memory device to provide freshness to a 28 TB CXL-expanded main memory pool in a rack server for a negligible performance overhead. We analyze the benefits of Toleo using several privacy-sensitive genomics, graph, generative AI, and database workloads.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12749v1",
    "code_url": null
  },
  {
    "title": "DRIP: A Versatile Family of Space-Time ISAC Waveforms",
    "author": "Dexin Wang, Ahmad Bazzi, Marwa Chafii",
    "summary": "The following paper introduces Dual beam-similarity awaRe Integrated sensing and communications (ISAC) with controlled Peak-to-average power ratio (DRIP) waveforms. DRIP is a novel family of space-time ISAC waveforms designed for dynamic peak-to-average power ratio (PAPR) adjustment. The proposed DRIP waveforms are designed to conform to specified PAPR levels while exhibiting beampattern properties, effectively targeting multiple desired directions and suppressing interference for multi-target sensing applications, while closely resembling radar chirps. For communication purposes, the proposed DRIP waveforms aim to minimize multi-user interference across various constellations. Addressing the non-convexity of the optimization framework required for generating DRIP waveforms, we introduce a block cyclic coordinate descent algorithm. This iterative approach ensures convergence to an optimal ISAC waveform solution. Simulation results validate the DRIP waveforms' superior performance, versatility, and favorable ISAC trade-offs, highlighting their potential in advanced multi-target sensing and communication systems.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12746v1",
    "code_url": null
  },
  {
    "title": "Gravitational instantons and the quality problem of the QCD axion: Facts, speculations, and statements in between",
    "author": "Pier Giuseppe Catinari, Alfredo Urbano",
    "summary": "In this work, we critically reanalyze the explicit breaking of the Peccei-Quinn global symmetry -- and the corresponding corrections to the QCD axion potential -- induced by gravity. Specifically, we examine the role of gravitational instantons, which are non-perturbative, finite-action solutions to the Euclidean Einstein equations. These instantons represent topologically nontrivial configurations of spacetime and are analogous to instantons in gauge theory. The amount of symmetry breaking induced by gravitational instantons can be computed in a controlled way within the framework of semi-classical gravity, using 't Hooft operators, in full analogy to the computation of the axion potential arising from QCD small instanton effects. Contrary to previous results in the literature, we find that the effects of gravitational instantons are extremely small and therefore do not give rise to a significant quality problem for the axion solution to the strong CP problem, both within the Standard Model and in beyond-the-Standard-Model scenarios that involve multiple copies of the Standard Model. In conclusion, we argue that, assuming the ultraviolet completion of gravity is weakly coupled, the axion solution to the strong CP problem remains free from any quality issues due to gravity. Along the way, we derive the effective Lagrangian of the QCD axion, including its gravitational coupling.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12741v1",
    "code_url": null
  },
  {
    "title": "Analytical Study of Surface Plasmon-Phonon Polaritons in Nonlinear-Graphene-LiF Heterostructures in the far-infrared region",
    "author": "Mohammad Bagher Heydari, Ali Abdollahi, Sina Asgari",
    "summary": "In this paper, a new heterostructure based on the hybridization of graphene-LiF layers with a nonlinear material is introduced and studied. The numerical results are depicted and discussed in detail. A high value of FOM (FOM=24.5) at the frequency of 9.22 THz is reported for the chemical potential of 0.2 ev. Our results show that the propagation features of the proposed structure can be varied by the graphene parameters and the nonlinearity inside and outside the phononic band. The Hybridization of graphene with a nonlinear medium and a polar dielectric like LiF can support high levels of confinement with low optical loss, which makes this platform a unique candidate for THz applications.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12739v1",
    "code_url": null
  },
  {
    "title": "CREAM: Consistency Regularized Self-Rewarding Language Models",
    "author": "Zhaoyang Wang, Weilei He, Zhiyuan Liang, Xuchao Zhang, Chetan Bansal, Ying Wei, Weitong Zhang, Huaxiu Yao",
    "summary": "Recent self-rewarding large language models (LLM) have successfully applied LLM-as-a-Judge to iteratively improve the alignment performance without the need of human annotations for preference data. These methods commonly utilize the same LLM to act as both the policy model (which generates responses) and the reward model (which scores and ranks those responses). The ranked responses are then used as preference pairs to train the LLM via direct alignment technologies (e.g. DPO). However, it is noteworthy that throughout this process, there is no guarantee of accuracy in the rewarding and ranking, which is critical for ensuring accurate rewards and high-quality preference data. Empirical results from relatively small LLMs (e.g., 7B parameters) also indicate that improvements from self-rewarding may diminish after several iterations in certain situations, which we hypothesize is due to accumulated bias in the reward system. This bias can lead to unreliable preference data for training the LLM. To address this issue, we first formulate and analyze the generalized iterative preference fine-tuning framework for self-rewarding language model. We then introduce the regularization to this generalized framework to mitigate the overconfident preference labeling in the self-rewarding process. Based on this theoretical insight, we propose a Consistency Regularized sElf-rewarding lAnguage Model (CREAM) that leverages the rewarding consistency across different iterations to regularize the self-rewarding training, helping the model to learn from more reliable preference data. With this explicit regularization, our empirical results demonstrate the superiority of CREAM in improving both reward consistency and alignment performance. The code is publicly available at https://github.com/Raibows/CREAM.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12735v1",
    "code_url": null
  },
  {
    "title": "The bulk one-arm exponent for the CLE$_{\u03ba'}$ percolations",
    "author": "Haoyu Liu, Xin Sun, Pu Yu, Zijie Zhuang",
    "summary": "The conformal loop ensemble (CLE) is a conformally invariant random collection of loops. In the non-simple regime $\\kappa'\\in (4,8)$, it describes the scaling limit of the critical Fortuin-Kasteleyn (FK) percolations. CLE percolations were introduced by Miller-Sheffield-Werner (2017). The CLE$_{\\kappa'}$ percolations describe the scaling limit of a natural variant of the FK percolation called the fuzzy Potts model, which has an additional percolation parameter $r$. Based on CLE percolations and assuming that the convergence of the FK percolation to CLE, K{\\\"o}hler-Schindler and Lehmkuehler (2022) derived all the arm exponents for the fuzzy Potts model except the bulk one-arm exponent. In this paper, we exactly solve this exponent, which prescribes the dimension of the clusters in CLE$_{\\kappa'}$ percolations. As a special case, the bichromatic one-arm exponent for the critical 3-state Potts model should be $4/135$. To the best of our knowledge, this natural exponent was not predicted in physics. Our derivation relies on the iterative construction of CLE percolations from the boundary conformal loop ensemble (BCLE), and the coupling between Liouville quantum gravity and SLE curves. The source of the exact solvability comes from the structure constants of boundary Liouville conformal field theory. A key technical step is to prove a conformal welding result for the target-invariant radial SLE curves. As intermediate steps in our derivation, we obtain several exact results for BCLE in both the simple and non-simple regimes, which extend results of Ang-Sun-Yu-Zhuang (2024) on the touching probability of non-simple CLE. This also provides an alternative derivation of the relation between the BCLE parameter $\\rho$ and the additional percolation parameter $r$ in CLE percolations, which was originally due to Miller-Sheffield-Werner (2021, 2022).",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12724v1",
    "code_url": null
  },
  {
    "title": "How Does Variance Shape the Regret in Contextual Bandits?",
    "author": "Zeyu Jia, Jian Qian, Alexander Rakhlin, Chen-Yu Wei",
    "summary": "We consider realizable contextual bandits with general function approximation, investigating how small reward variance can lead to better-than-minimax regret bounds. Unlike in minimax bounds, we show that the eluder dimension $d_\\text{elu}$$-$a complexity measure of the function class$-$plays a crucial role in variance-dependent bounds. We consider two types of adversary:   (1) Weak adversary: The adversary sets the reward variance before observing the learner's action. In this setting, we prove that a regret of $\\Omega(\\sqrt{\\min\\{A,d_\\text{elu}\\}\\Lambda}+d_\\text{elu})$ is unavoidable when $d_{\\text{elu}}\\leq\\sqrt{AT}$, where $A$ is the number of actions, $T$ is the total number of rounds, and $\\Lambda$ is the total variance over $T$ rounds. For the $A\\leq d_\\text{elu}$ regime, we derive a nearly matching upper bound $\\tilde{O}(\\sqrt{A\\Lambda}+d_\\text{elu})$ for the special case where the variance is revealed at the beginning of each round.   (2) Strong adversary: The adversary sets the reward variance after observing the learner's action. We show that a regret of $\\Omega(\\sqrt{d_\\text{elu}\\Lambda}+d_\\text{elu})$ is unavoidable when $\\sqrt{d_\\text{elu}\\Lambda}+d_\\text{elu}\\leq\\sqrt{AT}$. In this setting, we provide an upper bound of order $\\tilde{O}(d_\\text{elu}\\sqrt{\\Lambda}+d_\\text{elu})$.   Furthermore, we examine the setting where the function class additionally provides distributional information of the reward, as studied by Wang et al. (2024). We demonstrate that the regret bound $\\tilde{O}(\\sqrt{d_\\text{elu}\\Lambda}+d_\\text{elu})$ established in their work is unimprovable when $\\sqrt{d_{\\text{elu}}\\Lambda}+d_\\text{elu}\\leq\\sqrt{AT}$. However, with a slightly different definition of the total variance and with the assumption that the reward follows a Gaussian distribution, one can achieve a regret of $\\tilde{O}(\\sqrt{A\\Lambda}+d_\\text{elu})$.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12713v1",
    "code_url": null
  },
  {
    "title": "A Gamma-ray Stacking Survey of Fermi-LAT Undetected Globular Clusters",
    "author": "Owen K. Henry, Timothy A. D. Paglione, Yuzhe Song, Joshua Tan, David Zurek, Vanessa Pinto",
    "summary": "We present evidence for $\\gamma$-ray emission from a stacked population of 39 high-latitude globular clusters (GCs) not detected in the Fermi Point Source Catalog, likely attributable to populations of millisecond pulsars within them. In this work, we use 13 years of data collected by the Large Area Telescope aboard the Fermi Gamma-Ray Space Telescope to search for a cumulative signal from undetected GCs and compared them to control fields (CFs), selected to match the celestial distribution of the target clusters so as to distinguish the $\\gamma$-ray signal from background emission. The joint likelihood distribution of the GCs has a significant separation ($\\sim4\\sigma$) from that of the CFs. We also investigate correlations between detected cluster luminosities and other cluster properties such as distance, the number of millisecond pulsars associated with each cluster, and stellar encounter rate but find no significant relationships.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12711v1",
    "code_url": null
  },
  {
    "title": "AdaptiveDrag: Semantic-Driven Dragging on Diffusion-Based Image Editing",
    "author": "DuoSheng Chen, Binghui Chen, Yifeng Geng, Liefeng Bo",
    "summary": "Recently, several point-based image editing methods (e.g., DragDiffusion, FreeDrag, DragNoise) have emerged, yielding precise and high-quality results based on user instructions. However, these methods often make insufficient use of semantic information, leading to less desirable results. In this paper, we proposed a novel mask-free point-based image editing method, AdaptiveDrag, which provides a more flexible editing approach and generates images that better align with user intent. Specifically, we design an auto mask generation module using super-pixel division for user-friendliness. Next, we leverage a pre-trained diffusion model to optimize the latent, enabling the dragging of features from handle points to target points. To ensure a comprehensive connection between the input image and the drag process, we have developed a semantic-driven optimization. We design adaptive steps that are supervised by the positions of the points and the semantic regions derived from super-pixel segmentation. This refined optimization process also leads to more realistic and accurate drag results. Furthermore, to address the limitations in the generative consistency of the diffusion model, we introduce an innovative corresponding loss during the sampling process. Building on these effective designs, our method delivers superior generation results using only the single input image and the handle-target point pairs. Extensive experiments have been conducted and demonstrate that the proposed method outperforms others in handling various drag instructions (e.g., resize, movement, extension) across different domains (e.g., animals, human face, land space, clothing).",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12696v1",
    "code_url": null
  },
  {
    "title": "Area measures and branched polymers in supercritical Liouville quantum gravity",
    "author": "Manan Bhatia, Ewain Gwynne, Jinwoo Sung",
    "summary": "We study Liouville quantum gravity (LQG) in the supercritical (a.k.a. strongly coupled) phase, which has background charge $Q \\in (0,2)$ and central charge $\\mathbf{c}_{\\mathrm{L}} = 1+6Q^2 \\in (1,25)$. Recent works have shown how to define LQG in this phase as a planar random geometry associated with a variant of the Gaussian free field, which exhibits \"infinite spikes.\" In contrast, a number of results from physics, dating back to the 1980s, suggest that supercritical LQG surfaces should behave like \"branched polymers\": i.e., they should look like the continuum random tree. We prove a result which reconciles these two descriptions of supercritical LQG. More precisely, we show that for a family of random planar maps with boundary in the universality class of supercritical LQG, if we condition on the (small probability) event that the planar map is finite, then the scaling limit is the continuum random tree. We also show that there does not exist any locally finite measure associated with supercritical LQG which is locally determined by the field and satisfies the LQG coordinate change formula. Our proofs are based on a branching process description of supercritical LQG which comes from its coupling with CLE$_4$ (Ang and Gwynne, arXiv:2308.11832).",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12693v1",
    "code_url": null
  },
  {
    "title": "Local transfer learning Gaussian process modeling, with applications to surrogate modeling of expensive computer simulators",
    "author": "Xinming Wang, Simon Mak, John Miller, Jianguo Wu",
    "summary": "A critical bottleneck for scientific progress is the costly nature of computer simulations for complex systems. Surrogate models provide an appealing solution: such models are trained on simulator evaluations, then used to emulate and quantify uncertainty on the expensive simulator at unexplored inputs. In many applications, one often has available data on related systems. For example, in designing a new jet turbine, there may be existing studies on turbines with similar configurations. A key question is how information from such \"source\" systems can be transferred for effective surrogate training on the \"target\" system of interest. We thus propose a new LOcal transfer Learning Gaussian Process (LOL-GP) model, which leverages a carefully-designed Gaussian process to transfer such information for surrogate modeling. The key novelty of the LOL-GP is a latent regularization model, which identifies regions where transfer should be performed and regions where it should be avoided. This \"local transfer\" property is desirable in scientific systems: at certain parameters, such systems may behave similarly and thus transfer is beneficial; at other parameters, they may behave differently and thus transfer is detrimental. By accounting for local transfer, the LOL-GP can rectify a critical limitation of \"negative transfer\" in existing transfer learning models, where the transfer of information worsens predictive performance. We derive a Gibbs sampling algorithm for efficient posterior predictive sampling on the LOL-GP, for both the multi-source and multi-fidelity transfer settings. We then show, via a suite of numerical experiments and an application for jet turbine design, the improved surrogate performance of the LOL-GP over existing methods.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12690v1",
    "code_url": null
  },
  {
    "title": "A spatial hypergraph model where epidemic spread demonstrates clear higher-order effects",
    "author": "Omar Eldaghar, Yu Zhu, David Gleich",
    "summary": "We demonstrate a spatial hypergraph model that allows us to vary the amount of higher-order structure in the generated hypergraph. Specifically, we can vary from a model that is a pure pairwise graph into a model that is almost a pure hypergraph. We use this spatial hypergraph model to study higher-order effects in epidemic spread. We use a susceptible-infected-recovered-susceptible (SIRS) epidemic model designed to mimic the spread of an airborne pathogen. We study three types of airborne effects that emulate airborne dilution effects. For the scenario of linear dilution, which roughly correspond to constant ventilation per person as required in many building codes, we see essentially no impact from introducing small hyperedges up to size 15 whereas we do see effects when the hyperedge set is dominated by large hyperedges. Specifically, we track the mean infections after the SIRS epidemic has run for awhile so it is in a \"steady state\" and find the mean is higher in the large hyperedge regime wheras it is unchanged from pairwise to small hyperedge regime.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12688v1",
    "code_url": null
  },
  {
    "title": "WorldMedQA-V: a multilingual, multimodal medical examination dataset for multimodal language models evaluation",
    "author": "Jo\u00e3o Matos, Shan Chen, Siena Placino, Yingya Li, Juan Carlos Climent Pardo, Daphna Idan, Takeshi Tohyama, David Restrepo, Luis F. Nakayama, Jose M. M. Pascual-Leone, Guergana Savova, Hugo Aerts, Leo A. Celi, A. Ian Wong, Danielle S. Bitterman, Jack Gallifant",
    "summary": "Multimodal/vision language models (VLMs) are increasingly being deployed in healthcare settings worldwide, necessitating robust benchmarks to ensure their safety, efficacy, and fairness. Multiple-choice question and answer (QA) datasets derived from national medical examinations have long served as valuable evaluation tools, but existing datasets are largely text-only and available in a limited subset of languages and countries. To address these challenges, we present WorldMedQA-V, an updated multilingual, multimodal benchmarking dataset designed to evaluate VLMs in healthcare. WorldMedQA-V includes 568 labeled multiple-choice QAs paired with 568 medical images from four countries (Brazil, Israel, Japan, and Spain), covering original languages and validated English translations by native clinicians, respectively. Baseline performance for common open- and closed-source models are provided in the local language and English translations, and with and without images provided to the model. The WorldMedQA-V benchmark aims to better match AI systems to the diverse healthcare environments in which they are deployed, fostering more equitable, effective, and representative applications.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12722v1",
    "code_url": null
  },
  {
    "title": "VividMed: Vision Language Model with Versatile Visual Grounding for Medicine",
    "author": "Lingxiao Luo, Bingda Tang, Xuanzhong Chen, Rong Han, Ting Chen",
    "summary": "Recent advancements in Vision Language Models (VLMs) have demonstrated remarkable promise in generating visually grounded responses. However, their application in the medical domain is hindered by unique challenges. For instance, most VLMs rely on a single method of visual grounding, whereas complex medical tasks demand more versatile approaches. Additionally, while most VLMs process only 2D images, a large portion of medical images are 3D. The lack of medical data further compounds these obstacles. To address these challenges, we present VividMed, a vision language model with versatile visual grounding for medicine. Our model supports generating both semantic segmentation masks and instance-level bounding boxes, and accommodates various imaging modalities, including both 2D and 3D data. We design a three-stage training procedure and an automatic data synthesis pipeline based on open datasets and models. Besides visual grounding tasks, VividMed also excels in other common downstream tasks, including Visual Question Answering (VQA) and report generation. Ablation studies empirically show that the integration of visual grounding ability leads to improved performance on these tasks. Our code is publicly available at https://github.com/function2-llx/MMMM.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12694v1",
    "code_url": null
  },
  {
    "title": "Machine Learning Approach to Brain Tumor Detection and Classification",
    "author": "Alice Oh, Inyoung Noh, Jian Choo, Jihoo Lee, Justin Park, Kate Hwang, Sanghyeon Kim, Soo Min Oh",
    "summary": "Brain tumor detection and classification are critical tasks in medical image analysis, particularly in early-stage diagnosis, where accurate and timely detection can significantly improve treatment outcomes. In this study, we apply various statistical and machine learning models to detect and classify brain tumors using brain MRI images. We explore a variety of statistical models including linear, logistic, and Bayesian regressions, and the machine learning models including decision tree, random forest, single-layer perceptron, multi-layer perceptron, convolutional neural network (CNN), recurrent neural network, and long short-term memory. Our findings show that CNN outperforms other models, achieving the best performance. Additionally, we confirm that the CNN model can also work for multi-class classification, distinguishing between four categories of brain MRI images such as normal, glioma, meningioma, and pituitary tumor images. This study demonstrates that machine learning approaches are suitable for brain tumor detection and classification, facilitating real-world medical applications in assisting radiologists with early and accurate diagnosis.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12692v1",
    "code_url": null
  },
  {
    "title": "Automatic Mapping of Anatomical Landmarks from Free-Text Using Large Language Models: Insights from Llama-2",
    "author": "Mohamad Abdi, Gerardo Hemosillo Valadez, Halid Ziya Yerebakan",
    "summary": "Anatomical landmarks are vital in medical imaging for navigation and anomaly detection. Modern large language models (LLMs), like Llama-2, offer promise for automating the mapping of these landmarks in free-text radiology reports to corresponding positions in image data. Recent studies propose LLMs may develop coherent representations of generative processes. Motivated by these insights, we investigated whether LLMs accurately represent the spatial positions of anatomical landmarks. Through experiments with Llama-2 models, we found that they can linearly represent anatomical landmarks in space with considerable robustness to different prompts. These results underscore the potential of LLMs to enhance the efficiency and accuracy of medical imaging workflows.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12686v1",
    "code_url": null
  },
  {
    "title": "Cascade learning in multi-task encoder-decoder networks for concurrent bone segmentation and glenohumeral joint assessment in shoulder CT scans",
    "author": "Luca Marsilio, Davide Marzorati, Matteo Rossi, Andrea Moglia, Luca Mainardi, Alfonso Manzotti, Pietro Cerveri",
    "summary": "Osteoarthritis is a degenerative condition affecting bones and cartilage, often leading to osteophyte formation, bone density loss, and joint space narrowing. Treatment options to restore normal joint function vary depending on the severity of the condition. This work introduces an innovative deep-learning framework processing shoulder CT scans. It features the semantic segmentation of the proximal humerus and scapula, the 3D reconstruction of bone surfaces, the identification of the glenohumeral (GH) joint region, and the staging of three common osteoarthritic-related pathologies: osteophyte formation (OS), GH space reduction (JS), and humeroscapular alignment (HSA). The pipeline comprises two cascaded CNN architectures: 3D CEL-UNet for segmentation and 3D Arthro-Net for threefold classification. A retrospective dataset of 571 CT scans featuring patients with various degrees of GH osteoarthritic-related pathologies was used to train, validate, and test the pipeline. Root mean squared error and Hausdorff distance median values for 3D reconstruction were 0.22mm and 1.48mm for the humerus and 0.24mm and 1.48mm for the scapula, outperforming state-of-the-art architectures and making it potentially suitable for a PSI-based shoulder arthroplasty preoperative plan context. The classification accuracy for OS, JS, and HSA consistently reached around 90% across all three categories. The computational time for the inference pipeline was less than 15s, showcasing the framework's efficiency and compatibility with orthopedic radiology practice. The outcomes represent a promising advancement toward the medical translation of artificial intelligence tools. This progress aims to streamline the preoperative planning pipeline delivering high-quality bone surfaces and supporting surgeons in selecting the most suitable surgical approach according to the unique patient joint conditions.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12641v1",
    "code_url": null
  },
  {
    "title": "From Lab to Pocket: A Novel Continual Learning-based Mobile Application for Screening COVID-19",
    "author": "Danny Falero, Muhammad Ashad Kabir, Nusrat Homaira",
    "summary": "Artificial intelligence (AI) has emerged as a promising tool for predicting COVID-19 from medical images. In this paper, we propose a novel continual learning-based approach and present the design and implementation of a mobile application for screening COVID-19. Our approach demonstrates the ability to adapt to evolving datasets, including data collected from different locations or hospitals, varying virus strains, and diverse clinical presentations, without retraining from scratch. We have evaluated state-of-the-art continual learning methods for detecting COVID-19 from chest X-rays and selected the best-performing model for our mobile app. We evaluated various deep learning architectures to select the best-performing one as a foundation model for continual learning. Both regularization and memory-based methods for continual learning were tested, using different memory sizes to develop the optimal continual learning model for our app. DenseNet161 emerged as the best foundation model with 96.87\\% accuracy, and Learning without Forgetting (LwF) was the top continual learning method with an overall performance of 71.99\\%. The mobile app design considers both patient and doctor perspectives. It incorporates the continual learning DenseNet161 LwF model on a cloud server, enabling the model to learn from new instances of chest X-rays and their classifications as they are submitted. The app is designed, implemented, and evaluated to ensure it provides an efficient tool for COVID-19 screening. The app is available to download from https://github.com/DannyFGitHub/COVID-19PneumoCheckApp.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12589v1",
    "code_url": null
  },
  {
    "title": "REST API Testing in DevOps: A Study on an Evolving Healthcare IoT Application",
    "author": "Hassan Sartaj, Shaukat Ali, Julie Marie Gj\u00f8by",
    "summary": "Healthcare Internet of Things (IoT) applications often integrate various third-party healthcare applications and medical devices through REST APIs, resulting in complex and interdependent networks of REST APIs. Oslo City's healthcare department collaborates with various industry partners to develop such healthcare IoT applications enriched with a diverse set of REST APIs. Following the DevOps process, these REST APIs continuously evolve to accommodate evolving needs such as new features, services, and devices. Oslo City's primary goal is to utilize automated solutions for continuous testing of these REST APIs at each evolution stage, thereby ensuring their dependability. Although the literature offers various automated REST API testing tools, their effectiveness in regression testing of the evolving REST APIs of healthcare IoT applications within a DevOps context remains undetermined. This paper evaluates state-of-the-art and well-established REST API testing tools-specifically, RESTest, EvoMaster, Schemathesis, RESTler, and RestTestGen-for the regression testing of a real-world healthcare IoT application, considering failures, faults, coverage, regressions, and cost. We conducted experiments using all accessible REST APIs (17 APIs with 120 endpoints), and 14 releases evolved during DevOps. Overall, all tools generated tests leading to several failures, 18 potential faults, up to 84% coverage, 23 regressions, and over 80% cost overhead.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12547v1",
    "code_url": null
  },
  {
    "title": "Evaluating Utility of Memory Efficient Medical Image Generation: A Study on Lung Nodule Segmentation",
    "author": "Kathrin Khadra, Utku T\u00fcrkbey",
    "summary": "The scarcity of publicly available medical imaging data limits the development of effective AI models. This work proposes a memory-efficient patch-wise denoising diffusion probabilistic model (DDPM) for generating synthetic medical images, focusing on CT scans with lung nodules. Our approach generates high-utility synthetic images with nodule segmentation while efficiently managing memory constraints, enabling the creation of training datasets. We evaluate the method in two scenarios: training a segmentation model exclusively on synthetic data, and augmenting real-world training data with synthetic images. In the first case, models trained solely on synthetic data achieve Dice scores comparable to those trained on real-world data benchmarks. In the second case, augmenting real-world data with synthetic images significantly improves segmentation performance. The generated images demonstrate their potential to enhance medical image datasets in scenarios with limited real-world data.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12542v1",
    "code_url": null
  },
  {
    "title": "Radiation damage and recovery of plastic scintillators under ultra-high dose rate 200 MeV electrons at CERN CLEAR facility",
    "author": "Clo\u00e9 Gigu\u00e8re, Alexander Hart, Joseph Bateman, Pierre Korysko, Wilfrid Farabolini, Yoan LeChasseur, Magdalena Bazalova-Carter, Luc Beaulieu",
    "summary": "The FLASH effect holds significant potential in improving radiotherapy treatment outcomes. Very high energy electrons (VHEEs) can effectively target tumors deep in the body and can be accelerated to achieve ultra-high dose rates (UHDR), making them a promising modality for delivering FLASH radiotherapy in the clinic. However, apart from suitable VHEE sources, clinical translation requires accurate dosimetry, which is challenging due to the limitation of standard dosimeters under UHDR. Water-equivalent and real-time plastic scintillation dosimeters (PSDs) may offer a solution. In this study, a 4-channel PSD, consisting of polystyrene-based BCF12 and Medscint proprietary scintillators, polyvinyltoluene (PVT)-based EJ-212 and a clear plastic fiber channel for Cherenkov subtraction was exposed to the 200 MeV VHEE UHDR beam at the CLEAR CERN facility. The Hyperscint RP200 platform was used to assess linearity to dose pulses of up to 90 Gy and dose rates up to 4.6x10$^9$ Gy/s, and to investigate radiation damage and recovery after dose accumulation of 37.2 kGy. While clear fiber response was linear across the entire dose range studied, light output saturated above ~50 Gy/pulse for scintillators. Despite radiation damage, linearity was preserved, though it resulted in a decrease of scintillator and clear fiber light output of <1.85 %/kGy and a shift in spectra towards longer wavelengths. Short-term recovery (<100h) of these changes was observed and depended on rest duration and accumulated dose. After long-term rest (<172 days), light output recovery was partial, with 6-22% of residual permanent damage remaining, while spectral recovery was complete. We showed that PSDs are sensitive to radiation damage, but maintain dose linearity even after accumulated dose of 37.2 kGy, and exhibit significant response recovery. This work highlights the potential of PSDs for dosimetry in UHDR conditions.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12535v1",
    "code_url": null
  },
  {
    "title": "Imaging neutrons with a position-sensitive monolithic CLYC detector",
    "author": "J. Lerendegui-Marco, G. Cisterna, J. Hallam, V. Babiano-Su\u00e1rez, J. Balibrea-Correa, D. Calvo, I. Ladarescu, G. de la Fuente, B. Gameiro, A. Sanchis-Molt\u00f3, P. Torres-S\u00e1nchez, C. Domingo-Pardo",
    "summary": "In this work, we have developed and characterized a position-sensitive CLYC detector that acts as the neutron imaging layer and $\\gamma$-ray Compton scatterer of the novel dual \\g-ray and neutron imaging system GN-Vision, which aims at simultaneously obtaining information about the spatial origin of \\g-ray and neutron sources. We first investigated the performance of large 50$\\times$50~mm$^{2}$ monolithic CLYC crystals coupled to a pixelated SiPM in terms of energy resolution and neutron-gamma discrimination. The response of two different 95\\% $^{6}$Li-enriched CLYC detectors coupled to an array of 8$\\times$8 SiPMs was studied in comparison to the results of a conventional photo-multiplier tube. Energy resolution ranging from 6-8\\% for the $^{137}$Cs peak and a figure of merit of 3-4 for the neutron-gamma discrimination have been obtained. The spatial response of the CLYC-SiPM detector to $\\gamma$-rays and neutrons has also been characterized using charge modulation-based multiplexing techniques based on a diode-coupled charge division circuit. Average resolutions close to 5~mm FWHM with good linearity are obtained in the transverse crystal plane. Last, this work presents the first proof-of-concept experiments of the neutron imaging capability using a neutron pinhole collimator attached to the developed position sensitive CLYC detector.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12533v1",
    "code_url": null
  },
  {
    "title": "MedAide: Towards an Omni Medical Aide via Specialized LLM-based Multi-Agent Collaboration",
    "author": "Jinjie Wei, Dingkang Yang, Yanshu Li, Qingyao Xu, Zhaoyu Chen, Mingcheng Li, Yue Jiang, Xiaolu Hou, Lihua Zhang",
    "summary": "Large Language Model (LLM)-driven interactive systems currently show potential promise in healthcare domains. Despite their remarkable capabilities, LLMs typically lack personalized recommendations and diagnosis analysis in sophisticated medical applications, causing hallucinations and performance bottlenecks. To address these challenges, this paper proposes MedAide, an LLM-based omni medical multi-agent collaboration framework for specialized healthcare services. Specifically, MedAide first performs query rewriting through retrieval-augmented generation to accomplish accurate medical intent understanding. Immediately, we devise a contextual encoder to obtain intent prototype embeddings, which are used to recognize fine-grained intents by similarity matching. According to the intent relevance, the activated agents collaborate effectively to provide integrated decision analysis. Extensive experiments are conducted on four medical benchmarks with composite intents. Experimental results from automated metrics and expert doctor evaluations show that MedAide outperforms current LLMs and improves their medical proficiency and strategic reasoning.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12532v1",
    "code_url": null
  },
  {
    "title": "Thermal analysis of GaN-based photonic membranes for optoelectronics",
    "author": "Wilken Seemann, Mahmoud Elhajhasan, Julian Themann, Katharina Dudde, Guillaume W\u00fcrsch, Jana Lierath, Joachim Ciers, \u00c5sa Haglund, Nakib H. Protik, Giuseppe Romano, Rapha\u00ebl Butt\u00e9, Jean-Fran\u00e7ois Carlin, Nicolas Grandjean, Gordon Callsen",
    "summary": "Semiconductor membranes find their widespread use in various research fields targeting medical, biological, environmental, and optical applications. Often such membranes derive their functionality from an inherent nanopatterning, which renders the determination of their, e.g., optical, electronic, mechanical, and thermal properties a challenging task. In this work we demonstrate the non-invasive, all-optical thermal characterization of around 800-nm-thick and 150-$\\mu$m-wide membranes that consist of wurtzite GaN and a stack of In$_{0.15}$Ga$_{0.85}$N quantum wells as a built-in light source. Due to their application in photonics such membranes are bright light emitters, which challenges their non-invasive thermal characterization by only optical means. As a solution, we combine two-laser Raman thermometry with (time-resolved) photoluminescence measurements to extract the in-plane (i.e., $c$-plane) thermal conductivity $\\kappa_{\\text{in-plane}}$ of our membranes. Based on this approach, we can disentangle the entire laser-induced power balance during our thermal analysis, meaning that all fractions of reflected, scattered, transmitted, and reemitted light are considered. As a result of our thermal imaging via Raman spectroscopy, we obtain $\\kappa_{\\text{in-plane}}\\,=\\,165^{+16}_{-14}\\,$Wm$^{-1}$K$^{-1}$ for our best membrane, which compares well to our simulations yielding $\\kappa_{\\text{in-plane}}\\,=\\,177\\,$Wm$^{-1}$K$^{-1}$ based on an ab initio solution of the linearized phonon Boltzmann transport equation. Our work presents a promising pathway towards thermal imaging at cryogenic temperatures, e.g., when aiming to elucidate experimentally different phonon transport regimes via the recording of non-Fourier temperature distributions.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12515v1",
    "code_url": null
  },
  {
    "title": "Synthetic Augmentation for Anatomical Landmark Localization using DDPMs",
    "author": "Arnela Hadzic, Lea Bogensperger, Simon Johannes Joham, Martin Urschler",
    "summary": "Deep learning techniques for anatomical landmark localization (ALL) have shown great success, but their reliance on large annotated datasets remains a problem due to the tedious and costly nature of medical data acquisition and annotation. While traditional data augmentation, variational autoencoders (VAEs), and generative adversarial networks (GANs) have already been used to synthetically expand medical datasets, diffusion-based generative models have recently started to gain attention for their ability to generate high-quality synthetic images. In this study, we explore the use of denoising diffusion probabilistic models (DDPMs) for generating medical images and their corresponding heatmaps of landmarks to enhance the training of a supervised deep learning model for ALL. Our novel approach involves a DDPM with a 2-channel input, incorporating both the original medical image and its heatmap of annotated landmarks. We also propose a novel way to assess the quality of the generated images using a Markov Random Field (MRF) model for landmark matching and a Statistical Shape Model (SSM) to check landmark plausibility, before we evaluate the DDPM-augmented dataset in the context of an ALL task involving hand X-Rays.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12489v1",
    "code_url": null
  },
  {
    "title": "Attention-Guided Perturbation for Consistency Regularization in Semi-Supervised Medical Image Segmentation",
    "author": "Yuxuan Cheng, Chenxi Shao, Jie Ma, Guoliang Li",
    "summary": "Medical image segmentation is a pivotal step in diagnostic and therapeutic processes. However, the acquisition of high-quality annotated data is often constrained by scarcity and cost. Semi-supervised learning offers a promising approach to enhance model performance by using unlabeled data. While consistency regularization is a prevalent method in semi-supervised image segmentation, there is a dearth of research on perturbation strategies tailored for semi-supervised medical image segmentation tasks. This paper introduces an attention-guided perturbation strategy for semi-supervised consistency regularization in the context of medical image segmentation. We add the perturbation based on the attention from the model in the image and feature level to achieve consistency regularization. The method is adept at accommodating the intricate structures and high-dimensional semantics inherent in medical images, thereby enhancing the performance of semi-supervised segmentation tasks. Our method achieved state-of-the-art results on benchmark datasets, including a 90.4\\% Dice score on the ACDC dataset in the 7-case scenario.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12419v1",
    "code_url": null
  },
  {
    "title": "De-Identification of Medical Imaging Data: A Comprehensive Tool for Ensuring Patient Privacy",
    "author": "Moritz Rempe, Lukas Heine, Constantin Seibold, Fabian H\u00f6rst, Jens Kleesiek",
    "summary": "Medical data employed in research frequently comprises sensitive patient health information (PHI), which is subject to rigorous legal frameworks such as the General Data Protection Regulation (GDPR) or the Health Insurance Portability and Accountability Act (HIPAA). Consequently, these types of data must be pseudonymized prior to utilisation, which presents a significant challenge for many researchers. Given the vast array of medical data, it is necessary to employ a variety of de-identification techniques. To facilitate the anonymization process for medical imaging data, we have developed an open-source tool that can be used to de-identify DICOM magnetic resonance images, computer tomography images, whole slide images and magnetic resonance twix raw data. Furthermore, the implementation of a neural network enables the removal of text within the images. The proposed tool automates an elaborate anonymization pipeline for multiple types of inputs, reducing the need for additional tools used for de-identification of imaging data. We make our code publicly available at https://github.com/code-lukas/medical_image_deidentification.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12402v1",
    "code_url": null
  },
  {
    "title": "Quantifying Treatment Effects: Estimating Risk Ratios in Causal Inference",
    "author": "Ahmed Boughdiri, Julie Josse, Erwan Scornet",
    "summary": "Randomized Controlled Trials (RCT) are the current gold standards to empirically measure the effect of a new drug. However, they may be of limited size and resorting to complementary non-randomized data, referred to as observational, is promising, as additional sources of evidence. In both RCT and observational data, the Risk Difference (RD) is often used to characterize the effect of a drug. Additionally, medical guidelines recommend to also report the Risk Ratio (RR), which may provide a different comprehension of the effect of the same drug. While different methods have been proposed and studied to estimate the RD, few methods exist to estimate the RR. In this paper, we propose estimators of the RR both in RCT and observational data and provide both asymptotical and finite-sample analyses. We show that, even in an RCT, estimating treatment allocation probability or adjusting for covariates leads to lower asymptotic variance. In observational studies, we propose weighting and outcome modeling estimators and derive their asymptotic bias and variance for well-specified models. Using semi-parametric theory, we define two doubly robusts estimators with minimal variances among unbiased estimators. We support our theoretical analysis with empirical evaluations and illustrate our findings through experiments.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12333v1",
    "code_url": null
  },
  {
    "title": "Advancing Healthcare: Innovative ML Approaches for Improved Medical Imaging in Data-Constrained Environments",
    "author": "Al Amin, Kamrul Hasan, Saleh Zein-Sabatto, Liang Hong, Sachin Shetty, Imtiaz Ahmed, Tariqul Islam",
    "summary": "Healthcare industries face challenges when experiencing rare diseases due to limited samples. Artificial Intelligence (AI) communities overcome this situation to create synthetic data which is an ethical and privacy issue in the medical domain. This research introduces the CAT-U-Net framework as a new approach to overcome these limitations, which enhances feature extraction from medical images without the need for large datasets. The proposed framework adds an extra concatenation layer with downsampling parts, thereby improving its ability to learn from limited data while maintaining patient privacy. To validate, the proposed framework's robustness, different medical conditioning datasets were utilized including COVID-19, brain tumors, and wrist fractures. The framework achieved nearly 98% reconstruction accuracy, with a Dice coefficient close to 0.946. The proposed CAT-U-Net has the potential to make a big difference in medical image diagnostics in settings with limited data.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12245v1",
    "code_url": null
  },
  {
    "title": "In vivo high-resolution \u03c7-separation at 7T",
    "author": "Jiye Kim, Minjun Kim, Sooyeon Ji, Kyeongseon Min, Hwihun Jeong, Hyeong-Geol Shin, Chungseok Oh, Sina Straub, Seong-Gi Kim, Jongho Lee",
    "summary": "A recently introduced quantitative susceptibility mapping (QSM) technique, $\\chi$-separation, offers the capability to separate paramagnetic ($\\chi_{\\text{para}}$) and diamagnetic ($\\chi_{\\text{dia}}$) susceptibility distribution within the brain. In-vivo high-resolution mapping of iron and myelin distribution, estimated by $\\chi$-separation, could provide a deeper understanding of brain substructures, assisting the investigation of their functions and alterations. This can be achieved using 7T MRI, which benefits from a high signal-to-noise ratio and susceptibility effects. However, applying $\\chi$-separation at 7T presents difficulties due to the requirement of an $R_2$ map, coupled with issues such as high specific absorption rate (SAR), large $B_1$ transmit field inhomogeneities, and prolonged scan time.   To address these challenges, we developed a novel deep neural network, R2PRIMEnet7T, designed to convert a 7T $R_2^*$ map into a 3T $R_2'$ map. Building on this development, we present a new pipeline for $\\chi$-separation at 7T, enabling us to generate high-resolution $\\chi$-separation maps from multi-echo gradient-echo data. The proposed method is compared with alternative pipelines, such as an end-to-end network and linearly-scaled $R_2'$, and is validated against $\\chi$-separation maps at 3T, demonstrating its accuracy. The 7T $\\chi$-separation maps generated by the proposed method exhibit similar contrasts to those from 3T, while 7T high-resolution maps offer enhanced clarity and detail. Quantitative analysis confirms that the proposed method surpasses the alternative pipelines. The proposed method results well delineate the detailed brain structures associated with iron and myelin. This new pipeline holds promise for analyzing iron and myelin concentration changes in various neurodegenerative diseases through precise structural examination.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12239v1",
    "code_url": null
  },
  {
    "title": "Implementation of EMR System in Indonesian Health Facilities: Benefits and Constraints",
    "author": "Rasyid Juliansyah, Bukhori Muhammad Aqid, Andien Putri Salsabila, Kurnia Nurfiyanti",
    "summary": "This paper delves into the widespread implementation of Electronic Medical Records (EMR) within healthcare facilities across Indonesia. It examines the driving forces behind EMR adoption, particularly the role of government regulations, and addresses the challenges encountered by clinic owners and healthcare providers in transitioning to these digital systems. Furthermore, this paper highlights the significant benefits and transformative advantages of EMR systems, such as enhanced decision-making through real-time data access (around 15-20 minutes time saved for patient waiting time and approximately saved 20-25 minutes for all service duration), reduction in healthcare costs over time due to improved resource management, and increased patient satisfaction by providing faster and more personalized care. EMR systems also ensure higher levels of data security and privacy, adhering to national healthcare standards, while supporting continuous monitoring and updates that enhance system resilience and functionality. The findings are substantiated through case studies, such as case study at LAPAS II Purwokerto Clinic and case study at PMI Purbalingga Clinic and user testimonials from clinics that have successfully implemented EMR solutions in compliance with the standards established by the Ministry of Communication and Informatics (Kominfo) and the Ministry of Health (Kemenkes).",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12226v1",
    "code_url": null
  },
  {
    "title": "AutoSimTTF: A Fully Automatic Pipeline for Electric Field Simulation and Treatment Planning of Tumor Treating Fields",
    "author": "Minmin Wang, Xu Xie, Zhengbo Fan, Yue Lan, Yun Pan, Guangdi Chen, Shaomin Zhang, Yuxing Wang",
    "summary": "Objective: Tumor Treating Fields (TTFields) is an emerging approach for cancer therapy that inhibits tumor cell proliferation by applying alternating electric fields (EF) of intermediate frequency and low intensity. The TTFields-induced electric field intensity at the tumor site is closely related to the therapeutic efficacy. Therefore, the EF simulation based on realistic head models have been utilized for the dosage analysis and treatment optimization of TTFields. However, current modeling methods require manual segmentation of tumors and rely on commercial software, which is time-consuming and labor-intensive. Approach: We introduce AutoSimTTF, a fully automatic pipeline for simulating and optimizing the EF distribution for TTFields. The main steps of AutoSimTTF utilize open-source toolkits, enabling fully automated processing of individual MRI data for TTFields. Additionally, AutoSimTTF allows for parameter optimization based on individual anatomical information, thereby achieving a more focused and higher EF distribution at the tumor site. Main results: Compared to conventional EF calculation processes, deviations in AutoSimTTF are below 20%. The optimal treatment parameters generated by AutoSimTTF produces a higher EF intensity at the tumor site (111.9%) and better focality (19.4%) compared to traditional TTFields settings. Significance: AutoSimTTF provides significant reference value and guidance for the clinical application and treatment planning of TTFields.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12196v1",
    "code_url": null
  },
  {
    "title": "Adaptive Prompt Learning with SAM for Few-shot Scanning Probe Microscope Image Segmentation",
    "author": "Yao Shen, Ziwei Wei, Chunmeng Liu, Shuming Wei, Qi Zhao, Kaiyang Zeng, Guangyao Li",
    "summary": "The Segment Anything Model (SAM) has demonstrated strong performance in image segmentation of natural scene images. However, its effectiveness diminishes markedly when applied to specific scientific domains, such as Scanning Probe Microscope (SPM) images. This decline in accuracy can be attributed to the distinct data distribution and limited availability of the data inherent in the scientific images. On the other hand, the acquisition of adequate SPM datasets is both time-intensive and laborious as well as skill-dependent. To address these challenges, we propose an Adaptive Prompt Learning with SAM (APL-SAM) framework tailored for few-shot SPM image segmentation. Our approach incorporates two key innovations to enhance SAM: 1) An Adaptive Prompt Learning module leverages few-shot embeddings derived from limited support set to learn adaptively central representatives, serving as visual prompts. This innovation eliminates the need for time-consuming online user interactions for providing prompts, such as exhaustively marking points and bounding boxes slice by slice; 2) A multi-source, multi-level mask decoder specifically designed for few-shot SPM image segmentation is introduced, which can effectively capture the correspondence between the support and query images. To facilitate comprehensive training and evaluation, we introduce a new dataset, SPM-Seg, curated for SPM image segmentation. Extensive experiments on this dataset reveal that the proposed APL-SAM framework significantly outperforms the original SAM, achieving over a 30% improvement in terms of Dice Similarity Coefficient with only one-shot guidance. Moreover, APL-SAM surpasses state-of-the-art few-shot segmentation methods and even fully supervised approaches in performance. Code and dataset used in this study will be made available upon acceptance.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12562v1",
    "code_url": null
  },
  {
    "title": "A Holistic Weakly Supervised Approach for Liver Tumor Segmentation with Clinical Knowledge-Informed Label Smoothing",
    "author": "Hairong Wang, Lingchao Mao, Zihan Zhang, Jing Li",
    "summary": "Liver cancer is a leading cause of mortality worldwide, and accurate CT-based tumor segmentation is essential for diagnosis and treatment. Manual delineation is time-intensive, prone to variability, and highlights the need for reliable automation. While deep learning has shown promise for automated liver segmentation, precise liver tumor segmentation remains challenging due to the heterogeneous nature of tumors, imprecise tumor margins, and limited labeled data. We present a novel holistic weakly supervised framework that integrates clinical knowledge to address these challenges with (1) A knowledge-informed label smoothing technique that leverages clinical data to generate smooth labels, which regularizes model training reducing the risk of overfitting and enhancing model performance; (2) A global and local-view segmentation framework, breaking down the task into two simpler sub-tasks, allowing optimized preprocessing and training for each; and (3) Pre- and post-processing pipelines customized to the challenges of each subtask, which enhances tumor visibility and refines tumor boundaries. We evaluated the proposed method on the HCC-TACE-Seg dataset and showed that these three key components complementarily contribute to the improved performance. Lastly, we prototyped a tool for automated liver tumor segmentation and diagnosis summary generation called MedAssistLiver. The app and code are published at https://github.com/lingchm/medassist-liver-cancer.",
    "published": "2024-10-13",
    "link": "http://arxiv.org/abs/2410.10005v1",
    "code_url": "https://github.com/lingchm/medassist-liver-cancer"
  },
  {
    "title": "Iterative Optimization Annotation Pipeline and ALSS-YOLO-Seg for Efficient Banana Plantation Segmentation in UAV Imagery",
    "author": "Ang He, Ximei Wu, Xing Xu, Jing Chen, Xiaobin Guo, Sheng Xu",
    "summary": "Precise segmentation of Unmanned Aerial Vehicle (UAV)-captured images plays a vital role in tasks such as crop yield estimation and plant health assessment in banana plantations. By identifying and classifying planted areas, crop area can be calculated, which is indispensable for accurate yield predictions. However, segmenting banana plantation scenes requires a substantial amount of annotated data, and manual labeling of these images is both time-consuming and labor-intensive, limiting the development of large-scale datasets. Furthermore, challenges such as changing target sizes, complex ground backgrounds, limited computational resources, and correct identification of crop categories make segmentation even more difficult. To address these issues, we proposed a comprehensive solution. Firstly, we designed an iterative optimization annotation pipeline leveraging SAM2's zero-shot capabilities to generate high-quality segmentation annotations, thereby reducing the cost and time associated with data annotation significantly. Secondly, we developed ALSS-YOLO-Seg, an efficient lightweight segmentation model optimized for UAV imagery. The model's backbone includes an Adaptive Lightweight Channel Splitting and Shuffling (ALSS) module to improve information exchange between channels and optimize feature extraction, aiding accurate crop identification. Additionally, a Multi-Scale Channel Attention (MSCA) module combines multi-scale feature extraction with channel attention to tackle challenges of varying target sizes and complex ground backgrounds.",
    "published": "2024-10-09",
    "link": "http://arxiv.org/abs/2410.07955v1",
    "code_url": "https://github.com/helloworlder8/computer_vision"
  },
  {
    "title": "Leveraging CAM Algorithms for Explaining Medical Semantic Segmentation",
    "author": "Tillmann Rheude, Andreas Wirtz, Arjan Kuijper, Stefan Wesarg",
    "summary": "Convolutional neural networks (CNNs) achieve prevailing results in segmentation tasks nowadays and represent the state-of-the-art for image-based analysis. However, the understanding of the accurate decision-making process of a CNN is rather unknown. The research area of explainable artificial intelligence (xAI) primarily revolves around understanding and interpreting this black-box behavior. One way of interpreting a CNN is the use of class activation maps (CAMs) that represent heatmaps to indicate the importance of image areas for the prediction of the CNN. For classification tasks, a variety of CAM algorithms exist. But for segmentation tasks, only one CAM algorithm for the interpretation of the output of a CNN exist. We propose a transfer between existing classification- and segmentation-based methods for more detailed, explainable, and consistent results which show salient pixels in semantic segmentation tasks. The resulting Seg-HiRes-Grad CAM is an extension of the segmentation-based Seg-Grad CAM with the transfer to the classification-based HiRes CAM. Our method improves the previously-mentioned existing segmentation-based method by adjusting it to recently published classification-based methods. Especially for medical image segmentation, this transfer solves existing explainability disadvantages.",
    "published": "2024-09-30",
    "link": "http://arxiv.org/abs/2409.20287v1",
    "code_url": "https://github.com/TillmannRheude/SegHiResGrad_CAM"
  },
  {
    "title": "One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos",
    "author": "Zechen Bai, Tong He, Haiyang Mei, Pichao Wang, Ziteng Gao, Joya Chen, Lei Liu, Zheng Zhang, Mike Zheng Shou",
    "summary": "We introduce VideoLISA, a video-based multimodal large language model designed to tackle the problem of language-instructed reasoning segmentation in videos. Leveraging the reasoning capabilities and world knowledge of large language models, and augmented by the Segment Anything Model, VideoLISA generates temporally consistent segmentation masks in videos based on language instructions. Existing image-based methods, such as LISA, struggle with video tasks due to the additional temporal dimension, which requires temporal dynamic understanding and consistent segmentation across frames. VideoLISA addresses these challenges by integrating a Sparse Dense Sampling strategy into the video-LLM, which balances temporal context and spatial detail within computational constraints. Additionally, we propose a One-Token-Seg-All approach using a specially designed <TRK> token, enabling the model to segment and track objects across multiple frames. Extensive evaluations on diverse benchmarks, including our newly introduced ReasonVOS benchmark, demonstrate VideoLISA's superior performance in video object segmentation tasks involving complex reasoning, temporal understanding, and object tracking. While optimized for videos, VideoLISA also shows promising generalization to image segmentation, revealing its potential as a unified foundation model for language-instructed object segmentation. Code and model will be available at: https://github.com/showlab/VideoLISA.",
    "published": "2024-09-29",
    "link": "http://arxiv.org/abs/2409.19603v1",
    "code_url": null
  },
  {
    "title": "Diff-VPS: Video Polyp Segmentation via a Multi-task Diffusion Network with Adversarial Temporal Reasoning",
    "author": "Yingling Lu, Yijun Yang, Zhaohu Xing, Qiong Wang, Lei Zhu",
    "summary": "Diffusion Probabilistic Models have recently attracted significant attention in the community of computer vision due to their outstanding performance. However, while a substantial amount of diffusion-based research has focused on generative tasks, no work introduces diffusion models to advance the results of polyp segmentation in videos, which is frequently challenged by polyps' high camouflage and redundant temporal cues.In this paper, we present a novel diffusion-based network for video polyp segmentation task, dubbed as Diff-VPS. We incorporate multi-task supervision into diffusion models to promote the discrimination of diffusion models on pixel-by-pixel segmentation. This integrates the contextual high-level information achieved by the joint classification and detection tasks. To explore the temporal dependency, Temporal Reasoning Module (TRM) is devised via reasoning and reconstructing the target frame from the previous frames. We further equip TRM with a generative adversarial self-supervised strategy to produce more realistic frames and thus capture better dynamic cues. Extensive experiments are conducted on SUN-SEG, and the results indicate that our proposed Diff-VPS significantly achieves state-of-the-art performance. Code is available at https://github.com/lydia-yllu/Diff-VPS.",
    "published": "2024-09-11",
    "link": "http://arxiv.org/abs/2409.07238v1",
    "code_url": "https://github.com/lydia-yllu/diff-vps"
  },
  {
    "title": "Seg-HGNN: Unsupervised and Light-Weight Image Segmentation with Hyperbolic Graph Neural Networks",
    "author": "Debjyoti Mondal, Rahul Mishra, Chandan Pandey",
    "summary": "Image analysis in the euclidean space through linear hyperspaces is well studied. However, in the quest for more effective image representations, we turn to hyperbolic manifolds. They provide a compelling alternative to capture complex hierarchical relationships in images with remarkably small dimensionality. To demonstrate hyperbolic embeddings' competence, we introduce a light-weight hyperbolic graph neural network for image segmentation, encompassing patch-level features in a very small embedding size. Our solution, Seg-HGNN, surpasses the current best unsupervised method by 2.5\\%, 4\\% on VOC-07, VOC-12 for localization, and by 0.8\\%, 1.3\\% on CUB-200, ECSSD for segmentation, respectively. With less than 7.5k trainable parameters, Seg-HGNN delivers effective and fast ($\\approx 2$ images/second) results on very standard GPUs like the GTX1650. This empirical evaluation presents compelling evidence of the efficacy and potential of hyperbolic representations for vision tasks.",
    "published": "2024-09-10",
    "link": "http://arxiv.org/abs/2409.06589v1",
    "code_url": null
  },
  {
    "title": "Segmenting Object Affordances: Reproducibility and Sensitivity to Scale",
    "author": "Tommaso Apicella, Alessio Xompero, Paolo Gastaldo, Andrea Cavallaro",
    "summary": "Visual affordance segmentation identifies image regions of an object an agent can interact with. Existing methods re-use and adapt learning-based architectures for semantic segmentation to the affordance segmentation task and evaluate on small-size datasets. However, experimental setups are often not reproducible, thus leading to unfair and inconsistent comparisons. In this work, we benchmark these methods under a reproducible setup on two single objects scenarios, tabletop without occlusions and hand-held containers, to facilitate future comparisons. We include a version of a recent architecture, Mask2Former, re-trained for affordance segmentation and show that this model is the best-performing on most testing sets of both scenarios. Our analysis shows that models are not robust to scale variations when object resolutions differ from those in the training set.",
    "published": "2024-09-03",
    "link": "http://arxiv.org/abs/2409.01814v1",
    "code_url": "https://github.com/apicis/aff-seg"
  },
  {
    "title": "LSMS: Language-guided Scale-aware MedSegmentor for Medical Image Referring Segmentation",
    "author": "Shuyi Ouyang, Jinyang Zhang, Xiangye Lin, Xilai Wang, Qingqing Chen, Yen-Wei Chen, Lanfen Lin",
    "summary": "Conventional medical image segmentation methods have been found inadequate in facilitating physicians with the identification of specific lesions for diagnosis and treatment. Given the utility of text as an instructional format, we introduce a novel task termed Medical Image Referring Segmentation (MIRS), which requires segmenting specified lesions in images based on the given language expressions. Due to the varying object scales in medical images, MIRS demands robust vision-language modeling and comprehensive multi-scale interaction for precise localization and segmentation under linguistic guidance. However, existing medical image segmentation methods fall short in meeting these demands, resulting in insufficient segmentation accuracy. In response, we propose an approach named Language-guided Scale-aware MedSegmentor (LSMS), incorporating two appealing designs: (1)~a Scale-aware Vision-Language Attention module that leverages diverse convolutional kernels to acquire rich visual knowledge and interact closely with linguistic features, thereby enhancing lesion localization capability; (2)~a Full-Scale Decoder that globally models multi-modal features across various scales, capturing complementary information between scales to accurately outline lesion boundaries. Addressing the lack of suitable datasets for MIRS, we constructed a vision-language medical dataset called Reference Hepatic Lesion Segmentation (RefHL-Seg). This dataset comprises 2,283 abdominal CT slices from 231 cases, with corresponding textual annotations and segmentation masks for various liver lesions in images. We validated the performance of LSMS for MIRS and conventional medical image segmentation tasks across various datasets. Our LSMS consistently outperforms on all datasets with lower computational costs. The code and datasets will be released.",
    "published": "2024-08-30",
    "link": "http://arxiv.org/abs/2408.17347v2",
    "code_url": null
  },
  {
    "title": "LV-UNet: A Lightweight and Vanilla Model for Medical Image Segmentation",
    "author": "Juntao Jiang, Mengmeng Wang, Huizhong Tian, Lingbo Cheng, Yong Liu",
    "summary": "Although the progress made by large models in computer vision, optimization challenges, the complexity of transformer models, computational limitations, and the requirements of practical applications call for simpler designs in model architecture for medical image segmentation, especially in mobile medical devices that require lightweight and deployable models with real-time performance. However, some of the current lightweight models exhibit poor robustness across different datasets, which hinders their broader adoption. This paper proposes a lightweight and vanilla model called LV-UNet, which effectively utilizes pre-trained MobileNetv3-Large models and introduces fusible modules. It can be trained using an improved deep training strategy and switched to deployment mode during inference, reducing both parameter count and computational load. Experiments are conducted on ISIC 2016, BUSI, CVC- ClinicDB, CVC-ColonDB, and Kvair-SEG datasets, achieving better performance compared to the state-of-the-art and classic models.",
    "published": "2024-08-29",
    "link": "http://arxiv.org/abs/2408.16886v1",
    "code_url": null
  },
  {
    "title": "Sapiens: Foundation for Human Vision Models",
    "author": "Rawal Khirodkar, Timur Bagautdinov, Julieta Martinez, Su Zhaoen, Austin James, Peter Selednik, Stuart Anderson, Shunsuke Saito",
    "summary": "We present Sapiens, a family of models for four fundamental human-centric vision tasks -- 2D pose estimation, body-part segmentation, depth estimation, and surface normal prediction. Our models natively support 1K high-resolution inference and are extremely easy to adapt for individual tasks by simply fine-tuning models pretrained on over 300 million in-the-wild human images. We observe that, given the same computational budget, self-supervised pretraining on a curated dataset of human images significantly boosts the performance for a diverse set of human-centric tasks. The resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. Our simple model design also brings scalability -- model performance across tasks improves as we scale the number of parameters from 0.3 to 2 billion. Sapiens consistently surpasses existing baselines across various human-centric benchmarks. We achieve significant improvements over the prior state-of-the-art on Humans-5K (pose) by 7.6 mAP, Humans-2K (part-seg) by 17.1 mIoU, Hi4D (depth) by 22.4% relative RMSE, and THuman2 (normal) by 53.5% relative angular error. Project page: https://about.meta.com/realitylabs/codecavatars/sapiens.",
    "published": "2024-08-22",
    "link": "http://arxiv.org/abs/2408.12569v3",
    "code_url": null
  },
  {
    "title": "Tuning a SAM-Based Model with Multi-Cognitive Visual Adapter to Remote Sensing Instance Segmentation",
    "author": "Linghao Zheng, Xinyang Pu, Feng Xu",
    "summary": "The Segment Anything Model (SAM), a foundational model designed for promptable segmentation tasks, demonstrates exceptional generalization capabilities, making it highly promising for natural scene image segmentation. However, SAM's lack of pretraining on massive remote sensing images and its interactive structure limit its automatic mask prediction capabilities. In this paper, a Multi-Cognitive SAM-Based Instance Segmentation Model (MC-SAM SEG) is introduced to employ SAM on remote sensing domain. The SAM-Mona encoder utilizing the Multi-cognitive Visual Adapter (Mona) is conducted to facilitate SAM's transfer learning in remote sensing applications. The proposed method named MC-SAM SEG extracts high-quality features by fine-tuning the SAM-Mona encoder along with a feature aggregator. Subsequently, a pixel decoder and transformer decoder are designed for prompt-free mask generation and instance classification. The comprehensive experiments are conducted on the HRSID and WHU datasets for instance segmentation tasks on Synthetic Aperture Radar (SAR) images and optical remote sensing images respectively. The evaluation results indicate the proposed method surpasses other deep learning algorithms and verify its effectiveness and generalization.",
    "published": "2024-08-16",
    "link": "http://arxiv.org/abs/2408.08576v1",
    "code_url": null
  },
  {
    "title": "Multi-scale Contrastive Adaptor Learning for Segmenting Anything in Underperformed Scenes",
    "author": "Ke Zhou, Zhongwei Qiu, Dongmei Fu",
    "summary": "Foundational vision models, such as the Segment Anything Model (SAM), have achieved significant breakthroughs through extensive pre-training on large-scale visual datasets. Despite their general success, these models may fall short in specialized tasks with limited data, and fine-tuning such large-scale models is often not feasible. Current strategies involve incorporating adaptors into the pre-trained SAM to facilitate downstream task performance with minimal model adjustment. However, these strategies can be hampered by suboptimal learning approaches for the adaptors. In this paper, we introduce a novel Multi-scale Contrastive Adaptor learning method named MCA-SAM, which enhances adaptor performance through a meticulously designed contrastive learning framework at both token and sample levels. Our Token-level Contrastive adaptor (TC-adaptor) focuses on refining local representations by improving the discriminability of patch tokens, while the Sample-level Contrastive adaptor (SC-adaptor) amplifies global understanding across different samples. Together, these adaptors synergistically enhance feature comparison within and across samples, bolstering the model's representational strength and its ability to adapt to new tasks. Empirical results demonstrate that MCA-SAM sets new benchmarks, outperforming existing methods in three challenging domains: camouflage object detection, shadow segmentation, and polyp segmentation. Specifically, MCA-SAM exhibits substantial relative performance enhancements, achieving a 20.0% improvement in MAE on the COD10K dataset, a 6.0% improvement in MAE on the CAMO dataset, a 15.4% improvement in BER on the ISTD dataset, and a 7.9% improvement in mDice on the Kvasir-SEG dataset.",
    "published": "2024-08-12",
    "link": "http://arxiv.org/abs/2408.05936v1",
    "code_url": null
  },
  {
    "title": "Seg-CycleGAN : SAR-to-optical image translation guided by a downstream task",
    "author": "Hannuo Zhang, Huihui Li, Jiarui Lin, Yujie Zhang, Jianghua Fan, Hang Liu",
    "summary": "Optical remote sensing and Synthetic Aperture Radar(SAR) remote sensing are crucial for earth observation, offering complementary capabilities. While optical sensors provide high-quality images, they are limited by weather and lighting conditions. In contrast, SAR sensors can operate effectively under adverse conditions. This letter proposes a GAN-based SAR-to-optical image translation method named Seg-CycleGAN, designed to enhance the accuracy of ship target translation by leveraging semantic information from a pre-trained semantic segmentation model. Our method utilizes the downstream task of ship target semantic segmentation to guide the training of image translation network, improving the quality of output Optical-styled images. The potential of foundation-model-annotated datasets in SAR-to-optical translation tasks is revealed. This work suggests broader research and applications for downstream-task-guided frameworks. The code will be available at https://github.com/NPULHH/",
    "published": "2024-08-11",
    "link": "http://arxiv.org/abs/2408.05777v1",
    "code_url": null
  },
  {
    "title": "Embodied Uncertainty-Aware Object Segmentation",
    "author": "Xiaolin Fang, Leslie Pack Kaelbling, Tom\u00e1s Lozano-P\u00e9rez",
    "summary": "We introduce uncertainty-aware object instance segmentation (UncOS) and demonstrate its usefulness for embodied interactive segmentation. To deal with uncertainty in robot perception, we propose a method for generating a hypothesis distribution of object segmentation. We obtain a set of region-factored segmentation hypotheses together with confidence estimates by making multiple queries of large pre-trained models. This process can produce segmentation results that achieve state-of-the-art performance on unseen object segmentation problems. The output can also serve as input to a belief-driven process for selecting robot actions to perturb the scene to reduce ambiguity. We demonstrate the effectiveness of this method in real-robot experiments. Website: https://sites.google.com/view/embodied-uncertain-seg",
    "published": "2024-08-08",
    "link": "http://arxiv.org/abs/2408.04760v1",
    "code_url": null
  },
  {
    "title": "Smoothed Energy Guidance: Guiding Diffusion Models with Reduced Energy Curvature of Attention",
    "author": "Susung Hong",
    "summary": "Conditional diffusion models have shown remarkable success in visual content generation, producing high-quality samples across various domains, largely due to classifier-free guidance (CFG). Recent attempts to extend guidance to unconditional models have relied on heuristic techniques, resulting in suboptimal generation quality and unintended effects. In this work, we propose Smoothed Energy Guidance (SEG), a novel training- and condition-free approach that leverages the energy-based perspective of the self-attention mechanism to enhance image generation. By defining the energy of self-attention, we introduce a method to reduce the curvature of the energy landscape of attention and use the output as the unconditional prediction. Practically, we control the curvature of the energy landscape by adjusting the Gaussian kernel parameter while keeping the guidance scale parameter fixed. Additionally, we present a query blurring method that is equivalent to blurring the entire attention weights without incurring quadratic complexity in the number of tokens. In our experiments, SEG achieves a Pareto improvement in both quality and the reduction of side effects. The code is available at https://github.com/SusungHong/SEG-SDXL.",
    "published": "2024-08-01",
    "link": "http://arxiv.org/abs/2408.00760v2",
    "code_url": "https://github.com/susunghong/seg-sdxl"
  },
  {
    "title": "DDU-Net: A Domain Decomposition-based CNN for High-Resolution Image Segmentation on Multiple GPUs",
    "author": "Corn\u00e9 Verburg, Alexander Heinlein, Eric C. Cyr",
    "summary": "The segmentation of ultra-high resolution images poses challenges such as loss of spatial information or computational inefficiency. In this work, a novel approach that combines encoder-decoder architectures with domain decomposition strategies to address these challenges is proposed. Specifically, a domain decomposition-based U-Net (DDU-Net) architecture is introduced, which partitions input images into non-overlapping patches that can be processed independently on separate devices. A communication network is added to facilitate inter-patch information exchange to enhance the understanding of spatial context. Experimental validation is performed on a synthetic dataset that is designed to measure the effectiveness of the communication network. Then, the performance is tested on the DeepGlobe land cover classification dataset as a real-world benchmark data set. The results demonstrate that the approach, which includes inter-patch communication for images divided into $16\\times16$ non-overlapping subimages, achieves a $2-3\\,\\%$ higher intersection over union (IoU) score compared to the same network without inter-patch communication. The performance of the network which includes communication is equivalent to that of a baseline U-Net trained on the full image, showing that our model provides an effective solution for segmenting ultra-high-resolution images while preserving spatial context. The code is available at https://github.com/corne00/HiRes-Seg-CNN.",
    "published": "2024-07-31",
    "link": "http://arxiv.org/abs/2407.21266v2",
    "code_url": "https://github.com/corne00/hires-seg-cnn"
  },
  {
    "title": "ASI-Seg: Audio-Driven Surgical Instrument Segmentation with Surgeon Intention Understanding",
    "author": "Zhen Chen, Zongming Zhang, Wenwu Guo, Xingjian Luo, Long Bai, Jinlin Wu, Hongliang Ren, Hongbin Liu",
    "summary": "Surgical instrument segmentation is crucial in surgical scene understanding, thereby facilitating surgical safety. Existing algorithms directly detected all instruments of pre-defined categories in the input image, lacking the capability to segment specific instruments according to the surgeon's intention. During different stages of surgery, surgeons exhibit varying preferences and focus toward different surgical instruments. Therefore, an instrument segmentation algorithm that adheres to the surgeon's intention can minimize distractions from irrelevant instruments and assist surgeons to a great extent. The recent Segment Anything Model (SAM) reveals the capability to segment objects following prompts, but the manual annotations for prompts are impractical during the surgery. To address these limitations in operating rooms, we propose an audio-driven surgical instrument segmentation framework, named ASI-Seg, to accurately segment the required surgical instruments by parsing the audio commands of surgeons. Specifically, we propose an intention-oriented multimodal fusion to interpret the segmentation intention from audio commands and retrieve relevant instrument details to facilitate segmentation. Moreover, to guide our ASI-Seg segment of the required surgical instruments, we devise a contrastive learning prompt encoder to effectively distinguish the required instruments from the irrelevant ones. Therefore, our ASI-Seg promotes the workflow in the operating rooms, thereby providing targeted support and reducing the cognitive load on surgeons. Extensive experiments are performed to validate the ASI-Seg framework, which reveals remarkable advantages over classical state-of-the-art and medical SAMs in both semantic segmentation and intention-oriented segmentation. The source code is available at https://github.com/Zonmgin-Zhang/ASI-Seg.",
    "published": "2024-07-28",
    "link": "http://arxiv.org/abs/2407.19435v1",
    "code_url": "https://github.com/zonmgin-zhang/asi-seg"
  },
  {
    "title": "Accurate background velocity model building method based on iterative deep learning in sparse transform domain",
    "author": "Guoxin Chen",
    "summary": "Whether it is oil and gas exploration or geological science research, it is necessary to accurately grasp the structural information of underground media. Full waveform inversion is currently the most popular seismic wave inversion method, but it is highly dependent on a high-quality initial model. Artificial intelligence algorithm deep learning is completely data-driven and can get rid of the dependence on the initial model. However, the prediction accuracy of deep learning algorithms depends on the scale and diversity of training data sets. How to improve the prediction accuracy of deep learning without increasing the size of the training set while also improving computing efficiency is a worthy issue to study. In this paper, an iterative deep learning algorithm in the sparse transform domain is proposed based on the characteristics of deep learning: first, based on the computational efficiency and the effect of sparse transform, the cosine transform is selected as the sparse transform method, and the seismic data and the corresponding velocity model are cosine transformed to obtain their corresponding sparse expressions, which are then used as the input data and corresponding label data for deep learning; then we give an iterative deep learning algorithm in the cosine transform domain, that is, after obtaining the seismic data residuals and velocity model residuals of the previous round of test results, they are used again as new input data and label data, and re-trained in the cosine domain to obtain a new network, and the prediction results of the previous round are corrected, and then the cycle is repeated until the termination condition is reached. The algorithm effect was verified on the SEG/EAGE salt model and the seabed sulfide physical model site data.",
    "published": "2024-07-28",
    "link": "http://arxiv.org/abs/2407.19419v1",
    "code_url": null
  },
  {
    "title": "Polyp segmentation in colonoscopy images using DeepLabV3++",
    "author": "Al Mohimanul Islam, Sadia Shakiba Bhuiyan, Mysun Mashira, Md. Rayhan Ahmed, Salekul Islam, Swakkhar Shatabda",
    "summary": "Segmenting polyps in colonoscopy images is essential for the early identification and diagnosis of colorectal cancer, a significant cause of worldwide cancer deaths. Prior deep learning based models such as Attention based variation, UNet variations and Transformer-derived networks have had notable success in capturing intricate features and complex polyp shapes. In this study, we have introduced the DeepLabv3++ model which is an enhanced version of the DeepLabv3+ architecture. It is designed to improve the precision and robustness of polyp segmentation in colonoscopy images. We have utilized The proposed model incorporates diverse separable convolutional layers and attention mechanisms within the MSPP block, enhancing its capacity to capture multi-scale and directional features. Additionally, the redesigned decoder further transforms the extracted features from the encoder into a more meaningful segmentation map. Our model was evaluated on three public datasets (CVC-ColonDB, CVC-ClinicDB, Kvasir-SEG) achieving Dice coefficient scores of 96.20%, 96.54%, and 96.08%, respectively. The experimental analysis shows that DeepLabV3++ outperforms several state-of-the-art models in polyp segmentation tasks. Furthermore, compared to the baseline DeepLabV3+ model, our DeepLabV3++ with its MSPP module and redesigned decoder architecture, significantly reduced segmentation errors (e.g., false positives/negatives) across small, medium, and large polyps. This improvement in polyp delineation is crucial for accurate clinical decision-making in colonoscopy.",
    "published": "2024-07-27",
    "link": "http://arxiv.org/abs/2407.19327v1",
    "code_url": null
  },
  {
    "title": "MambaBEV: An efficient 3D detection model with Mamba2",
    "author": "Zihan You, Hao Wang, Qichao Zhao, Jinxiang Wang",
    "summary": "A stable 3D object detection model based on BEV paradigm with temporal information is very important for autonomous driving systems. However, current temporal fusion model use convolutional layer or deformable self-attention is not conducive to the exchange of global information of BEV space and has more computational cost. Recently, a newly proposed based model specialized in processing sequence called mamba has shown great potential in multiple downstream task. In this work, we proposed a mamba2-based BEV 3D object detection model named MambaBEV. We also adapt an end to end self driving paradigm to test the performance of the model. Our work performs pretty good results on nucences datasets:Our base version achieves 51.7% NDS. Our code will be available soon.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12673v1",
    "code_url": null
  },
  {
    "title": "UmambaTSF: A U-shaped Multi-Scale Long-Term Time Series Forecasting Method Using Mamba",
    "author": "Li Wu, Wenbin Pei, Jiulong Jiao, Qiang Zhang",
    "summary": "Multivariate Time series forecasting is crucial in domains such as transportation, meteorology, and finance, especially for predicting extreme weather events. State-of-the-art methods predominantly rely on Transformer architectures, which utilize attention mechanisms to capture temporal dependencies. However, these methods are hindered by quadratic time complexity, limiting the model's scalability with respect to input sequence length. This significantly restricts their practicality in the real world. Mamba, based on state space models (SSM), provides a solution with linear time complexity, increasing the potential for efficient forecasting of sequential data. In this study, we propose UmambaTSF, a novel long-term time series forecasting framework that integrates multi-scale feature extraction capabilities of U-shaped encoder-decoder multilayer perceptrons (MLP) with Mamba's long sequence representation. To improve performance and efficiency, the Mamba blocks introduced in the framework adopt a refined residual structure and adaptable design, enabling the capture of unique temporal signals and flexible channel processing. In the experiments, UmambaTSF achieves state-of-the-art performance and excellent generality on widely used benchmark datasets while maintaining linear time complexity and low memory consumption.",
    "published": "2024-10-15",
    "link": "http://arxiv.org/abs/2410.11278v1",
    "code_url": null
  },
  {
    "title": "Mimetic Initialization Helps State Space Models Learn to Recall",
    "author": "Asher Trockman, Hrayr Harutyunyan, J. Zico Kolter, Sanjiv Kumar, Srinadh Bhojanapalli",
    "summary": "Recent work has shown that state space models such as Mamba are significantly worse than Transformers on recall-based tasks due to the fact that their state size is constant with respect to their input sequence length. But in practice, state space models have fairly large state sizes, and we conjecture that they should be able to perform much better at these tasks than previously reported. We investigate whether their poor copying and recall performance could be due in part to training difficulties rather than fundamental capacity constraints. Based on observations of their \"attention\" maps, we propose a structured initialization technique that allows state space layers to more readily mimic attention. Across a variety of architecture settings, our initialization makes it substantially easier for Mamba to learn to copy and do associative recall from scratch.",
    "published": "2024-10-14",
    "link": "http://arxiv.org/abs/2410.11135v1",
    "code_url": null
  },
  {
    "title": "CleanUMamba: A Compact Mamba Network for Speech Denoising using Channel Pruning",
    "author": "Sjoerd Groot, Qinyu Chen, Jan C. van Gemert, Chang Gao",
    "summary": "This paper presents CleanUMamba, a time-domain neural network architecture designed for real-time causal audio denoising directly applied to raw waveforms. CleanUMamba leverages a U-Net encoder-decoder structure, incorporating the Mamba state-space model in the bottleneck layer. By replacing conventional self-attention and LSTM mechanisms with Mamba, our architecture offers superior denoising performance while maintaining a constant memory footprint, enabling streaming operation. To enhance efficiency, we applied structured channel pruning, achieving an 8X reduction in model size without compromising audio quality. Our model demonstrates strong results in the Interspeech 2020 Deep Noise Suppression challenge. Specifically, CleanUMamba achieves a PESQ score of 2.42 and STOI of 95.1% with only 442K parameters and 468M MACs, matching or outperforming larger models in real-time performance. Code will be available at: https://github.com/lab-emi/CleanUMamba",
    "published": "2024-10-14",
    "link": "http://arxiv.org/abs/2410.11062v1",
    "code_url": null
  },
  {
    "title": "V2M: Visual 2-Dimensional Mamba for Image Representation Learning",
    "author": "Chengkun Wang, Wenzhao Zheng, Yuanhui Huang, Jie Zhou, Jiwen Lu",
    "summary": "Mamba has garnered widespread attention due to its flexible design and efficient hardware performance to process 1D sequences based on the state space model (SSM). Recent studies have attempted to apply Mamba to the visual domain by flattening 2D images into patches and then regarding them as a 1D sequence. To compensate for the 2D structure information loss (e.g., local similarity) of the original image, most existing methods focus on designing different orders to sequentially process the tokens, which could only alleviate this issue to some extent. In this paper, we propose a Visual 2-Dimensional Mamba (V2M) model as a complete solution, which directly processes image tokens in the 2D space. We first generalize SSM to the 2-dimensional space which generates the next state considering two adjacent states on both dimensions (e.g., columns and rows). We then construct our V2M based on the 2-dimensional SSM formulation and incorporate Mamba to achieve hardware-efficient parallel processing. The proposed V2M effectively incorporates the 2D locality prior yet inherits the efficiency and input-dependent scalability of Mamba. Extensive experimental results on ImageNet classification and downstream visual tasks including object detection and instance segmentation on COCO and semantic segmentation on ADE20K demonstrate the effectiveness of our V2M compared with other visual backbones.",
    "published": "2024-10-14",
    "link": "http://arxiv.org/abs/2410.10382v1",
    "code_url": "https://github.com/wangck20/v2m"
  },
  {
    "title": "GlobalMamba: Global Image Serialization for Vision Mamba",
    "author": "Chengkun Wang, Wenzhao Zheng, Jie Zhou, Jiwen Lu",
    "summary": "Vision mambas have demonstrated strong performance with linear complexity to the number of vision tokens. Their efficiency results from processing image tokens sequentially. However, most existing methods employ patch-based image tokenization and then flatten them into 1D sequences for causal processing, which ignore the intrinsic 2D structural correlations of images. It is also difficult to extract global information by sequential processing of local patches. In this paper, we propose a global image serialization method to transform the image into a sequence of causal tokens, which contain global information of the 2D image. We first convert the image from the spatial domain to the frequency domain using Discrete Cosine Transform (DCT) and then arrange the pixels with corresponding frequency ranges. We further transform each set within the same frequency band back to the spatial domain to obtain a series of images before tokenization. We construct a vision mamba model, GlobalMamba, with a causal input format based on the proposed global image serialization, which can better exploit the causal relations among image sequences. Extensive experiments demonstrate the effectiveness of our GlobalMamba, including image classification on ImageNet-1K, object detection on COCO, and semantic segmentation on ADE20K.",
    "published": "2024-10-14",
    "link": "http://arxiv.org/abs/2410.10316v1",
    "code_url": "https://github.com/wangck20/globalmamba"
  },
  {
    "title": "Hi-Mamba: Hierarchical Mamba for Efficient Image Super-Resolution",
    "author": "Junbo Qiao, Jincheng Liao, Wei Li, Yulun Zhang, Yong Guo, Yi Wen, Zhangxizi Qiu, Jiao Xie, Jie Hu, Shaohui Lin",
    "summary": "State Space Models (SSM), such as Mamba, have shown strong representation ability in modeling long-range dependency with linear complexity, achieving successful applications from high-level to low-level vision tasks. However, SSM's sequential nature necessitates multiple scans in different directions to compensate for the loss of spatial dependency when unfolding the image into a 1D sequence. This multi-direction scanning strategy significantly increases the computation overhead and is unbearable for high-resolution image processing. To address this problem, we propose a novel Hierarchical Mamba network, namely, Hi-Mamba, for image super-resolution (SR). Hi-Mamba consists of two key designs: (1) The Hierarchical Mamba Block (HMB) assembled by a Local SSM (L-SSM) and a Region SSM (R-SSM) both with the single-direction scanning, aggregates multi-scale representations to enhance the context modeling ability. (2) The Direction Alternation Hierarchical Mamba Group (DA-HMG) allocates the isomeric single-direction scanning into cascading HMBs to enrich the spatial relationship modeling. Extensive experiments demonstrate the superiority of Hi-Mamba across five benchmark datasets for efficient SR. For example, Hi-Mamba achieves a significant PSNR improvement of 0.29 dB on Manga109 for $\\times3$ SR, compared to the strong lightweight MambaIR.",
    "published": "2024-10-14",
    "link": "http://arxiv.org/abs/2410.10140v1",
    "code_url": null
  },
  {
    "title": "SlimSeiz: Efficient Channel-Adaptive Seizure Prediction Using a Mamba-Enhanced Network",
    "author": "Guorui Lu, Jing Peng, Bingyuan Huang, Chang Gao, Todor Stefanov, Yong Hao, Qinyu Chen",
    "summary": "Epileptic seizures cause abnormal brain activity, and their unpredictability can lead to accidents, underscoring the need for long-term seizure prediction. Although seizures can be predicted by analyzing electroencephalogram (EEG) signals, existing methods often require too many electrode channels or larger models, limiting mobile usability. This paper introduces a SlimSeiz framework that utilizes adaptive channel selection with a lightweight neural network model. SlimSeiz operates in two states: the first stage selects the optimal channel set for seizure prediction using machine learning algorithms, and the second stage employs a lightweight neural network based on convolution and Mamba for prediction. On the Children's Hospital Boston-MIT (CHB-MIT) EEG dataset, SlimSeiz can reduce channels from 22 to 8 while achieving a satisfactory result of 94.8% accuracy, 95.5% sensitivity, and 94.0% specificity with only 21.2K model parameters, matching or outperforming larger models' performance. We also validate SlimSeiz on a new EEG dataset, SRH-LEI, collected from Shanghai Renji Hospital, demonstrating its effectiveness across different patients. The code and SRH-LEI dataset are available at https://github.com/guoruilu/SlimSeiz.",
    "published": "2024-10-13",
    "link": "http://arxiv.org/abs/2410.09998v1",
    "code_url": null
  },
  {
    "title": "Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models",
    "author": "Sathya Kamesh Bhethanabhotla, Omar Swelam, Julien Siems, David Salinas, Frank Hutter",
    "summary": "This paper introduces Mamba4Cast, a zero-shot foundation model for time series forecasting. Based on the Mamba architecture and inspired by Prior-data Fitted Networks (PFNs), Mamba4Cast generalizes robustly across diverse time series tasks without the need for dataset specific fine-tuning. Mamba4Cast's key innovation lies in its ability to achieve strong zero-shot performance on real-world datasets while having much lower inference times than time series foundation models based on the transformer architecture. Trained solely on synthetic data, the model generates forecasts for entire horizons in a single pass, outpacing traditional auto-regressive approaches. Our experiments show that Mamba4Cast performs competitively against other state-of-the-art foundation models in various data sets while scaling significantly better with the prediction length. The source code can be accessed at https://github.com/automl/Mamba4Cast.",
    "published": "2024-10-12",
    "link": "http://arxiv.org/abs/2410.09385v1",
    "code_url": null
  },
  {
    "title": "Parameter-Efficient Fine-Tuning of State Space Models",
    "author": "Kevin Galim, Wonjun Kang, Yuchen Zeng, Hyung Il Koo, Kangwook Lee",
    "summary": "Deep State Space Models (SSMs), such as Mamba (Gu & Dao, 2024), have emerged as powerful tools for language modeling, offering high performance with efficient inference and linear scaling in sequence length. However, the application of parameter-efficient fine-tuning (PEFT) methods to SSM-based models remains largely unexplored. This paper aims to systematically study two key questions: (i) How do existing PEFT methods perform on SSM-based models? (ii) Which modules are most effective for fine-tuning? We conduct an empirical benchmark of four basic PEFT methods on SSM-based models. Our findings reveal that prompt-based methods (e.g., prefix-tuning) are no longer effective, an empirical result further supported by theoretical analysis. In contrast, LoRA remains effective for SSM-based models. We further investigate the optimal application of LoRA within these models, demonstrating both theoretically and experimentally that applying LoRA to linear projection matrices without modifying SSM modules yields the best results, as LoRA is not effective at tuning SSM modules. To further improve performance, we introduce LoRA with Selective Dimension tuning (SDLoRA), which selectively updates certain channels and states on SSM modules while applying LoRA to linear projection matrices. Extensive experimental results show that this approach outperforms standard LoRA.",
    "published": "2024-10-11",
    "link": "http://arxiv.org/abs/2410.09016v1",
    "code_url": "https://github.com/furiosa-ai/ssm-peft"
  },
  {
    "title": "Drama: Mamba-Enabled Model-Based Reinforcement Learning Is Sample and Parameter Efficient",
    "author": "Wenlong Wang, Ivana Dusparic, Yucheng Shi, Ke Zhang, Vinny Cahill",
    "summary": "Model-based reinforcement learning (RL) offers a solution to the data inefficiency that plagues most model-free RL algorithms. However, learning a robust world model often demands complex and deep architectures, which are expensive to compute and train. Within the world model, dynamics models are particularly crucial for accurate predictions, and various dynamics-model architectures have been explored, each with its own set of challenges. Currently, recurrent neural network (RNN) based world models face issues such as vanishing gradients and difficulty in capturing long-term dependencies effectively. In contrast, use of transformers suffers from the well-known issues of self-attention mechanisms, where both memory and computational complexity scale as $O(n^2)$, with $n$ representing the sequence length.   To address these challenges we propose a state space model (SSM) based world model, specifically based on Mamba, that achieves $O(n)$ memory and computational complexity while effectively capturing long-term dependencies and facilitating the use of longer training sequences efficiently. We also introduce a novel sampling method to mitigate the suboptimality caused by an incorrect world model in the early stages of training, combining it with the aforementioned technique to achieve a normalised score comparable to other state-of-the-art model-based RL algorithms using only a 7 million trainable parameter world model. This model is accessible and can be trained on an off-the-shelf laptop. Our code is available at https://github.com/realwenlongwang/drama.git.",
    "published": "2024-10-11",
    "link": "http://arxiv.org/abs/2410.08893v1",
    "code_url": "https://github.com/realwenlongwang/drama"
  },
  {
    "title": "Moyun: A Diffusion-Based Model for Style-Specific Chinese Calligraphy Generation",
    "author": "Kaiyuan Liu, Jiahao Mei, Hengyu Zhang, Yihuai Zhang, Xingjiao Wu, Daoguo Dong, Liang He",
    "summary": "Although Chinese calligraphy generation has achieved style transfer, generating calligraphy by specifying the calligrapher, font, and character style remains challenging. To address this, we propose a new Chinese calligraphy generation model 'Moyun' , which replaces the Unet in the Diffusion model with Vision Mamba and introduces the TripleLabel control mechanism to achieve controllable calligraphy generation. The model was tested on our large-scale dataset 'Mobao' of over 1.9 million images, and the results demonstrate that 'Moyun' can effectively control the generation process and produce calligraphy in the specified style. Even for calligraphy the calligrapher has not written, 'Moyun' can generate calligraphy that matches the style of the calligrapher.",
    "published": "2024-10-10",
    "link": "http://arxiv.org/abs/2410.07618v1",
    "code_url": null
  },
  {
    "title": "Stuffed Mamba: State Collapse and State Capacity of RNN-Based Long-Context Modeling",
    "author": "Yingfa Chen, Xinrong Zhang, Shengding Hu, Xu Han, Zhiyuan Liu, Maosong Sun",
    "summary": "One essential advantage of recurrent neural networks (RNNs) over transformer-based language models is their linear computational complexity concerning the sequence length, which makes them much faster in handling long sequences during inference. However, most publicly available RNNs (e.g., Mamba and RWKV) are trained on sequences with less than 10K tokens, and their effectiveness in longer contexts remains largely unsatisfying so far. In this paper, we study the cause of the inability to process long context for RNNs and suggest critical mitigations. We examine two practical concerns when applying state-of-the-art RNNs to long contexts: (1) the inability to extrapolate to inputs longer than the training length and (2) the upper bound of memory capacity. Addressing the first concern, we first investigate *state collapse* (SC), a phenomenon that causes severe performance degradation on sequence lengths not encountered during training. With controlled experiments, we attribute this to overfitting due to the recurrent state being overparameterized for the training length. For the second concern, we train a series of Mamba-2 models on long documents to empirically estimate the recurrent state capacity in language modeling and passkey retrieval. Then, three SC mitigation methods are proposed to improve Mamba-2's length generalizability, allowing the model to process more than 1M tokens without SC. We also find that the recurrent state capacity in passkey retrieval scales exponentially to the state size, and we empirically train a Mamba-2 370M with near-perfect passkey retrieval accuracy on 256K context length. This suggests a promising future for RNN-based long-context modeling.",
    "published": "2024-10-09",
    "link": "http://arxiv.org/abs/2410.07145v1",
    "code_url": null
  },
  {
    "title": "Joint Fine-tuning and Conversion of Pretrained Speech and Language Models towards Linear Complexity",
    "author": "Mutian He, Philip N. Garner",
    "summary": "Architectures such as Linformer and Mamba have recently emerged as competitive linear time replacements for transformers. However, corresponding large pretrained models are often unavailable, especially in non-text domains. To remedy this, we present a Cross-Architecture Layerwise Distillation (CALD) approach that jointly converts a transformer model to a linear time substitute and fine-tunes it to a target task. We also compare several means to guide the fine-tuning to optimally retain the desired inference capability from the original model. The methods differ in their use of the target model and the trajectory of the parameters. In a series of empirical studies on language processing, language modeling, and speech processing, we show that CALD can effectively recover the result of the original model, and that the guiding strategy contributes to the result. Some reasons for the variation are suggested.",
    "published": "2024-10-09",
    "link": "http://arxiv.org/abs/2410.06846v1",
    "code_url": null
  },
  {
    "title": "QuadMamba: Learning Quadtree-based Selective Scan for Visual State Space Model",
    "author": "Fei Xie, Weijia Zhang, Zhongdao Wang, Chao Ma",
    "summary": "Recent advancements in State Space Models, notably Mamba, have demonstrated superior performance over the dominant Transformer models, particularly in reducing the computational complexity from quadratic to linear. Yet, difficulties in adapting Mamba from language to vision tasks arise due to the distinct characteristics of visual data, such as the spatial locality and adjacency within images and large variations in information granularity across visual tokens. Existing vision Mamba approaches either flatten tokens into sequences in a raster scan fashion, which breaks the local adjacency of images, or manually partition tokens into windows, which limits their long-range modeling and generalization capabilities. To address these limitations, we present a new vision Mamba model, coined QuadMamba, that effectively captures local dependencies of varying granularities via quadtree-based image partition and scan. Concretely, our lightweight quadtree-based scan module learns to preserve the 2D locality of spatial regions within learned window quadrants. The module estimates the locality score of each token from their features, before adaptively partitioning tokens into window quadrants. An omnidirectional window shifting scheme is also introduced to capture more intact and informative features across different local regions. To make the discretized quadtree partition end-to-end trainable, we further devise a sequence masking strategy based on Gumbel-Softmax and its straight-through gradient estimator. Extensive experiments demonstrate that QuadMamba achieves state-of-the-art performance in various vision tasks, including image classification, object detection, instance segmentation, and semantic segmentation. The code is in https://github.com/VISION-SJTU/QuadMamba.",
    "published": "2024-10-09",
    "link": "http://arxiv.org/abs/2410.06806v2",
    "code_url": "https://github.com/vision-sjtu/quadmamba"
  },
  {
    "title": "Towards Universality: Studying Mechanistic Similarity Across Language Model Architectures",
    "author": "Junxuan Wang, Xuyang Ge, Wentao Shu, Qiong Tang, Yunhua Zhou, Zhengfu He, Xipeng Qiu",
    "summary": "The hypothesis of Universality in interpretability suggests that different neural networks may converge to implement similar algorithms on similar tasks. In this work, we investigate two mainstream architectures for language modeling, namely Transformers and Mambas, to explore the extent of their mechanistic similarity. We propose to use Sparse Autoencoders (SAEs) to isolate interpretable features from these models and show that most features are similar in these two models. We also validate the correlation between feature similarity and Universality. We then delve into the circuit-level analysis of Mamba models and find that the induction circuits in Mamba are structurally analogous to those in Transformers. We also identify a nuanced difference we call \\emph{Off-by-One motif}: The information of one token is written into the SSM state in its next position. Whilst interaction between tokens in Transformers does not exhibit such trend.",
    "published": "2024-10-09",
    "link": "http://arxiv.org/abs/2410.06672v2",
    "code_url": null
  },
  {
    "title": "Mamba-based Segmentation Model for Speaker Diarization",
    "author": "Alexis Plaquet, Naohiro Tawara, Marc Delcroix, Shota Horiguchi, Atsushi Ando, Shoko Araki",
    "summary": "Mamba is a newly proposed architecture which behaves like a recurrent neural network (RNN) with attention-like capabilities. These properties are promising for speaker diarization, as attention-based models have unsuitable memory requirements for long-form audio, and traditional RNN capabilities are too limited. In this paper, we propose to assess the potential of Mamba for diarization by comparing the state-of-the-art neural segmentation of the pyannote pipeline with our proposed Mamba-based variant. Mamba's stronger processing capabilities allow usage of longer local windows, which significantly improve diarization quality by making the speaker embedding extraction more reliable. We find Mamba to be a superior alternative to both traditional RNN and the tested attention-based model. Our proposed Mamba-based system achieves state-of-the-art performance on three widely used diarization datasets.",
    "published": "2024-10-09",
    "link": "http://arxiv.org/abs/2410.06459v2",
    "code_url": "https://github.com/nttcslab-sp/mamba-diarization"
  },
  {
    "title": "DeMo: Decoupling Motion Forecasting into Directional Intentions and Dynamic States",
    "author": "Bozhou Zhang, Nan Song, Li Zhang",
    "summary": "Accurate motion forecasting for traffic agents is crucial for ensuring the safety and efficiency of autonomous driving systems in dynamically changing environments. Mainstream methods adopt a one-query-one-trajectory paradigm, where each query corresponds to a unique trajectory for predicting multi-modal trajectories. While straightforward and effective, the absence of detailed representation of future trajectories may yield suboptimal outcomes, given that the agent states dynamically evolve over time. To address this problem, we introduce DeMo, a framework that decouples multi-modal trajectory queries into two types: mode queries capturing distinct directional intentions and state queries tracking the agent's dynamic states over time. By leveraging this format, we separately optimize the multi-modality and dynamic evolutionary properties of trajectories. Subsequently, the mode and state queries are integrated to obtain a comprehensive and detailed representation of the trajectories. To achieve these operations, we additionally introduce combined Attention and Mamba techniques for global information aggregation and state sequence modeling, leveraging their respective strengths. Extensive experiments on both the Argoverse 2 and nuScenes benchmarks demonstrate that our DeMo achieves state-of-the-art performance in motion forecasting.",
    "published": "2024-10-08",
    "link": "http://arxiv.org/abs/2410.05982v1",
    "code_url": "https://github.com/fudan-zvg/demo"
  },
  {
    "title": "EMMA: Empowering Multi-modal Mamba with Structural and Hierarchical Alignment",
    "author": "Yifei Xing, Xiangyuan Lan, Ruiping Wang, Dongmei Jiang, Wenjun Huang, Qingfang Zheng, Yaowei Wang",
    "summary": "Mamba-based architectures have shown to be a promising new direction for deep learning models owing to their competitive performance and sub-quadratic deployment speed. However, current Mamba multi-modal large language models (MLLM) are insufficient in extracting visual features, leading to imbalanced cross-modal alignment between visual and textural latents, negatively impacting performance on multi-modal tasks. In this work, we propose Empowering Multi-modal Mamba with Structural and Hierarchical Alignment (EMMA), which enables the MLLM to extract fine-grained visual information. Specifically, we propose a pixel-wise alignment module to autoregressively optimize the learning and processing of spatial image-level features along with textual tokens, enabling structural alignment at the image level. In addition, to prevent the degradation of visual information during the cross-model alignment process, we propose a multi-scale feature fusion (MFF) module to combine multi-scale visual features from intermediate layers, enabling hierarchical alignment at the feature level. Extensive experiments are conducted across a variety of multi-modal benchmarks. Our model shows lower latency than other Mamba-based MLLMs and is nearly four times faster than transformer-based MLLMs of similar scale during inference. Due to better cross-modal alignment, our model exhibits lower degrees of hallucination and enhanced sensitivity to visual details, which manifests in superior performance across diverse multi-modal benchmarks. Code will be provided.",
    "published": "2024-10-08",
    "link": "http://arxiv.org/abs/2410.05938v1",
    "code_url": null
  },
  {
    "title": "TIMBA: Time series Imputation with Bi-directional Mamba Blocks and Diffusion models",
    "author": "Javier Sol\u00eds-Garc\u00eda, Bel\u00e9n Vega-M\u00e1rquez, Juan A. Nepomuceno, Isabel A. Nepomuceno-Chamorro",
    "summary": "The problem of imputing multivariate time series spans a wide range of fields, from clinical healthcare to multi-sensor systems. Initially, Recurrent Neural Networks (RNNs) were employed for this task; however, their error accumulation issues led to the adoption of Transformers, leveraging attention mechanisms to mitigate these problems. Concurrently, the promising results of diffusion models in capturing original distributions have positioned them at the forefront of current research, often in conjunction with Transformers. In this paper, we propose replacing time-oriented Transformers with State-Space Models (SSM), which are better suited for temporal data modeling. Specifically, we utilize the latest SSM variant, S6, which incorporates attention-like mechanisms. By embedding S6 within Mamba blocks, we develop a model that integrates SSM, Graph Neural Networks, and node-oriented Transformers to achieve enhanced spatiotemporal representations. Implementing these architectural modifications, previously unexplored in this field, we present Time series Imputation with Bi-directional mamba blocks and diffusion models (TIMBA). TIMBA achieves superior performance in almost all benchmark scenarios and performs comparably in others across a diverse range of missing value situations and three real-world datasets. We also evaluate how the performance of our model varies with different amounts of missing values and analyse its performance on downstream tasks. In addition, we provide the original code to replicate the results.",
    "published": "2024-10-08",
    "link": "http://arxiv.org/abs/2410.05916v1",
    "code_url": null
  },
  {
    "title": "Adaptive Prompt Learning with SAM for Few-shot Scanning Probe Microscope Image Segmentation",
    "author": "Yao Shen, Ziwei Wei, Chunmeng Liu, Shuming Wei, Qi Zhao, Kaiyang Zeng, Guangyao Li",
    "summary": "The Segment Anything Model (SAM) has demonstrated strong performance in image segmentation of natural scene images. However, its effectiveness diminishes markedly when applied to specific scientific domains, such as Scanning Probe Microscope (SPM) images. This decline in accuracy can be attributed to the distinct data distribution and limited availability of the data inherent in the scientific images. On the other hand, the acquisition of adequate SPM datasets is both time-intensive and laborious as well as skill-dependent. To address these challenges, we propose an Adaptive Prompt Learning with SAM (APL-SAM) framework tailored for few-shot SPM image segmentation. Our approach incorporates two key innovations to enhance SAM: 1) An Adaptive Prompt Learning module leverages few-shot embeddings derived from limited support set to learn adaptively central representatives, serving as visual prompts. This innovation eliminates the need for time-consuming online user interactions for providing prompts, such as exhaustively marking points and bounding boxes slice by slice; 2) A multi-source, multi-level mask decoder specifically designed for few-shot SPM image segmentation is introduced, which can effectively capture the correspondence between the support and query images. To facilitate comprehensive training and evaluation, we introduce a new dataset, SPM-Seg, curated for SPM image segmentation. Extensive experiments on this dataset reveal that the proposed APL-SAM framework significantly outperforms the original SAM, achieving over a 30% improvement in terms of Dice Similarity Coefficient with only one-shot guidance. Moreover, APL-SAM surpasses state-of-the-art few-shot segmentation methods and even fully supervised approaches in performance. Code and dataset used in this study will be made available upon acceptance.",
    "published": "2024-10-16",
    "link": "http://arxiv.org/abs/2410.12562v1",
    "code_url": null
  },
  {
    "title": "A Holistic Weakly Supervised Approach for Liver Tumor Segmentation with Clinical Knowledge-Informed Label Smoothing",
    "author": "Hairong Wang, Lingchao Mao, Zihan Zhang, Jing Li",
    "summary": "Liver cancer is a leading cause of mortality worldwide, and accurate CT-based tumor segmentation is essential for diagnosis and treatment. Manual delineation is time-intensive, prone to variability, and highlights the need for reliable automation. While deep learning has shown promise for automated liver segmentation, precise liver tumor segmentation remains challenging due to the heterogeneous nature of tumors, imprecise tumor margins, and limited labeled data. We present a novel holistic weakly supervised framework that integrates clinical knowledge to address these challenges with (1) A knowledge-informed label smoothing technique that leverages clinical data to generate smooth labels, which regularizes model training reducing the risk of overfitting and enhancing model performance; (2) A global and local-view segmentation framework, breaking down the task into two simpler sub-tasks, allowing optimized preprocessing and training for each; and (3) Pre- and post-processing pipelines customized to the challenges of each subtask, which enhances tumor visibility and refines tumor boundaries. We evaluated the proposed method on the HCC-TACE-Seg dataset and showed that these three key components complementarily contribute to the improved performance. Lastly, we prototyped a tool for automated liver tumor segmentation and diagnosis summary generation called MedAssistLiver. The app and code are published at https://github.com/lingchm/medassist-liver-cancer.",
    "published": "2024-10-13",
    "link": "http://arxiv.org/abs/2410.10005v1",
    "code_url": "https://github.com/lingchm/medassist-liver-cancer"
  },
  {
    "title": "Iterative Optimization Annotation Pipeline and ALSS-YOLO-Seg for Efficient Banana Plantation Segmentation in UAV Imagery",
    "author": "Ang He, Ximei Wu, Xing Xu, Jing Chen, Xiaobin Guo, Sheng Xu",
    "summary": "Precise segmentation of Unmanned Aerial Vehicle (UAV)-captured images plays a vital role in tasks such as crop yield estimation and plant health assessment in banana plantations. By identifying and classifying planted areas, crop area can be calculated, which is indispensable for accurate yield predictions. However, segmenting banana plantation scenes requires a substantial amount of annotated data, and manual labeling of these images is both time-consuming and labor-intensive, limiting the development of large-scale datasets. Furthermore, challenges such as changing target sizes, complex ground backgrounds, limited computational resources, and correct identification of crop categories make segmentation even more difficult. To address these issues, we proposed a comprehensive solution. Firstly, we designed an iterative optimization annotation pipeline leveraging SAM2's zero-shot capabilities to generate high-quality segmentation annotations, thereby reducing the cost and time associated with data annotation significantly. Secondly, we developed ALSS-YOLO-Seg, an efficient lightweight segmentation model optimized for UAV imagery. The model's backbone includes an Adaptive Lightweight Channel Splitting and Shuffling (ALSS) module to improve information exchange between channels and optimize feature extraction, aiding accurate crop identification. Additionally, a Multi-Scale Channel Attention (MSCA) module combines multi-scale feature extraction with channel attention to tackle challenges of varying target sizes and complex ground backgrounds.",
    "published": "2024-10-09",
    "link": "http://arxiv.org/abs/2410.07955v1",
    "code_url": "https://github.com/helloworlder8/computer_vision"
  },
  {
    "title": "Leveraging CAM Algorithms for Explaining Medical Semantic Segmentation",
    "author": "Tillmann Rheude, Andreas Wirtz, Arjan Kuijper, Stefan Wesarg",
    "summary": "Convolutional neural networks (CNNs) achieve prevailing results in segmentation tasks nowadays and represent the state-of-the-art for image-based analysis. However, the understanding of the accurate decision-making process of a CNN is rather unknown. The research area of explainable artificial intelligence (xAI) primarily revolves around understanding and interpreting this black-box behavior. One way of interpreting a CNN is the use of class activation maps (CAMs) that represent heatmaps to indicate the importance of image areas for the prediction of the CNN. For classification tasks, a variety of CAM algorithms exist. But for segmentation tasks, only one CAM algorithm for the interpretation of the output of a CNN exist. We propose a transfer between existing classification- and segmentation-based methods for more detailed, explainable, and consistent results which show salient pixels in semantic segmentation tasks. The resulting Seg-HiRes-Grad CAM is an extension of the segmentation-based Seg-Grad CAM with the transfer to the classification-based HiRes CAM. Our method improves the previously-mentioned existing segmentation-based method by adjusting it to recently published classification-based methods. Especially for medical image segmentation, this transfer solves existing explainability disadvantages.",
    "published": "2024-09-30",
    "link": "http://arxiv.org/abs/2409.20287v1",
    "code_url": "https://github.com/TillmannRheude/SegHiResGrad_CAM"
  },
  {
    "title": "One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos",
    "author": "Zechen Bai, Tong He, Haiyang Mei, Pichao Wang, Ziteng Gao, Joya Chen, Lei Liu, Zheng Zhang, Mike Zheng Shou",
    "summary": "We introduce VideoLISA, a video-based multimodal large language model designed to tackle the problem of language-instructed reasoning segmentation in videos. Leveraging the reasoning capabilities and world knowledge of large language models, and augmented by the Segment Anything Model, VideoLISA generates temporally consistent segmentation masks in videos based on language instructions. Existing image-based methods, such as LISA, struggle with video tasks due to the additional temporal dimension, which requires temporal dynamic understanding and consistent segmentation across frames. VideoLISA addresses these challenges by integrating a Sparse Dense Sampling strategy into the video-LLM, which balances temporal context and spatial detail within computational constraints. Additionally, we propose a One-Token-Seg-All approach using a specially designed <TRK> token, enabling the model to segment and track objects across multiple frames. Extensive evaluations on diverse benchmarks, including our newly introduced ReasonVOS benchmark, demonstrate VideoLISA's superior performance in video object segmentation tasks involving complex reasoning, temporal understanding, and object tracking. While optimized for videos, VideoLISA also shows promising generalization to image segmentation, revealing its potential as a unified foundation model for language-instructed object segmentation. Code and model will be available at: https://github.com/showlab/VideoLISA.",
    "published": "2024-09-29",
    "link": "http://arxiv.org/abs/2409.19603v1",
    "code_url": null
  },
  {
    "title": "Diff-VPS: Video Polyp Segmentation via a Multi-task Diffusion Network with Adversarial Temporal Reasoning",
    "author": "Yingling Lu, Yijun Yang, Zhaohu Xing, Qiong Wang, Lei Zhu",
    "summary": "Diffusion Probabilistic Models have recently attracted significant attention in the community of computer vision due to their outstanding performance. However, while a substantial amount of diffusion-based research has focused on generative tasks, no work introduces diffusion models to advance the results of polyp segmentation in videos, which is frequently challenged by polyps' high camouflage and redundant temporal cues.In this paper, we present a novel diffusion-based network for video polyp segmentation task, dubbed as Diff-VPS. We incorporate multi-task supervision into diffusion models to promote the discrimination of diffusion models on pixel-by-pixel segmentation. This integrates the contextual high-level information achieved by the joint classification and detection tasks. To explore the temporal dependency, Temporal Reasoning Module (TRM) is devised via reasoning and reconstructing the target frame from the previous frames. We further equip TRM with a generative adversarial self-supervised strategy to produce more realistic frames and thus capture better dynamic cues. Extensive experiments are conducted on SUN-SEG, and the results indicate that our proposed Diff-VPS significantly achieves state-of-the-art performance. Code is available at https://github.com/lydia-yllu/Diff-VPS.",
    "published": "2024-09-11",
    "link": "http://arxiv.org/abs/2409.07238v1",
    "code_url": "https://github.com/lydia-yllu/diff-vps"
  },
  {
    "title": "Seg-HGNN: Unsupervised and Light-Weight Image Segmentation with Hyperbolic Graph Neural Networks",
    "author": "Debjyoti Mondal, Rahul Mishra, Chandan Pandey",
    "summary": "Image analysis in the euclidean space through linear hyperspaces is well studied. However, in the quest for more effective image representations, we turn to hyperbolic manifolds. They provide a compelling alternative to capture complex hierarchical relationships in images with remarkably small dimensionality. To demonstrate hyperbolic embeddings' competence, we introduce a light-weight hyperbolic graph neural network for image segmentation, encompassing patch-level features in a very small embedding size. Our solution, Seg-HGNN, surpasses the current best unsupervised method by 2.5\\%, 4\\% on VOC-07, VOC-12 for localization, and by 0.8\\%, 1.3\\% on CUB-200, ECSSD for segmentation, respectively. With less than 7.5k trainable parameters, Seg-HGNN delivers effective and fast ($\\approx 2$ images/second) results on very standard GPUs like the GTX1650. This empirical evaluation presents compelling evidence of the efficacy and potential of hyperbolic representations for vision tasks.",
    "published": "2024-09-10",
    "link": "http://arxiv.org/abs/2409.06589v1",
    "code_url": null
  },
  {
    "title": "Segmenting Object Affordances: Reproducibility and Sensitivity to Scale",
    "author": "Tommaso Apicella, Alessio Xompero, Paolo Gastaldo, Andrea Cavallaro",
    "summary": "Visual affordance segmentation identifies image regions of an object an agent can interact with. Existing methods re-use and adapt learning-based architectures for semantic segmentation to the affordance segmentation task and evaluate on small-size datasets. However, experimental setups are often not reproducible, thus leading to unfair and inconsistent comparisons. In this work, we benchmark these methods under a reproducible setup on two single objects scenarios, tabletop without occlusions and hand-held containers, to facilitate future comparisons. We include a version of a recent architecture, Mask2Former, re-trained for affordance segmentation and show that this model is the best-performing on most testing sets of both scenarios. Our analysis shows that models are not robust to scale variations when object resolutions differ from those in the training set.",
    "published": "2024-09-03",
    "link": "http://arxiv.org/abs/2409.01814v1",
    "code_url": "https://github.com/apicis/aff-seg"
  },
  {
    "title": "LSMS: Language-guided Scale-aware MedSegmentor for Medical Image Referring Segmentation",
    "author": "Shuyi Ouyang, Jinyang Zhang, Xiangye Lin, Xilai Wang, Qingqing Chen, Yen-Wei Chen, Lanfen Lin",
    "summary": "Conventional medical image segmentation methods have been found inadequate in facilitating physicians with the identification of specific lesions for diagnosis and treatment. Given the utility of text as an instructional format, we introduce a novel task termed Medical Image Referring Segmentation (MIRS), which requires segmenting specified lesions in images based on the given language expressions. Due to the varying object scales in medical images, MIRS demands robust vision-language modeling and comprehensive multi-scale interaction for precise localization and segmentation under linguistic guidance. However, existing medical image segmentation methods fall short in meeting these demands, resulting in insufficient segmentation accuracy. In response, we propose an approach named Language-guided Scale-aware MedSegmentor (LSMS), incorporating two appealing designs: (1)~a Scale-aware Vision-Language Attention module that leverages diverse convolutional kernels to acquire rich visual knowledge and interact closely with linguistic features, thereby enhancing lesion localization capability; (2)~a Full-Scale Decoder that globally models multi-modal features across various scales, capturing complementary information between scales to accurately outline lesion boundaries. Addressing the lack of suitable datasets for MIRS, we constructed a vision-language medical dataset called Reference Hepatic Lesion Segmentation (RefHL-Seg). This dataset comprises 2,283 abdominal CT slices from 231 cases, with corresponding textual annotations and segmentation masks for various liver lesions in images. We validated the performance of LSMS for MIRS and conventional medical image segmentation tasks across various datasets. Our LSMS consistently outperforms on all datasets with lower computational costs. The code and datasets will be released.",
    "published": "2024-08-30",
    "link": "http://arxiv.org/abs/2408.17347v2",
    "code_url": null
  },
  {
    "title": "LV-UNet: A Lightweight and Vanilla Model for Medical Image Segmentation",
    "author": "Juntao Jiang, Mengmeng Wang, Huizhong Tian, Lingbo Cheng, Yong Liu",
    "summary": "Although the progress made by large models in computer vision, optimization challenges, the complexity of transformer models, computational limitations, and the requirements of practical applications call for simpler designs in model architecture for medical image segmentation, especially in mobile medical devices that require lightweight and deployable models with real-time performance. However, some of the current lightweight models exhibit poor robustness across different datasets, which hinders their broader adoption. This paper proposes a lightweight and vanilla model called LV-UNet, which effectively utilizes pre-trained MobileNetv3-Large models and introduces fusible modules. It can be trained using an improved deep training strategy and switched to deployment mode during inference, reducing both parameter count and computational load. Experiments are conducted on ISIC 2016, BUSI, CVC- ClinicDB, CVC-ColonDB, and Kvair-SEG datasets, achieving better performance compared to the state-of-the-art and classic models.",
    "published": "2024-08-29",
    "link": "http://arxiv.org/abs/2408.16886v1",
    "code_url": null
  },
  {
    "title": "Sapiens: Foundation for Human Vision Models",
    "author": "Rawal Khirodkar, Timur Bagautdinov, Julieta Martinez, Su Zhaoen, Austin James, Peter Selednik, Stuart Anderson, Shunsuke Saito",
    "summary": "We present Sapiens, a family of models for four fundamental human-centric vision tasks -- 2D pose estimation, body-part segmentation, depth estimation, and surface normal prediction. Our models natively support 1K high-resolution inference and are extremely easy to adapt for individual tasks by simply fine-tuning models pretrained on over 300 million in-the-wild human images. We observe that, given the same computational budget, self-supervised pretraining on a curated dataset of human images significantly boosts the performance for a diverse set of human-centric tasks. The resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. Our simple model design also brings scalability -- model performance across tasks improves as we scale the number of parameters from 0.3 to 2 billion. Sapiens consistently surpasses existing baselines across various human-centric benchmarks. We achieve significant improvements over the prior state-of-the-art on Humans-5K (pose) by 7.6 mAP, Humans-2K (part-seg) by 17.1 mIoU, Hi4D (depth) by 22.4% relative RMSE, and THuman2 (normal) by 53.5% relative angular error. Project page: https://about.meta.com/realitylabs/codecavatars/sapiens.",
    "published": "2024-08-22",
    "link": "http://arxiv.org/abs/2408.12569v3",
    "code_url": null
  },
  {
    "title": "Tuning a SAM-Based Model with Multi-Cognitive Visual Adapter to Remote Sensing Instance Segmentation",
    "author": "Linghao Zheng, Xinyang Pu, Feng Xu",
    "summary": "The Segment Anything Model (SAM), a foundational model designed for promptable segmentation tasks, demonstrates exceptional generalization capabilities, making it highly promising for natural scene image segmentation. However, SAM's lack of pretraining on massive remote sensing images and its interactive structure limit its automatic mask prediction capabilities. In this paper, a Multi-Cognitive SAM-Based Instance Segmentation Model (MC-SAM SEG) is introduced to employ SAM on remote sensing domain. The SAM-Mona encoder utilizing the Multi-cognitive Visual Adapter (Mona) is conducted to facilitate SAM's transfer learning in remote sensing applications. The proposed method named MC-SAM SEG extracts high-quality features by fine-tuning the SAM-Mona encoder along with a feature aggregator. Subsequently, a pixel decoder and transformer decoder are designed for prompt-free mask generation and instance classification. The comprehensive experiments are conducted on the HRSID and WHU datasets for instance segmentation tasks on Synthetic Aperture Radar (SAR) images and optical remote sensing images respectively. The evaluation results indicate the proposed method surpasses other deep learning algorithms and verify its effectiveness and generalization.",
    "published": "2024-08-16",
    "link": "http://arxiv.org/abs/2408.08576v1",
    "code_url": null
  },
  {
    "title": "Multi-scale Contrastive Adaptor Learning for Segmenting Anything in Underperformed Scenes",
    "author": "Ke Zhou, Zhongwei Qiu, Dongmei Fu",
    "summary": "Foundational vision models, such as the Segment Anything Model (SAM), have achieved significant breakthroughs through extensive pre-training on large-scale visual datasets. Despite their general success, these models may fall short in specialized tasks with limited data, and fine-tuning such large-scale models is often not feasible. Current strategies involve incorporating adaptors into the pre-trained SAM to facilitate downstream task performance with minimal model adjustment. However, these strategies can be hampered by suboptimal learning approaches for the adaptors. In this paper, we introduce a novel Multi-scale Contrastive Adaptor learning method named MCA-SAM, which enhances adaptor performance through a meticulously designed contrastive learning framework at both token and sample levels. Our Token-level Contrastive adaptor (TC-adaptor) focuses on refining local representations by improving the discriminability of patch tokens, while the Sample-level Contrastive adaptor (SC-adaptor) amplifies global understanding across different samples. Together, these adaptors synergistically enhance feature comparison within and across samples, bolstering the model's representational strength and its ability to adapt to new tasks. Empirical results demonstrate that MCA-SAM sets new benchmarks, outperforming existing methods in three challenging domains: camouflage object detection, shadow segmentation, and polyp segmentation. Specifically, MCA-SAM exhibits substantial relative performance enhancements, achieving a 20.0% improvement in MAE on the COD10K dataset, a 6.0% improvement in MAE on the CAMO dataset, a 15.4% improvement in BER on the ISTD dataset, and a 7.9% improvement in mDice on the Kvasir-SEG dataset.",
    "published": "2024-08-12",
    "link": "http://arxiv.org/abs/2408.05936v1",
    "code_url": null
  },
  {
    "title": "Seg-CycleGAN : SAR-to-optical image translation guided by a downstream task",
    "author": "Hannuo Zhang, Huihui Li, Jiarui Lin, Yujie Zhang, Jianghua Fan, Hang Liu",
    "summary": "Optical remote sensing and Synthetic Aperture Radar(SAR) remote sensing are crucial for earth observation, offering complementary capabilities. While optical sensors provide high-quality images, they are limited by weather and lighting conditions. In contrast, SAR sensors can operate effectively under adverse conditions. This letter proposes a GAN-based SAR-to-optical image translation method named Seg-CycleGAN, designed to enhance the accuracy of ship target translation by leveraging semantic information from a pre-trained semantic segmentation model. Our method utilizes the downstream task of ship target semantic segmentation to guide the training of image translation network, improving the quality of output Optical-styled images. The potential of foundation-model-annotated datasets in SAR-to-optical translation tasks is revealed. This work suggests broader research and applications for downstream-task-guided frameworks. The code will be available at https://github.com/NPULHH/",
    "published": "2024-08-11",
    "link": "http://arxiv.org/abs/2408.05777v1",
    "code_url": null
  },
  {
    "title": "Embodied Uncertainty-Aware Object Segmentation",
    "author": "Xiaolin Fang, Leslie Pack Kaelbling, Tom\u00e1s Lozano-P\u00e9rez",
    "summary": "We introduce uncertainty-aware object instance segmentation (UncOS) and demonstrate its usefulness for embodied interactive segmentation. To deal with uncertainty in robot perception, we propose a method for generating a hypothesis distribution of object segmentation. We obtain a set of region-factored segmentation hypotheses together with confidence estimates by making multiple queries of large pre-trained models. This process can produce segmentation results that achieve state-of-the-art performance on unseen object segmentation problems. The output can also serve as input to a belief-driven process for selecting robot actions to perturb the scene to reduce ambiguity. We demonstrate the effectiveness of this method in real-robot experiments. Website: https://sites.google.com/view/embodied-uncertain-seg",
    "published": "2024-08-08",
    "link": "http://arxiv.org/abs/2408.04760v1",
    "code_url": null
  },
  {
    "title": "Smoothed Energy Guidance: Guiding Diffusion Models with Reduced Energy Curvature of Attention",
    "author": "Susung Hong",
    "summary": "Conditional diffusion models have shown remarkable success in visual content generation, producing high-quality samples across various domains, largely due to classifier-free guidance (CFG). Recent attempts to extend guidance to unconditional models have relied on heuristic techniques, resulting in suboptimal generation quality and unintended effects. In this work, we propose Smoothed Energy Guidance (SEG), a novel training- and condition-free approach that leverages the energy-based perspective of the self-attention mechanism to enhance image generation. By defining the energy of self-attention, we introduce a method to reduce the curvature of the energy landscape of attention and use the output as the unconditional prediction. Practically, we control the curvature of the energy landscape by adjusting the Gaussian kernel parameter while keeping the guidance scale parameter fixed. Additionally, we present a query blurring method that is equivalent to blurring the entire attention weights without incurring quadratic complexity in the number of tokens. In our experiments, SEG achieves a Pareto improvement in both quality and the reduction of side effects. The code is available at https://github.com/SusungHong/SEG-SDXL.",
    "published": "2024-08-01",
    "link": "http://arxiv.org/abs/2408.00760v2",
    "code_url": "https://github.com/susunghong/seg-sdxl"
  },
  {
    "title": "DDU-Net: A Domain Decomposition-based CNN for High-Resolution Image Segmentation on Multiple GPUs",
    "author": "Corn\u00e9 Verburg, Alexander Heinlein, Eric C. Cyr",
    "summary": "The segmentation of ultra-high resolution images poses challenges such as loss of spatial information or computational inefficiency. In this work, a novel approach that combines encoder-decoder architectures with domain decomposition strategies to address these challenges is proposed. Specifically, a domain decomposition-based U-Net (DDU-Net) architecture is introduced, which partitions input images into non-overlapping patches that can be processed independently on separate devices. A communication network is added to facilitate inter-patch information exchange to enhance the understanding of spatial context. Experimental validation is performed on a synthetic dataset that is designed to measure the effectiveness of the communication network. Then, the performance is tested on the DeepGlobe land cover classification dataset as a real-world benchmark data set. The results demonstrate that the approach, which includes inter-patch communication for images divided into $16\\times16$ non-overlapping subimages, achieves a $2-3\\,\\%$ higher intersection over union (IoU) score compared to the same network without inter-patch communication. The performance of the network which includes communication is equivalent to that of a baseline U-Net trained on the full image, showing that our model provides an effective solution for segmenting ultra-high-resolution images while preserving spatial context. The code is available at https://github.com/corne00/HiRes-Seg-CNN.",
    "published": "2024-07-31",
    "link": "http://arxiv.org/abs/2407.21266v2",
    "code_url": "https://github.com/corne00/hires-seg-cnn"
  },
  {
    "title": "ASI-Seg: Audio-Driven Surgical Instrument Segmentation with Surgeon Intention Understanding",
    "author": "Zhen Chen, Zongming Zhang, Wenwu Guo, Xingjian Luo, Long Bai, Jinlin Wu, Hongliang Ren, Hongbin Liu",
    "summary": "Surgical instrument segmentation is crucial in surgical scene understanding, thereby facilitating surgical safety. Existing algorithms directly detected all instruments of pre-defined categories in the input image, lacking the capability to segment specific instruments according to the surgeon's intention. During different stages of surgery, surgeons exhibit varying preferences and focus toward different surgical instruments. Therefore, an instrument segmentation algorithm that adheres to the surgeon's intention can minimize distractions from irrelevant instruments and assist surgeons to a great extent. The recent Segment Anything Model (SAM) reveals the capability to segment objects following prompts, but the manual annotations for prompts are impractical during the surgery. To address these limitations in operating rooms, we propose an audio-driven surgical instrument segmentation framework, named ASI-Seg, to accurately segment the required surgical instruments by parsing the audio commands of surgeons. Specifically, we propose an intention-oriented multimodal fusion to interpret the segmentation intention from audio commands and retrieve relevant instrument details to facilitate segmentation. Moreover, to guide our ASI-Seg segment of the required surgical instruments, we devise a contrastive learning prompt encoder to effectively distinguish the required instruments from the irrelevant ones. Therefore, our ASI-Seg promotes the workflow in the operating rooms, thereby providing targeted support and reducing the cognitive load on surgeons. Extensive experiments are performed to validate the ASI-Seg framework, which reveals remarkable advantages over classical state-of-the-art and medical SAMs in both semantic segmentation and intention-oriented segmentation. The source code is available at https://github.com/Zonmgin-Zhang/ASI-Seg.",
    "published": "2024-07-28",
    "link": "http://arxiv.org/abs/2407.19435v1",
    "code_url": "https://github.com/zonmgin-zhang/asi-seg"
  },
  {
    "title": "Accurate background velocity model building method based on iterative deep learning in sparse transform domain",
    "author": "Guoxin Chen",
    "summary": "Whether it is oil and gas exploration or geological science research, it is necessary to accurately grasp the structural information of underground media. Full waveform inversion is currently the most popular seismic wave inversion method, but it is highly dependent on a high-quality initial model. Artificial intelligence algorithm deep learning is completely data-driven and can get rid of the dependence on the initial model. However, the prediction accuracy of deep learning algorithms depends on the scale and diversity of training data sets. How to improve the prediction accuracy of deep learning without increasing the size of the training set while also improving computing efficiency is a worthy issue to study. In this paper, an iterative deep learning algorithm in the sparse transform domain is proposed based on the characteristics of deep learning: first, based on the computational efficiency and the effect of sparse transform, the cosine transform is selected as the sparse transform method, and the seismic data and the corresponding velocity model are cosine transformed to obtain their corresponding sparse expressions, which are then used as the input data and corresponding label data for deep learning; then we give an iterative deep learning algorithm in the cosine transform domain, that is, after obtaining the seismic data residuals and velocity model residuals of the previous round of test results, they are used again as new input data and label data, and re-trained in the cosine domain to obtain a new network, and the prediction results of the previous round are corrected, and then the cycle is repeated until the termination condition is reached. The algorithm effect was verified on the SEG/EAGE salt model and the seabed sulfide physical model site data.",
    "published": "2024-07-28",
    "link": "http://arxiv.org/abs/2407.19419v1",
    "code_url": null
  },
  {
    "title": "Polyp segmentation in colonoscopy images using DeepLabV3++",
    "author": "Al Mohimanul Islam, Sadia Shakiba Bhuiyan, Mysun Mashira, Md. Rayhan Ahmed, Salekul Islam, Swakkhar Shatabda",
    "summary": "Segmenting polyps in colonoscopy images is essential for the early identification and diagnosis of colorectal cancer, a significant cause of worldwide cancer deaths. Prior deep learning based models such as Attention based variation, UNet variations and Transformer-derived networks have had notable success in capturing intricate features and complex polyp shapes. In this study, we have introduced the DeepLabv3++ model which is an enhanced version of the DeepLabv3+ architecture. It is designed to improve the precision and robustness of polyp segmentation in colonoscopy images. We have utilized The proposed model incorporates diverse separable convolutional layers and attention mechanisms within the MSPP block, enhancing its capacity to capture multi-scale and directional features. Additionally, the redesigned decoder further transforms the extracted features from the encoder into a more meaningful segmentation map. Our model was evaluated on three public datasets (CVC-ColonDB, CVC-ClinicDB, Kvasir-SEG) achieving Dice coefficient scores of 96.20%, 96.54%, and 96.08%, respectively. The experimental analysis shows that DeepLabV3++ outperforms several state-of-the-art models in polyp segmentation tasks. Furthermore, compared to the baseline DeepLabV3+ model, our DeepLabV3++ with its MSPP module and redesigned decoder architecture, significantly reduced segmentation errors (e.g., false positives/negatives) across small, medium, and large polyps. This improvement in polyp delineation is crucial for accurate clinical decision-making in colonoscopy.",
    "published": "2024-07-27",
    "link": "http://arxiv.org/abs/2407.19327v1",
    "code_url": null
  }
]